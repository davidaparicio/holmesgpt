{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HolmesGPT","text":"<p>AI Agent for Troubleshooting Cloud-Native Environments.</p> <p></p>"},{"location":"#quick-start","title":"Quick Start","text":"<ul> <li> <p> Install CLI</p> <p>Run HolmesGPT from your terminal</p> <p> Install</p> </li> <li> <p> Install UI/TUI</p> <p>Use through a web interface or K9s plugin</p> <p> Install</p> </li> </ul>"},{"location":"#need-help","title":"Need Help?","text":"<ul> <li>Join our Slack - Get help from the community</li> <li>Request features on GitHub - Suggest improvements or report bugs</li> </ul>"},{"location":"cncf-self-assesment/","title":"General Technical Review - HolmesGPT / Sandbox","text":"<ul> <li>Project: HolmesGPT</li> <li>Project Version: 0.11.5</li> <li>Website: https://github.com/robusta-dev/holmesgpt</li> <li>Date Updated: 2025-07-16</li> <li>Template Version: v1.0</li> <li>Description: HolmesGPT is an AI agent that automates cloud-native troubleshooting, bridging knowledge gaps by investigating alerts, executing runbooks, and correlating observability data in cloud-native platforms.</li> </ul>"},{"location":"cncf-self-assesment/#day-0-planning-phase","title":"Day 0 - Planning Phase","text":""},{"location":"cncf-self-assesment/#scope","title":"Scope","text":"<p>Describe the roadmap process, how scope is determined for mid to long term features, as well as how the roadmap maps back to current contributions and maintainer ladder?</p> <p>HolmesGPT follows an open and community-driven roadmap process. The roadmap is maintained publicly (via GitHub Projects and issues) and is shaped by a combination of the following inputs:</p> <ul> <li>Community feedback from users and contributors, gathered through GitHub issues, Slack, and regular discussions</li> <li>Strategic alignment with the project\u2019s long-term mission: enabling AI-powered root cause analysis in cloud-native environments</li> <li>Technical priorities based on gaps surfaced during usage or contributor pain points</li> <li>Upstream integration plans with core CNCF projects like Prometheus, Kubernetes, and OpenTelemetry</li> </ul> <p>Mid to long-term scope is defined quarterly and iterated collaboratively among maintainers</p> <p>Describe the target persona or user(s) for the project? DevOps, SREs, and platform engineers</p> <p>Explain the primary use case for the project. What additional use cases are supported by the project?</p> <p>Two primary use cases: 1. Root-cause analysis of alerts (i.e. Prometheus alerts) 2. Troubleshooting problems (via free-text questions) in cloud-native environments</p> <p>In both cases, the analysis can be 100% autonomous, or driven by predefined runbooks.</p> <p>Explain which use cases have been identified as unsupported by the project. General purpose AI agent use cases (outside of troubleshooting and RCA), coding agents, and security use cases.</p> <p>Describe the intended types of organizations who would benefit from adopting this project. (i.e. financial services, any software manufacturer, organizations providing platform engineering services)?</p> <p>End users running cloud-native services at scale, especially in larger environments with many microservices and potential for complex cascading failures.</p> <p>Please describe any completed end user research and link to any reports. N/A</p>"},{"location":"cncf-self-assesment/#usability","title":"Usability","text":"<p>How should the target personas interact with your project? The most common entry point is via a cli tool that is run on-demand to troubleshoot a problem or an alert.</p> <p>Describe the user experience (UX) and user interface (UI) of the project. The open source includes a CLI tool with an interactive console and /slash commands. Several vendors have built graphical user interfaces on top of that.</p> <p>Describe how this project integrates with other projects in a production environment. The project exposes an HTTP API and a Helm chart for running the HTTP server in Kubernetes clusters.</p>"},{"location":"cncf-self-assesment/#design","title":"Design","text":"<p>Explain the design principles and best practices the project is following. * Human-in-the-loop: users are able to interact with and guide HolmesGPT investigations * Safety-first - The agent is restricted by default and only allowed to run safe commands. * Interoperable \u2013 works seamlessly with existing observability stacks * Kubernetes-native: Works with Prometheus, Loki, and other CNCF stack components. * Extensible: Modular plugin system for adding new data sources, including external MCP servers</p> <p>Outline or link to the project\u2019s architecture requirements? Describe how they differ for Proof of Concept, Development, Test and Production environments, as applicable. Lightweight, can run as a standalone local CLI or in-cluster as an HTTP server. Typical during POC users start with the local CLI and when rolling into production use a more advanced setup.</p> <p>Define any specific service dependencies the project relies on in the cluster. No relevant.</p> <p>Describe how the project implements Identity and Access Management. HolmesGPT runs with user-provided credentials (e.g. service account) and respects whichever permissions were given to it.</p> <p>Describe how the project has addressed sovereignty. HolmesGPT runs fully within the user\u2019s infrastructure. All data\u2014logs, metrics, traces, and AI-generated insights\u2014remains under user control. There\u2019s no dependency on external SaaS or third-party APIs unless explicitly configured. This ensures data privacy, compliance, and operational sovereignty.</p> <p>Regarding data sent to the LLM, here too users have the choice of providing their own LLM or using a trusted cloud provider of their choice.</p> <p>Describe any compliance requirements addressed by the project. N/A</p> <p>Describe the project\u2019s High Availability requirements. Each request to HolmesGPT is stateless, so it's possible to run multiple instances.</p> <p>Describe the project\u2019s resource requirements, including CPU, Network and Memory. Minimal, similar to any standard Python application running in a Kubernetes cluster. We recommend some defaults in the Helm chart, but this can be customized by the user.</p> <p>Describe the project\u2019s storage requirements, including its use of ephemeral and/or persistent storage. N/A</p> <p>Please outline the project\u2019s API Design The project itself exposes a REST API, following standard conventions. We strive to maintain backwards compatibility, and to add new endpoints when changing something instead of breaking an existing endpoint.</p> <p>It will perform HTTP calls to collect data when investigating problems - the exact calls depend on which data sources the user enabled.</p> <p>We bump the major release number only on breaking changes. Minor releases are done about monthly, when there are substantial new features. Bug fixes are done as needed with a patch release.</p> <p>Describe how the project is installed and initialized, e.g. a minimal install with a few lines of code or does it require more complex integration and configuration? Please refer to https://holmesgpt.dev/installation/cli-installation/</p> <p>How does an adopter test and validate the installation? Please refer to https://holmesgpt.dev/walkthrough/</p>"},{"location":"cncf-self-assesment/#security","title":"Security","text":"<p>Please provide a link to the project\u2019s cloud native security self assessment.</p> <p>Please review the Cloud Native Security Tenets from TAG Security. How are you satisfying the tenets of cloud native security projects? This is extremely relevant for us, given the risk that AI models can hallucinate and thereby that HolmesGPT could run malicious commands. To mitigate this, default access is read-only and non-mutating and limited to a pre-approved list of safe commands and integrations.</p> <p>Describe how each of the cloud native principles apply to your project. * Make security a design requirement - see above. * Applying secure configuration has the best user experience - also covered above * Selecting insecure configuration is a conscious decision -  Users must make a conscious and concerted effort to add insecure toolsets (data sources) to HolmesGPT - it cannot be done accidentally. * Transition from insecure to secure state is possible - users are free to reduce the permissions with which Holmes runs at any point in time and Holmes will identify it and adapt * Secure defaults are inherited - by default Holmes inherits service roles and permissions from its environment * Exception lists have first class support - users can add their own toolsets to give Holmes access to additional commands * Secure defaults protect against pervasive vulnerability exploits - in the case of Holmes, this is equivalent to providing security even when used with malicious/hallucinating LLM which is done as described above * Security limitations of a system are explainable - Holmes reports permission issues when encountered</p> <p>How do you recommend users alter security defaults in order to \"loosen\" the security of the project? Please link to any documentation the project has written concerning these use cases. https://holmesgpt.dev/data-sources/permissions/</p> <p>Security Hygiene We discuss security implications of features at the design phase, when working on new features. Where warranted there are dedicated discussions around security related aspects. Code reviews function as the final review, but very few issues reach that stage due to thinking about security earlier in the process.</p> <p>Explain the least minimal privileges required by the project and reasons for additional privileges. Read-only access to the data sources that are relevant to the requested RCA.</p> <p>Describe how the project is handling certificate rotation and mitigates any issues with certificates. Not relevant.</p> <p>Describe how the project is following and implementing secure software supply chain best practices Link is broken, but we strictly review all changes to CI/CD and anything that impacts building the project and distributing it to end users.</p>"},{"location":"community/","title":"Community","text":"<p>Join our community to collaborate on the future of AI-powered troubleshooting.</p>"},{"location":"community/#community-meetup-recording","title":"Community Meetup Recording","text":"<p>\ud83d\udcf9 Watch our first HolmesGPT Community Meetup</p> <p>We held our inaugural community meetup on August 21, 2025. Watch the recording to learn about:</p> <ul> <li>HolmesGPT roadmap and upcoming features</li> <li>Community Q&amp;A and feedback</li> <li>Ways to get involved with the project</li> </ul> <p>\u25b6\ufe0f Watch Recording on YouTube</p>"},{"location":"community/#resources","title":"Resources","text":"<ul> <li>\ud83d\udcdd Meeting Notes</li> <li>\ud83d\udccb HolmesGPT Roadmap</li> </ul>"},{"location":"community/#get-involved","title":"Get Involved","text":"<p>\ud83d\udcac Join the Discussion:</p> <ul> <li>Slack - #holmesgpt channel</li> <li>GitHub Issues - Report bugs and request features</li> </ul> <p>\ud83e\udd1d Contribute:</p> <ul> <li>Share your HolmesGPT use cases</li> <li>Help improve documentation</li> <li>Submit code contributions</li> </ul>"},{"location":"using-mock-data/","title":"Using Mock Data in LLM Tests","text":"<p>This document describes mock data usage in HolmesGPT's LLM evaluation tests. Live evaluations (<code>RUN_LIVE=true</code>) are strongly preferred because they're more reliable and accurate.</p>"},{"location":"using-mock-data/#why-live-evaluations-are-preferred","title":"Why Live Evaluations Are Preferred","text":"<p>LLMs can take multiple paths to reach the same conclusion. When using mock data: - The LLM might call tools in a different order than when mocks were generated - It might use different tool combinations to diagnose the same issue - It might ask for additional information not captured in the mocks - Mock data represents only one possible investigation path</p> <p>With live evaluations, the LLM can explore any path it chooses, making tests more robust and realistic.</p>"},{"location":"using-mock-data/#when-mock-data-is-necessary","title":"When Mock Data Is Necessary","text":"<p>Mock data is sometimes unavoidable: - CI/CD environments without Kubernetes cluster access - Testing specific edge cases that require controlled responses - Reproducing exact historical scenarios</p> <p>Important: Even when using mocks, always validate with <code>RUN_LIVE=true</code> in a real environment.</p>"},{"location":"using-mock-data/#mock-data-structure","title":"Mock Data Structure","text":"<p>Mock files are stored in <code>tests/llm/fixtures/{test_name}/</code> directories: - Each test has mock tool responses and expected outputs - Mock responses are YAML files matching tool names - Uses LLM-as-judge for automated evaluation</p>"},{"location":"using-mock-data/#generating-mock-data","title":"Generating Mock Data","text":"<pre><code># Generate mocks for a specific test\npoetry run pytest tests/llm/test_ask_holmes.py -k \"test_name\" --generate-mocks\n\n# Regenerate all mock files\npoetry run pytest tests/llm/test_ask_holmes.py --regenerate-all-mocks\n</code></pre>"},{"location":"using-mock-data/#mock-data-guidelines","title":"Mock Data Guidelines","text":"<p>When creating mock data: - Never generate mock data manually - always use <code>--generate-mocks</code> with live execution - Mock data should match real-world responses exactly - Include all fields that would be present in actual responses - Maintain proper timestamps and data relationships</p>"},{"location":"using-mock-data/#important-notes","title":"Important Notes","text":"<ul> <li>Mock data captures only one investigation path - LLMs may take completely different approaches to reach the same conclusion</li> <li>Tests with mocks often fail when the LLM chooses a different but equally valid investigation strategy</li> <li>Mock execution misses the dynamic nature of real troubleshooting</li> <li>Always develop and validate tests with <code>RUN_LIVE=true</code></li> <li>Mock data becomes stale as APIs and tool behaviors evolve</li> </ul>"},{"location":"using-mock-data/#testing-workflow","title":"Testing Workflow","text":"<ol> <li>Develop test with <code>RUN_LIVE=true</code></li> <li>Generate mocks if needed: <code>--generate-mocks</code></li> <li>Validate mock execution matches live behavior</li> <li>Always use <code>RUN_LIVE=true</code> for final validation</li> </ol>"},{"location":"ai-providers/","title":"AI Providers","text":"<p>HolmesGPT supports multiple AI providers, giving you flexibility in choosing the best model for your needs and budget.</p> <ul> <li> Anthropic</li> <li> AWS Bedrock</li> <li> Azure OpenAI</li> <li> Gemini</li> <li> Google Vertex AI</li> <li> Ollama</li> <li> OpenAI</li> <li> OpenAI-Compatible</li> <li> Using Multiple Providers</li> </ul>"},{"location":"ai-providers/#quick-start","title":"Quick Start","text":"<p>Recommended for New Users</p> <p>OpenAI models provide a good balance of accuracy and speed.</p> <p>Anthropic models often give better results at the expense of speed.</p> <p>To get started with an OpenAI model:</p> <ol> <li>Get an OpenAI API key</li> <li>Set <code>export OPENAI_API_KEY=\"your-api-key\"</code></li> <li>Run <code>holmes ask \"what pods are failing?\"</code> (OpenAI is the default provider)</li> </ol> <p>Choose your provider above to see detailed configuration instructions.</p>"},{"location":"ai-providers/#configuration","title":"Configuration","text":"<p>Each AI provider requires specific environment variables for authentication. See the Environment Variables Reference for a complete list of all configuration options beyond just API keys.</p>"},{"location":"ai-providers/anthropic/","title":"Anthropic","text":"<p>Configure HolmesGPT to use Anthropic's Claude models.</p>"},{"location":"ai-providers/anthropic/#setup","title":"Setup","text":"<p>Get an Anthropic API key.</p>"},{"location":"ai-providers/anthropic/#configuration","title":"Configuration","text":"<pre><code>export ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nholmes ask \"what pods are failing?\" --model=\"anthropic/&lt;your-claude-model&gt;\"\n</code></pre>"},{"location":"ai-providers/anthropic/#using-cli-parameters","title":"Using CLI Parameters","text":"<p>You can also pass the API key directly as a command-line parameter:</p> <pre><code>holmes ask \"what pods are failing?\" --model=\"anthropic/&lt;your-claude-model&gt;\" --api-key=\"your-api-key\"\n</code></pre>"},{"location":"ai-providers/anthropic/#prompt-caching","title":"Prompt Caching","text":"<p>HolmesGPT adds Anthropic's prompt caching feature, which can significantly reduce costs and latency for repeated API calls with similar prompts.</p> <p>HolmesGPT automatically adds cache control to the last message in each API call. This caches everything from the beginning of the conversation up to that point, making subsequent calls with the same prefix much faster and cheaper.</p>"},{"location":"ai-providers/anthropic/#how-it-works","title":"How It Works","text":"<ul> <li>Anthropic uses prefix-based caching - it caches the exact sequence of messages up to the cache control point</li> <li>The cache has a 5-minute lifetime by default</li> <li>Cached content must be at least 1024 tokens to be effective</li> <li>You're charged for cache writes on the first call, but subsequent cache hits are much cheaper</li> </ul>"},{"location":"ai-providers/anthropic/#benefits-in-holmesgpt","title":"Benefits in HolmesGPT","text":"<p>Prompt caching is particularly effective for HolmesGPT because:</p> <ul> <li>System prompts with tool definitions are large and static - perfect for caching</li> <li>Tool investigation loops reuse the same context multiple times</li> <li>Multi-step investigations benefit from cached conversation history</li> </ul>"},{"location":"ai-providers/anthropic/#additional-resources","title":"Additional Resources","text":"<p>HolmesGPT uses the LiteLLM API to support Anthropic provider. Refer to LiteLLM Anthropic docs for more details.</p>"},{"location":"ai-providers/aws-bedrock/","title":"AWS Bedrock","text":"<p>Configure HolmesGPT to use AWS Bedrock foundation models.</p>"},{"location":"ai-providers/aws-bedrock/#setup","title":"Setup","text":""},{"location":"ai-providers/aws-bedrock/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install boto3: AWS Bedrock requires boto3 version 1.28.57 or higher:    <pre><code>pip install \"boto3&gt;=1.28.57\"\n</code></pre></p> </li> <li> <p>AWS credentials: Ensure you have AWS credentials configured with access to Bedrock models. See AWS Docs.</p> </li> </ol>"},{"location":"ai-providers/aws-bedrock/#configuration","title":"Configuration","text":""},{"location":"ai-providers/aws-bedrock/#environment-variables","title":"Environment Variables","text":"<pre><code>export AWS_REGION_NAME=\"us-east-1\"  # Replace with your region\nexport AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\n\nholmes ask \"what pods are failing?\" --model=\"bedrock/&lt;your-bedrock-model&gt;\"\n</code></pre>"},{"location":"ai-providers/aws-bedrock/#finding-your-aws-credentials","title":"Finding Your AWS Credentials","text":"<p>If the AWS CLI is already configured on your machine, you may be able to find the above values with:</p> <pre><code>cat ~/.aws/credentials ~/.aws/config\n</code></pre>"},{"location":"ai-providers/aws-bedrock/#finding-available-models","title":"Finding Available Models","text":"<p>To list models your account can access (replacing <code>us-east-1</code> with the relevant region):</p> <pre><code>aws bedrock list-foundation-models --region=us-east-1 | grep modelId\n</code></pre> <p>Important: Different models are available in different regions. For example, Claude Opus is only available in us-west-2.</p>"},{"location":"ai-providers/aws-bedrock/#model-name-examples","title":"Model Name Examples","text":"<p>Be sure to replace <code>&lt;your-bedrock-model&gt;</code> with a model you have access to, such as <code>anthropic.claude-3-5-sonnet-20240620-v1:0</code></p>"},{"location":"ai-providers/aws-bedrock/#additional-resources","title":"Additional Resources","text":"<p>HolmesGPT uses the LiteLLM API to support AWS Bedrock provider. Refer to LiteLLM Bedrock docs for more details.</p>"},{"location":"ai-providers/azure-openai/","title":"Azure OpenAI","text":"<p>Configure HolmesGPT to use Azure OpenAI Service.</p>"},{"location":"ai-providers/azure-openai/#setup","title":"Setup","text":"<p>Create an Azure OpenAI resource.</p>"},{"location":"ai-providers/azure-openai/#configuration","title":"Configuration","text":"<pre><code>export AZURE_API_VERSION=\"2024-02-15-preview\"\nexport AZURE_API_BASE=\"https://your-resource.openai.azure.com\"\nexport AZURE_API_KEY=\"your-azure-api-key\"\n\nholmes ask \"what pods are failing?\" --model=\"azure/&lt;your-deployment-name&gt;\"\n</code></pre>"},{"location":"ai-providers/azure-openai/#using-cli-parameters","title":"Using CLI Parameters","text":"<p>You can also pass the API key directly as a command-line parameter:</p> <pre><code>holmes ask \"what pods are failing?\" --model=\"azure/&lt;your-deployment-name&gt;\" --api-key=\"your-api-key\"\n</code></pre>"},{"location":"ai-providers/azure-openai/#additional-resources","title":"Additional Resources","text":"<p>HolmesGPT uses the LiteLLM API to support Azure OpenAI provider. Refer to LiteLLM Azure docs for more details.</p>"},{"location":"ai-providers/gemini/","title":"Gemini","text":"<p>Configure HolmesGPT to use Google's Gemini models via Google AI Studio.</p>"},{"location":"ai-providers/gemini/#setup","title":"Setup","text":"<p>Get your API key from Google AI Studio.</p>"},{"location":"ai-providers/gemini/#configuration","title":"Configuration","text":"<pre><code>export GEMINI_API_KEY=\"your-gemini-api-key\"\nholmes ask \"what pods are failing?\" --model=\"gemini/&lt;your-gemini-model&gt;\"\n</code></pre>"},{"location":"ai-providers/gemini/#using-cli-parameters","title":"Using CLI Parameters","text":"<p>You can also pass the API key directly as a command-line parameter:</p> <pre><code>holmes ask \"what pods are failing?\" --model=\"gemini/&lt;your-gemini-model&gt;\" --api-key=\"your-api-key\"\n</code></pre>"},{"location":"ai-providers/gemini/#gemini-specific-configuration","title":"Gemini-Specific Configuration","text":"<p>Gemini models require special handling for tools without parameters. Set the following environment variable when using Gemini:</p> <pre><code>export TOOL_SCHEMA_NO_PARAM_OBJECT_IF_NO_PARAMS=true\n</code></pre> <p>This ensures that tool schemas are formatted correctly for Gemini's requirements. See the Environment Variables Reference for more details.</p>"},{"location":"ai-providers/gemini/#additional-resources","title":"Additional Resources","text":"<p>HolmesGPT uses the LiteLLM API to support Gemini provider. Refer to LiteLLM Gemini docs for more details.</p>"},{"location":"ai-providers/google-vertex-ai/","title":"Google Vertex AI","text":"<p>Configure HolmesGPT to use Google Vertex AI with Gemini models.</p>"},{"location":"ai-providers/google-vertex-ai/#setup","title":"Setup","text":"<ol> <li>Create a Google Cloud project with Vertex AI API enabled</li> <li>Create a service account with <code>Vertex AI User</code> role</li> <li>Download the JSON key file</li> </ol>"},{"location":"ai-providers/google-vertex-ai/#configuration","title":"Configuration","text":"<pre><code>export VERTEXAI_PROJECT=\"your-project-id\"\nexport VERTEXAI_LOCATION=\"us-central1\"\nexport GOOGLE_APPLICATION_CREDENTIALS=\"path/to/service-account-key.json\"\n\nholmes ask \"what pods are failing?\" --model=\"vertex_ai/&lt;your-vertex-model&gt;\"\n</code></pre>"},{"location":"ai-providers/google-vertex-ai/#using-cli-parameters","title":"Using CLI Parameters","text":"<p>You can also pass credentials directly as command-line parameters:</p> <pre><code>holmes ask \"what pods are failing?\" --model=\"vertex_ai/&lt;your-vertex-model&gt;\" --api-key=\"your-service-account-key\"\n</code></pre>"},{"location":"ai-providers/google-vertex-ai/#additional-resources","title":"Additional Resources","text":"<p>HolmesGPT uses the LiteLLM API to support Google Vertex AI provider. Refer to LiteLLM Google Vertex AI docs for more details.</p>"},{"location":"ai-providers/ollama/","title":"Ollama","text":"<p>Configure HolmesGPT to use local models with Ollama.</p> <p>Warning</p> <p>Ollama support is experimental. Tool-calling capabilities are limited and may produce inconsistent results. Only LiteLLM supported Ollama models work with HolmesGPT.</p>"},{"location":"ai-providers/ollama/#setup","title":"Setup","text":"<ol> <li>Download Ollama from ollama.com</li> <li>Start Ollama: <code>ollama serve</code></li> <li>Download models: <code>ollama pull &lt;model-name&gt;</code></li> </ol>"},{"location":"ai-providers/ollama/#configuration","title":"Configuration","text":"<pre><code>export OLLAMA_API_BASE=\"http://localhost:11434\"\nholmes ask \"what pods are failing?\" --model=\"ollama_chat/&lt;your-ollama-model&gt;\"\n</code></pre>"},{"location":"ai-providers/ollama/#using-cli-parameters","title":"Using CLI Parameters","text":"<p>You can also specify the model directly as a command-line parameter:</p> <pre><code>holmes ask \"what pods are failing?\" --model=\"ollama_chat/&lt;your-ollama-model&gt;\"\n</code></pre>"},{"location":"ai-providers/ollama/#additional-resources","title":"Additional Resources","text":"<p>HolmesGPT uses the LiteLLM API to support Ollama provider. Refer to LiteLLM Ollama docs for more details.</p>"},{"location":"ai-providers/openai-compatible/","title":"OpenAI-Compatible Models","text":"<p>Configure HolmesGPT to use any OpenAI-compatible API.</p> <p>Function Calling Required</p> <p>Your model and inference server must support function calling (tool calling). Models that lack this capability may produce incorrect results.</p>"},{"location":"ai-providers/openai-compatible/#requirements","title":"Requirements","text":"<ul> <li>Function calling support - OpenAI-style tool calling</li> <li>OpenAI-compatible API - Standard endpoints and request/response format</li> </ul>"},{"location":"ai-providers/openai-compatible/#supported-inference-servers","title":"Supported Inference Servers","text":"<ul> <li>llama-cpp-python</li> <li>LocalAI</li> <li>Text Generation WebUI (with OpenAI extension)</li> </ul>"},{"location":"ai-providers/openai-compatible/#configuration","title":"Configuration","text":"<pre><code>export OPENAI_API_BASE=\"http://localhost:8000/v1\"\nexport OPENAI_API_KEY=\"not-needed\"\nholmes ask \"what pods are failing?\" --model=\"openai/&lt;your-model&gt;\"\n</code></pre>"},{"location":"ai-providers/openai-compatible/#using-cli-parameters","title":"Using CLI Parameters","text":"<p>You can also specify the model directly as a command-line parameter:</p> <pre><code>holmes ask \"what pods are failing?\" --model=\"openai/&lt;your-model&gt;\"\n</code></pre>"},{"location":"ai-providers/openai-compatible/#setup-examples","title":"Setup Examples","text":""},{"location":"ai-providers/openai-compatible/#localai","title":"LocalAI","text":"<pre><code>docker run -p 8080:8080 localai/localai:latest\nexport OPENAI_API_BASE=\"http://localhost:8080/v1\"\n</code></pre>"},{"location":"ai-providers/openai-compatible/#llama-cpp-python","title":"llama-cpp-python","text":"<pre><code>pip install 'llama-cpp-python[server]'\npython -m llama_cpp.server --model model.gguf --chat_format chatml\nexport OPENAI_API_BASE=\"http://localhost:8000/v1\"\nholmes ask \"analyze my deployment\" --model=openai/your-loaded-model\n</code></pre>"},{"location":"ai-providers/openai-compatible/#custom-ssl-certificates","title":"Custom SSL Certificates","text":"<p>If your LLM provider uses a custom Certificate Authority (CA):</p> <pre><code># Base64 encode your certificate and set it as an environment variable\nexport CERTIFICATE=\"base64-encoded-cert-here\"\n</code></pre>"},{"location":"ai-providers/openai-compatible/#known-limitations","title":"Known Limitations","text":"<ul> <li>vLLM: Does not yet support function calling</li> <li>Text Generation WebUI: Requires OpenAI extension enabled</li> <li>Some models: May hallucinate responses instead of reporting function calling limitations</li> </ul>"},{"location":"ai-providers/openai-compatible/#additional-resources","title":"Additional Resources","text":"<p>HolmesGPT uses the LiteLLM API to support OpenAI-compatible providers. Refer to LiteLLM OpenAI-compatible docs for more details.</p>"},{"location":"ai-providers/openai/","title":"OpenAI","text":"<p>Configure HolmesGPT to use OpenAI's GPT models.</p>"},{"location":"ai-providers/openai/#setup","title":"Setup","text":"<p>Get a paid OpenAI API key.</p> <p>Note</p> <p>Requires a paid OpenAI API key, not a ChatGPT Plus subscription.</p>"},{"location":"ai-providers/openai/#configuration","title":"Configuration","text":"<pre><code>export OPENAI_API_KEY=\"your-openai-api-key\"\nholmes ask \"what pods are failing?\"\n</code></pre>"},{"location":"ai-providers/openai/#using-cli-parameters","title":"Using CLI Parameters","text":"<p>You can also pass the API key directly as a command-line parameter:</p> <pre><code>holmes ask \"what pods are failing?\" --api-key=\"your-api-key\"\n</code></pre>"},{"location":"ai-providers/openai/#available-models","title":"Available Models","text":"<pre><code># GPT-4o (default)\nholmes ask \"what pods are failing?\"\n\n# GPT-4o mini (faster, but results are not as accurate)\nholmes ask \"what pods are failing?\" --model=\"gpt-4o-mini\"\n\n# GPT-5\nholmes ask \"what pods are failing?\" --model=\"gpt-5\"\n</code></pre>"},{"location":"ai-providers/openai/#gpt-5-reasoning-effort","title":"GPT-5 Reasoning Effort","text":"<p>When using GPT-5 models, you can control the reasoning effort level by setting the <code>REASONING_EFFORT</code> environment variable. This allows you to balance between response quality and processing time/cost.</p> <pre><code># Use minimal reasoning effort for faster responses\nexport REASONING_EFFORT=\"minimal\"\nholmes ask \"what pods are failing?\" --model=\"gpt-5\"\n\n# Use default reasoning effort\nexport REASONING_EFFORT=\"medium\"\nholmes ask \"what pods are failing?\" --model=\"gpt-5\"\n\n# Use high reasoning effort for complex investigations\nexport REASONING_EFFORT=\"high\"\nholmes ask \"what pods are failing?\" --model=\"gpt-5\"\n</code></pre> <p>Available reasoning effort levels:</p> <ul> <li><code>minimal</code> - Fastest responses, suitable for simple queries</li> <li><code>low</code> - Balance between speed and quality</li> <li><code>medium</code> - Standard reasoning depth (default)</li> <li><code>high</code> - Deeper reasoning for complex problems</li> </ul> <p>For more details on reasoning effort levels, refer to the OpenAI documentation.</p>"},{"location":"ai-providers/openai/#additional-resources","title":"Additional Resources","text":"<p>HolmesGPT uses the LiteLLM API to support OpenAI provider. Refer to LiteLLM OpenAI docs for more details.</p>"},{"location":"ai-providers/using-multiple-providers/","title":"Using Multiple Providers","text":"<p>Configure multiple AI providers to give users flexibility in choosing models through the Robusta UI.</p> <p>UI Feature Only</p> <p>Multiple provider configuration currently only impacts investigations run from the Robusta UI. When multiple providers are defined, users can select which model to use from a dropdown in the UI. This feature does not affect CLI usage or investigations triggered from other sources.</p>"},{"location":"ai-providers/using-multiple-providers/#configuration","title":"Configuration","text":"<p>Configure multiple models using the <code>modelList</code> parameter in your Helm values, along with the necessary environment variables.</p>"},{"location":"ai-providers/using-multiple-providers/#step-1-create-the-kubernetes-secret","title":"Step 1: Create the Kubernetes Secret","text":"<p>First, create a secret with your API keys (only include the ones you need):</p> <pre><code># Example with all providers - only include what you're using\nkubectl create secret generic holmes-secrets \\\n  --from-literal=openai-api-key=\"sk-...\" \\\n  --from-literal=anthropic-api-key=\"sk-ant-...\" \\\n  --from-literal=azure-api-key=\"...\" \\\n  --from-literal=aws-access-key-id=\"AKIA...\" \\\n  --from-literal=aws-secret-access-key=\"...\" \\\n  -n &lt;namespace&gt;\n\n# Example with just OpenAI and Anthropic\nkubectl create secret generic holmes-secrets \\\n  --from-literal=openai-api-key=\"sk-...\" \\\n  --from-literal=anthropic-api-key=\"sk-ant-...\" \\\n  -n &lt;namespace&gt;\n</code></pre>"},{"location":"ai-providers/using-multiple-providers/#step-2-configure-helm-values","title":"Step 2: Configure Helm Values","text":"<pre><code># values.yaml\n# Reference only the API keys you created in the secret\nadditionalEnvVars:\n  - name: AZURE_API_KEY\n    valueFrom:\n      secretKeyRef:\n        name: holmes-secrets\n        key: azure-api-key\n  - name: ANTHROPIC_API_KEY\n    valueFrom:\n      secretKeyRef:\n        name: holmes-secrets\n        key: anthropic-api-key\n  - name: AWS_ACCESS_KEY_ID\n    valueFrom:\n      secretKeyRef:\n        name: holmes-secrets\n        key: aws-access-key-id\n  - name: AWS_SECRET_ACCESS_KEY\n    valueFrom:\n      secretKeyRef:\n        name: holmes-secrets\n        key: aws-secret-access-key\n  - name: OPENAI_API_KEY\n    valueFrom:\n      secretKeyRef:\n        name: holmes-secrets\n        key: openai-api-key\n\n# Configure the model list using the environment variables\nmodelList:\n  # Standard OpenAI\n  openai-4o:\n    api_key: \"{{ env.OPENAI_API_KEY }}\"\n    model: openai/gpt-4o\n    temperature: 0\n\n  # Azure OpenAI Models\n  azure-4o:\n    api_key: \"{{ env.AZURE_API_KEY }}\"\n    model: azure/gpt-4o\n    api_base: https://your-resource.openai.azure.com/\n    api_version: \"2025-01-01-preview\"\n    temperature: 0\n\n  azure-gpt-5:\n    api_key: \"{{ env.AZURE_API_KEY }}\"\n    model: azure/gpt-5-chat\n    api_base: https://your-resource.openai.azure.com/\n    api_version: \"2025-01-01-preview\"\n    temperature: 1 # only 1 is supported for gpt-5 models\n\n  # Anthropic Models\n  claude-sonnet-4:\n    api_key: \"{{ env.ANTHROPIC_API_KEY }}\"\n    model: claude-sonnet-4-20250514\n    temperature: 1\n    thinking:\n      budget_tokens: 10000\n      type: enabled\n\n  # AWS Bedrock\n  bedrock-claude:\n    aws_access_key_id: \"{{ env.AWS_ACCESS_KEY_ID }}\"\n    aws_region_name: eu-south-2\n    aws_secret_access_key: \"{{ env.AWS_SECRET_ACCESS_KEY }}\"\n    model: bedrock/eu.anthropic.claude-sonnet-4-20250514-v1:0\n    temperature: 1\n    thinking:\n      budget_tokens: 10000\n      type: enabled\n</code></pre>"},{"location":"ai-providers/using-multiple-providers/#model-parameters","title":"Model Parameters","text":"<p>Each model in <code>modelList</code> can accept any parameter supported by LiteLLM for that provider. The <code>model</code> parameter is required, while authentication requirements vary by provider. Any additional LiteLLM parameters will be passed directly through to the provider.</p>"},{"location":"ai-providers/using-multiple-providers/#required-parameter","title":"Required Parameter","text":"<ul> <li><code>model</code>: Model identifier (provider-specific format)</li> </ul>"},{"location":"ai-providers/using-multiple-providers/#common-parameters","title":"Common Parameters","text":"<ul> <li><code>api_key</code>: API key for authentication where required (can use <code>{{ env.VAR_NAME }}</code> syntax)</li> <li><code>temperature</code>: Creativity level (0-2, lower is more deterministic)</li> </ul>"},{"location":"ai-providers/using-multiple-providers/#additional-parameters","title":"Additional Parameters","text":"<p>You can pass any LiteLLM-supported parameter for your provider. Examples include:</p> <ul> <li>Azure: <code>api_base</code>, <code>api_version</code>, <code>deployment_id</code></li> <li>Anthropic: <code>thinking</code> (with <code>budget_tokens</code> and <code>type</code>)</li> <li>AWS Bedrock: <code>aws_access_key_id</code>, <code>aws_secret_access_key</code>, <code>aws_region_name</code>, <code>aws_session_token</code></li> <li>Google Vertex: <code>vertex_project</code>, <code>vertex_location</code></li> </ul> <p>Refer to LiteLLM documentation for the complete list of parameters supported by each provider.</p>"},{"location":"ai-providers/using-multiple-providers/#user-experience","title":"User Experience","text":"<p>When multiple models are configured:</p> <ol> <li>Users see a model selector dropdown in the Robusta UI</li> <li>Each model appears with its configured name (e.g., \"azure-4o\", \"claude-sonnet-4\")</li> <li>Users can switch between models for different investigations</li> </ol>"},{"location":"ai-providers/using-multiple-providers/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive names: Name models clearly (e.g., <code>fast-gpt4</code>, <code>accurate-claude</code>, <code>budget-mini</code>)</li> <li>Secure API keys: Always use Kubernetes secrets for API keys</li> </ol>"},{"location":"ai-providers/using-multiple-providers/#limitations","title":"Limitations","text":"<ul> <li>UI Only: Model selection currently only works in the Robusta UI</li> <li>No automatic failover: If a selected model fails, users must manually switch</li> </ul>"},{"location":"ai-providers/using-multiple-providers/#see-also","title":"See Also","text":"<ul> <li>UI Installation</li> <li>Helm Configuration</li> <li>Individual provider documentation for specific configuration details</li> </ul>"},{"location":"data-sources/","title":"Data Sources","text":"<p>HolmesGPT connects to your monitoring and observability tools to provide comprehensive root cause analysis.</p>"},{"location":"data-sources/#available-options","title":"Available Options","text":"<ul> <li> <p>Built-in Toolsets</p> <p>Pre-configured integrations for popular tools like Prometheus, Grafana, DataDog, and more.</p> </li> <li> <p>Custom Toolsets</p> <p>Create your own integrations for proprietary or specialized tools.</p> </li> <li> <p>Remote MCP Servers</p> <p>Connect to Model Context Protocol servers for extended capabilities.</p> </li> </ul>"},{"location":"data-sources/custom-toolsets/","title":"Custom Toolsets","text":"<p>If the built-in toolsets don't meet your needs, you can extend HolmesGPT's investigation capabilities by creating custom toolsets. This is especially useful for unique use cases, proprietary tools, or specialized infrastructure setups. Examples include advanced log analysis tools, external monitoring integrations, or custom diagnostic scripts.</p> <p>By creating custom toolsets, you can ensure HolmesGPT has access to all the data sources and tools necessary for thorough investigations in your specific environment.</p>"},{"location":"data-sources/custom-toolsets/#examples","title":"Examples","text":"<p>Below are three examples of how to create custom toolsets for different scenarios.</p>"},{"location":"data-sources/custom-toolsets/#example-1-grafana-toolset","title":"Example 1: Grafana Toolset","text":"<p>This example creates a toolset that helps HolmesGPT view and suggest relevant Grafana dashboards.</p> Holmes CLIRobusta Helm Chart <p>Configuration File (<code>toolsets.yaml</code>):</p> <pre><code>toolsets:\n  grafana:\n    description: \"View and suggest Grafana dashboards\"\n    prerequisites: \"Grafana instance accessible from HolmesGPT\"\n    tags: [monitoring, observability]\n    installation: |\n      1. Ensure Grafana is accessible from HolmesGPT\n      2. Configure Grafana API credentials if authentication is required\n    tools:\n      - name: view_dashboard\n        description: \"View a specific Grafana dashboard by ID or name\"\n        command: |\n          curl -s \"${GRAFANA_URL}/api/dashboards/uid/{{ dashboard_uid }}\" \\\n            -H \"Authorization: Bearer ${GRAFANA_TOKEN}\"\n        additionalInstructions: |\n          Parse the JSON response to extract dashboard information.\n          If dashboard is not found, suggest similar dashboards.\n\n      - name: search_dashboards\n        description: \"Search for dashboards related to specific keywords\"\n        command: |\n          curl -s \"${GRAFANA_URL}/api/search?query={{ search_query }}\" \\\n            -H \"Authorization: Bearer ${GRAFANA_TOKEN}\"\n        additionalInstructions: |\n          Return the most relevant dashboards based on the search query.\n          Include dashboard URLs for easy access.\n</code></pre> <p>Environment Variables:</p> <pre><code>export GRAFANA_URL=\"http://grafana.monitoring.svc.cluster.local:3000\"\nexport GRAFANA_TOKEN=\"your-grafana-api-token\"\n</code></pre> <p>Run HolmesGPT:</p> <pre><code>holmes ask \"show me dashboards related to CPU usage\" --toolsets=toolsets.yaml\n</code></pre> <p>After making changes to your toolsets file, run: <pre><code>holmes toolset refresh\n</code></pre></p> <p>Helm Values:</p> <pre><code>holmes:\n  customToolsets:\n    grafana:\n      description: \"View and suggest Grafana dashboards\"\n      prerequisites: \"Grafana instance accessible from HolmesGPT\"\n      tags: [monitoring, observability]\n      installation: |\n        1. Ensure Grafana is accessible from HolmesGPT\n        2. Configure Grafana API credentials if authentication is required\n      tools:\n        - name: view_dashboard\n          description: \"View a specific Grafana dashboard by ID or name\"\n          command: |\n            curl -s \"{{ grafana_url }}/api/dashboards/uid/{{ dashboard_uid }}\" \\\n              -H \"Authorization: Bearer {{ grafana_token }}\"\n          additionalInstructions: |\n            Parse the JSON response to extract dashboard information.\n            If dashboard is not found, suggest similar dashboards.\n\n        - name: search_dashboards\n          description: \"Search for dashboards related to specific keywords\"\n          command: |\n            curl -s \"{{ grafana_url }}/api/search?query={{ search_query }}\" \\\n              -H \"Authorization: Bearer {{ grafana_token }}\"\n          additionalInstructions: |\n            Return the most relevant dashboards based on the search query.\n            Include dashboard URLs for easy access.\n</code></pre> <p>Environment Variables:</p> <pre><code>export GRAFANA_URL=\"http://grafana.monitoring.svc.cluster.local:3000\"\nexport GRAFANA_TOKEN=\"your-grafana-api-token\"\n</code></pre> <p>Helm Upgrade:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre>"},{"location":"data-sources/custom-toolsets/#example-2-kubernetes-diagnostics-toolset","title":"Example 2: Kubernetes Diagnostics Toolset","text":"<p>This example creates a toolset with advanced diagnostic tools for Kubernetes clusters.</p> Holmes CLIRobusta Helm Chart <p>Configuration File (<code>toolsets.yaml</code>):</p> <pre><code>toolsets:\n  k8s-diagnostics:\n    description: \"Advanced Kubernetes diagnostic tools\"\n    prerequisites: \"kubectl access to the cluster\"\n    tags: [kubernetes, diagnostics]\n    installation: |\n      1. Ensure kubectl is configured with cluster access\n      2. Verify necessary RBAC permissions are in place\n    tools:\n      - name: check_node_pressure\n        description: \"Check for node pressure conditions and resource usage\"\n        command: |\n          kubectl get nodes -o json | jq -r '\n            .items[] |\n            select(.status.conditions[]? | select(.type == \"MemoryPressure\" or .type == \"DiskPressure\" or .type == \"PIDPressure\") | .status == \"True\") |\n            .metadata.name + \": \" + (.status.conditions[] | select(.type == \"MemoryPressure\" or .type == \"DiskPressure\" or .type == \"PIDPressure\") | .type + \" = \" + .status)\n          '\n        additionalInstructions: |\n          If any nodes show pressure conditions, investigate further and suggest remediation steps.\n\n      - name: analyze_pod_distribution\n        description: \"Analyze pod distribution across nodes in a namespace\"\n        command: |\n          kubectl get pods -n {{ namespace }} -o wide --no-headers |\n          awk '{print $7}' | sort | uniq -c | sort -nr\n        additionalInstructions: |\n          Check for uneven pod distribution that might indicate scheduling issues.\n          Suggest rebalancing if necessary.\n\n      - name: check_resource_quotas\n        description: \"Check resource quota usage in a namespace\"\n        command: |\n          kubectl describe resourcequota -n {{ namespace }}\n        additionalInstructions: |\n          Alert if resource quotas are close to limits. Suggest scaling or quota adjustments.\n</code></pre> <p>Run HolmesGPT:</p> <pre><code>holmes ask \"check for any resource pressure in the cluster\" --toolsets=toolsets.yaml\n</code></pre> <p>After making changes to your toolsets file, run: <pre><code>holmes toolset refresh\n</code></pre></p> <p>Helm Values:</p> <pre><code>holmes:\n  customToolsets:\n    k8s-diagnostics:\n      description: \"Advanced Kubernetes diagnostic tools\"\n      prerequisites: \"kubectl access to the cluster\"\n      tags: [kubernetes, diagnostics]\n      installation: |\n        1. Ensure kubectl is configured with cluster access\n        2. Verify necessary RBAC permissions are in place\n      tools:\n        - name: check_node_pressure\n          description: \"Check for node pressure conditions and resource usage\"\n          command: |\n            kubectl get nodes -o json | jq -r '\n              .items[] |\n              select(.status.conditions[]? | select(.type == \"MemoryPressure\" or .type == \"DiskPressure\" or .type == \"PIDPressure\") | .status == \"True\") |\n              .metadata.name + \": \" + (.status.conditions[] | select(.type == \"MemoryPressure\" or .type == \"DiskPressure\" or .type == \"PIDPressure\") | .type + \" = \" + .status)\n            '\n          additionalInstructions: |\n            If any nodes show pressure conditions, investigate further and suggest remediation steps.\n\n        - name: analyze_pod_distribution\n          description: \"Analyze pod distribution across nodes in a namespace\"\n          command: |\n            kubectl get pods -n {{ namespace }} -o wide --no-headers |\n            awk '{print $7}' | sort | uniq -c | sort -nr\n          additionalInstructions: |\n            Check for uneven pod distribution that might indicate scheduling issues.\n            Suggest rebalancing if necessary.\n\n        - name: check_resource_quotas\n          description: \"Check resource quota usage in a namespace\"\n          command: |\n            kubectl describe resourcequota -n {{ namespace }}\n          additionalInstructions: |\n            Alert if resource quotas are close to limits. Suggest scaling or quota adjustments.\n</code></pre> <p>Helm Upgrade:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre>"},{"location":"data-sources/custom-toolsets/#example-3-github-toolset","title":"Example 3: GitHub Toolset","text":"<p>This example shows how to create a toolset for fetching information from GitHub repositories.</p> Holmes CLIRobusta Helm Chart <p>Configuration File (<code>toolsets.yaml</code>):</p> <pre><code>toolsets:\n  github:\n    description: \"Fetch information from GitHub repositories\"\n    prerequisites: \"GitHub API token with repository access\"\n    tags: [source-control, github]\n    installation: |\n      1. Create a GitHub personal access token\n      2. Set the token as an environment variable\n      3. Ensure network access to GitHub API\n    tools:\n      - name: get_repository_info\n        description: \"Get information about a GitHub repository\"\n        command: |\n          curl -s -H \"Authorization: token ${GITHUB_TOKEN}\" \\\n            \"https://api.github.com/repos/{{ owner }}/{{ repo }}\"\n        additionalInstructions: |\n          Extract relevant repository information like description, language, last update.\n          Check for any security alerts or issues.\n\n      - name: get_recent_commits\n        description: \"Get recent commits from a repository\"\n        command: |\n          curl -s -H \"Authorization: token ${GITHUB_TOKEN}\" \\\n            \"https://api.github.com/repos/{{ owner }}/{{ repo }}/commits?per_page={{ limit | default(10) }}\"\n        additionalInstructions: |\n          Show commit messages, authors, and timestamps.\n          Look for patterns that might relate to the current issue.\n\n      - name: search_issues\n        description: \"Search for issues in a repository\"\n        command: |\n          curl -s -H \"Authorization: token ${GITHUB_TOKEN}\" \\\n            \"https://api.github.com/search/issues?q=repo:{{ owner }}/{{ repo }}+{{ search_query }}\"\n        additionalInstructions: |\n          Find relevant issues that might be related to the current problem.\n          Include issue titles, states, and URLs.\n</code></pre> <p>Environment Variables:</p> <pre><code>export GITHUB_TOKEN=\"your-github-personal-access-token\"\n</code></pre> <p>Run HolmesGPT:</p> <pre><code>holmes ask \"check recent commits in robusta-dev/robusta repository\" --toolsets=toolsets.yaml\n</code></pre> <p>After making changes to your toolsets file, run: <pre><code>holmes toolset refresh\n</code></pre></p> <p>Helm Values:</p> <pre><code>holmes:\n  customToolsets:\n    github:\n      description: \"Fetch information from GitHub repositories\"\n      prerequisites: \"GitHub API token with repository access\"\n      tags: [source-control, github]\n      installation: |\n        1. Create a GitHub personal access token\n        2. Set the token as an environment variable\n        3. Ensure network access to GitHub API\n      tools:\n        - name: get_repository_info\n          description: \"Get information about a GitHub repository\"\n          command: |\n            curl -s -H \"Authorization: token {{ github_token }}\" \\\n              \"https://api.github.com/repos/{{ owner }}/{{ repo }}\"\n          additionalInstructions: |\n            Extract relevant repository information like description, language, last update.\n            Check for any security alerts or issues.\n\n        - name: get_recent_commits\n          description: \"Get recent commits from a repository\"\n          command: |\n            curl -s -H \"Authorization: token {{ github_token }}\" \\\n              \"https://api.github.com/repos/{{ owner }}/{{ repo }}/commits?per_page={{ limit | default(10) }}\"\n          additionalInstructions: |\n            Show commit messages, authors, and timestamps.\n            Look for patterns that might relate to the current issue.\n\n        - name: search_issues\n          description: \"Search for issues in a repository\"\n          command: |\n            curl -s -H \"Authorization: token {{ github_token }}\" \\\n              \"https://api.github.com/search/issues?q=repo:{{ owner }}/{{ repo }}+{{ search_query }}\"\n          additionalInstructions: |\n            Find relevant issues that might be related to the current problem.\n            Include issue titles, states, and URLs.\n</code></pre> <p>Environment Variables:</p> <pre><code>export GITHUB_TOKEN=\"your-github-personal-access-token\"\n</code></pre> <p>Helm Upgrade:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre>"},{"location":"data-sources/custom-toolsets/#reference","title":"Reference","text":""},{"location":"data-sources/custom-toolsets/#toolset-configuration","title":"Toolset Configuration","text":"<p>A custom toolset consists of the following components:</p> <pre><code>toolsets:\n  &lt;toolset-name&gt;:\n    description: \"Human-readable description\"\n    prerequisites: \"What needs to be installed/configured\"\n    tags: [tag1, tag2]  # Optional: for categorization\n    installation: |\n      Multi-line installation instructions\n    tools:\n      - name: tool_name\n        description: \"What this tool does\"\n        command: |\n          Command or script to execute\n        parameters:  # Optional: can be inferred by LLM\n          - name: param_name\n            description: \"Parameter description\"\n        additionalInstructions: |\n          Instructions for post-processing the command output\n</code></pre>"},{"location":"data-sources/custom-toolsets/#tool-configuration","title":"Tool Configuration","text":"<p>Each tool within a toolset can be configured with:</p> <ul> <li>name: Unique identifier for the tool</li> <li>description: What the tool does (visible to the AI)</li> <li>command: Shell command or script to execute</li> <li>parameters: Optional parameter definitions (usually inferred)</li> <li>additionalInstructions: How to interpret/process the output</li> </ul>"},{"location":"data-sources/custom-toolsets/#variable-syntax","title":"Variable Syntax","text":"<p>HolmesGPT supports two types of variables in commands:</p> <ul> <li><code>{{ variable }}</code>: Dynamic variables inferred by the LLM based on context</li> <li><code>${VARIABLE}</code>: Environment variables (not visible to the LLM)</li> </ul>"},{"location":"data-sources/custom-toolsets/#tags","title":"Tags","text":"<p>Optional tags help categorize toolsets:</p> <ul> <li>core: Essential system tools</li> <li>cluster: Cluster-specific tools</li> <li>monitoring: Observability tools</li> <li>networking: Network-related tools</li> <li>storage: Storage-related tools</li> </ul>"},{"location":"data-sources/custom-toolsets/#advanced-adding-custom-binaries","title":"Advanced: Adding Custom Binaries","text":"<p>If your custom toolset requires additional binaries not available in the base HolmesGPT image, you can extend the Docker image:</p>"},{"location":"data-sources/custom-toolsets/#create-a-custom-dockerfile","title":"Create a Custom Dockerfile","text":"<pre><code>FROM us-central1-docker.pkg.dev/genuine-flight-317411/devel/holmes:latest\n\n# Install additional tools\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    your-custom-tool \\\n    another-binary \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy custom scripts\nCOPY scripts/ /usr/local/bin/\n\n# Make scripts executable\nRUN chmod +x /usr/local/bin/*.sh\n</code></pre>"},{"location":"data-sources/custom-toolsets/#build-and-push-your-image","title":"Build and Push Your Image","text":"<pre><code>docker build -t your-registry/holmes-custom:latest .\ndocker push your-registry/holmes-custom:latest\n</code></pre>"},{"location":"data-sources/custom-toolsets/#use-custom-image-in-helm-values","title":"Use Custom Image in Helm Values","text":"<pre><code>holmes:\n  image:\n    repository: your-registry/holmes-custom\n    tag: latest\n  customToolsets:\n    # Your custom toolset configuration\n</code></pre> <p>This approach allows you to include any additional tools or dependencies your custom toolsets might need.</p>"},{"location":"data-sources/permissions/","title":"Adding Permissions for Additional Resources (In-Cluster Deployments)","text":"<p>In-Cluster Only</p> <p>This page applies only to HolmesGPT running inside a Kubernetes cluster via Helm. For local CLI deployments, permissions are managed through your kubeconfig file.</p> <p>HolmesGPT may require access to additional Kubernetes resources or CRDs for specific analyses. Permissions can be extended by modifying the ClusterRole rules. The default configuration has limited resource access.</p>"},{"location":"data-sources/permissions/#common-scenarios-for-adding-permissions","title":"Common Scenarios for Adding Permissions","text":"<ol> <li>External Integrations and CRDs - Access to custom resources from ArgoCD, Istio, etc.</li> <li>Additional Kubernetes resources - Resources not included in the default permissions</li> </ol>"},{"location":"data-sources/permissions/#example-scenario-adding-argo-cd-permissions","title":"Example Scenario: Adding Argo CD Permissions","text":"<p>To enable HolmesGPT to analyze ArgoCD applications and projects, you need to add permissions for ArgoCD custom resources.</p>"},{"location":"data-sources/permissions/#steps-to-add-permissions","title":"Steps to Add Permissions","text":"Holmes Helm ChartRobusta Helm Chart <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>customClusterRoleRules:\n  - apiGroups: [\"argoproj.io\"]\n    resources: [\"applications\", \"appprojects\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml\n</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code> (note: add the <code>holmes:</code> prefix):</p> <pre><code>enableHolmesGPT: true\nholmes:\n  customClusterRoleRules:\n    - apiGroups: [\"argoproj.io\"]\n      resources: [\"applications\", \"appprojects\"]\n      verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre>"},{"location":"data-sources/permissions/#key-benefits","title":"Key Benefits","text":"<ul> <li>Enables HolmesGPT to analyze specific Kubernetes resources</li> <li>Allows interaction with custom resources and CRDs</li> <li>Provides more comprehensive troubleshooting capabilities</li> </ul> <p>The configuration provides flexibility to extend HolmesGPT's permissions to suit specific cluster and tooling requirements.</p>"},{"location":"data-sources/remote-mcp-servers/","title":"Remote MCP Servers","text":"<p>Warning</p> <p>Remote MCP servers are in Tech Preview stage.</p> <p>HolmesGPT can integrate with remote MCP servers using SSE mode. This capability enables HolmesGPT to access external data sources and tools in real time. This guide provides step-by-step instructions for configuring HolmesGPT to connect with remote MCP servers over SSE.</p>"},{"location":"data-sources/remote-mcp-servers/#example-mcp-server-configuration","title":"Example: MCP server configuration","text":"<p> Holmes Helm Chart Robusta Helm Chart <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>mcp_servers:\n  mcp_server_1:\n    # human-readable description of the mcp server (this is not seen by the AI model - its just for users)\n    description: \"Remote mcp server\"\n    url: \"http://example.com:8000/sse\"\n\n  mcp_server_2:\n    description: \"MCP server that runs in my cluster\"\n    url: \"http://&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local:&lt;service-port&gt;\"\n    config:\n      headers:\n        key: \"{{ env.my_mcp_server_key }}\" # You can use holmes environment variables as headers for the MCP server requests.</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code> (note: add the <code>holmes:</code> prefix):</p> <pre><code>enableHolmesGPT: true\nholmes:\n  mcp_servers:\n    mcp_server_1:\n      # human-readable description of the mcp server (this is not seen by the AI model - its just for users)\n      description: \"Remote mcp server\"\n      url: \"http://example.com:8000/sse\"\n\n    mcp_server_2:\n      description: \"MCP server that runs in my cluster\"\n      url: \"http://&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local:&lt;service-port&gt;\"\n      config:\n        headers:\n          key: \"{{ env.my_mcp_server_key }}\" # You can use holmes environment variables as headers for the MCP server requests.</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;</code></pre> </p>"},{"location":"data-sources/remote-mcp-servers/#example-working-with-stdio-mcp-servers","title":"Example: Working with Stdio MCP servers","text":"<p>MCP currently supports three transport mechanisms: stdio, Server-Sent Events (SSE), and Streamable HTTP. At this time, HolmesGPT is compatible only with MCP servers that use SSE. However, many existing MCP servers\u2014such as Dynatrace MCP\u2014rely exclusively on the stdio transport. To overcome this incompatibility, tools like Supergateway can act as a bridge by converting stdio-based MCPs into SSE-compatible endpoints.</p> <p>For this demo we will use: - Dynatrace MCP - Supergateway - runs MCP stdio-based servers over SSE</p> <p>Check out supergatway docs to find out other useful flags.</p> <p>See it in action</p>"},{"location":"data-sources/remote-mcp-servers/#1-run-stdio-mcp-as-sse","title":"1. Run stdio MCP as SSE","text":"DockerKubernetes Pod <p>This command runs the Dynatrace MCP server locally via Docker using Supergateway to wrap it with SSE support. Credentials (e.g., API keys) should be stored in a .env file passed to Docker using --env-file. you can change <code>\"npx -y @dynatrace-oss/dynatrace-mcp-server@latest /\"</code> to your specific MCP.</p> <pre><code>docker run --env-file .env -it --rm -p  8003:8003 supercorp/supergateway \\\n--stdio \"npx -y @dynatrace-oss/dynatrace-mcp-server@latest /\" \\\n--port 8003 \\\n--logLevel debug\n</code></pre> <p>Once the container starts, you should see logs similar to:</p> <pre><code>[supergateway] Starting...\n[supergateway] Supergateway is supported by Supermachine (hosted MCPs) - https://supermachine.ai\n[supergateway]   - outputTransport: sse\n[supergateway]   - Headers: (none)\n[supergateway]   - port: 8003\n[supergateway]   - stdio: npx -y @dynatrace-oss/dynatrace-mcp-server@latest /\n[supergateway]   - ssePath: /sse\n[supergateway]   - messagePath: /message\n[supergateway]   - CORS: disabled\n[supergateway]   - Health endpoints: (none)\n[supergateway] Listening on port 8003\n[supergateway] SSE endpoint: http://localhost:8003/sse\n[supergateway] POST messages: http://localhost:8003/message\n</code></pre> <p>This will run dynatrace MCP server as a pod in your cluster. credentials are passed as env vars.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: dynatrace-mcp\n  labels:\n    app: dynatrace-mcp\nspec:\n  containers:\n    - name: supergateway\n      image: supercorp/supergateway\n      env:\n        - name: DT_ENVIRONMENT\n          value: https://abcd1234.apps.dynatrace.com\n        - name: OAUTH_CLIENT_ID\n          value: dt0s02.SAMPLE\n        - name: OAUTH_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: dynatrace-credentials\n              key: client_secret\n      ports:\n        - containerPort: 8003\n      args:\n        - \"--stdio\"\n        - \"npx -y @dynatrace-oss/dynatrace-mcp-server@latest /\"\n        - \"--port\"\n        - \"8003\"\n        - \"--logLevel\"\n        - \"debug\"\n      stdin: true\n      tty: true\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: dynatrace-mcp\nspec:\n  selector:\n    app: dynatrace-mcp\n  ports:\n    - protocol: TCP\n      port: 8003\n      targetPort: 8003\n  type: ClusterIP\n</code></pre>"},{"location":"data-sources/remote-mcp-servers/#2-add-mcp-server-to-holmes-config","title":"2. Add MCP server to holmes config","text":"<p>With the MCP server running in SSE mode, we need to let HolmesGPT know of the mcp server. Use this config according to your use case.</p> <p>Configuration:</p> Holmes CLI <p>Use a config file, and pass it when running cli commands.</p> <p>custom_toolset.yaml:</p> <pre><code>mcp_servers:\n  mcp_server_1:\n    description: \"Dynatrace observability platform. Bring real-time observability data directly into your development workflow.\"\n    url: \"http://localhost:8003/sse\"\n</code></pre> <p>You can now use Holmes via the CLI with your configured MCP server. For example:</p> <pre><code>holmes ask -t custom_toolset.yaml  \"Using dynatrace what issues do I have in my cluster?\"\n</code></pre> <p>Alternatively, you can add the <code>mcp_servers</code> configurations to ** ~/.holmes/config.yaml**, and run:</p> <pre><code>holmes ask \"Using dynatrace what issues do I have in my cluster?\"\n</code></pre> <p> Holmes Helm Chart Robusta Helm Chart <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>mcp_servers:\n  mcp_server_1:\n    description: \"Dynatrace observability platform. Bring real-time observability data directly into your development workflow.\"\n    url: \"http://dynatrace-mcp.default.svc.cluster.local:8003\"</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code> (note: add the <code>holmes:</code> prefix):</p> <pre><code>enableHolmesGPT: true\nholmes:\n  mcp_servers:\n    mcp_server_1:\n      description: \"Dynatrace observability platform. Bring real-time observability data directly into your development workflow.\"\n      url: \"http://dynatrace-mcp.default.svc.cluster.local:8003\"</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;</code></pre> </p> <p>After the deployment is complete, you can use HolmesGPT and ask questions like Using dynatrace what issues do I have in my cluster?.</p>"},{"location":"data-sources/builtin-toolsets/","title":"Built-in Toolsets","text":"<p>HolmesGPT includes pre-built integrations for popular monitoring and observability tools. Some work automatically with Kubernetes, while others require API keys or configuration.</p>"},{"location":"data-sources/builtin-toolsets/#available-toolsets","title":"Available Toolsets","text":"<ul> <li> AKS Node Health</li> <li> ArgoCD</li> <li> AWS</li> <li> Azure Kubernetes Service</li> <li> Azure SQL Database</li> <li> Confluence</li> <li> Coralogix logs</li> <li> Datadog</li> <li> Datetime</li> <li> Docker</li> <li> GitHub</li> <li> Helm</li> <li> Internet</li> <li> Kafka</li> <li> Kubernetes</li> <li> Loki</li> <li> MongoDB Atlas</li> <li> New Relic</li> <li> Notion</li> <li> OpenSearch logs</li> <li> OpenSearch status</li> <li> Prometheus</li> <li> RabbitMQ</li> <li> Robusta</li> <li> ServiceNow</li> <li> Slab</li> <li> Tempo</li> </ul>"},{"location":"data-sources/builtin-toolsets/#getting-started","title":"Getting Started","text":"<ol> <li>Choose toolsets that match your infrastructure (Prometheus, Grafana, etc.)</li> <li>Configure authentication - some need API keys, others work automatically</li> <li>Run a test investigation to verify data access</li> </ol> <p>\ud83d\udca1 Tip: Start with Kubernetes and Prometheus for basic cluster monitoring.</p>"},{"location":"data-sources/builtin-toolsets/aks-node-health/","title":"AKS Node Health","text":"<p>By enabling this toolset, HolmesGPT will be able to perform specialized health checks and troubleshooting for Azure Kubernetes Service (AKS) nodes, including node-specific diagnostics and performance analysis.</p>"},{"location":"data-sources/builtin-toolsets/aks-node-health/#prerequisites","title":"Prerequisites","text":"<ol> <li>Azure CLI installed and configured</li> <li>Appropriate Azure RBAC permissions for AKS clusters</li> <li>Access to the target AKS cluster</li> <li>Node-level access permissions</li> </ol>"},{"location":"data-sources/builtin-toolsets/aks-node-health/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, ensure you're authenticated with Azure:</p> <pre><code>az login\naz account set --subscription \"&lt;your subscription id&gt;\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  aks/node-health:\n    enabled: true\n    config:\n      subscription_id: \"&lt;your Azure subscription ID&gt;\"\n      resource_group: \"&lt;your AKS resource group&gt;\"\n      cluster_name: \"&lt;your AKS cluster name&gt;\"\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <pre><code>holmes:\n  toolsets:\n    aks/node-health:\n      enabled: true\n      config:\n        subscription_id: \"&lt;your Azure subscription ID&gt;\"\n        resource_group: \"&lt;your AKS resource group&gt;\"\n        cluster_name: \"&lt;your AKS cluster name&gt;\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/aks-node-health/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can configure additional health check parameters:</p> <pre><code>toolsets:\n  aks/node-health:\n    enabled: true\n    config:\n      subscription_id: \"&lt;your Azure subscription ID&gt;\"\n      resource_group: \"&lt;your AKS resource group&gt;\"\n      cluster_name: \"&lt;your AKS cluster name&gt;\"\n      health_check_interval: 300  # Health check interval in seconds\n      max_unhealthy_nodes: 3  # Maximum number of unhealthy nodes to report\n</code></pre>"},{"location":"data-sources/builtin-toolsets/aks-node-health/#capabilities","title":"Capabilities","text":"Tool Name Description aks_check_node_health Perform comprehensive health checks on AKS nodes aks_get_node_metrics Get detailed metrics for AKS nodes aks_diagnose_node_issues Diagnose common node-level issues aks_check_node_readiness Check if nodes are ready and schedulable aks_get_node_events Get events related to specific nodes aks_check_node_resources Check resource utilization on nodes"},{"location":"data-sources/builtin-toolsets/aks/","title":"Azure Kubernetes Service (AKS)","text":"<p>By enabling this toolset, HolmesGPT will be able to interact with Azure Kubernetes Service clusters, providing Azure-specific troubleshooting capabilities and cluster management.</p>"},{"location":"data-sources/builtin-toolsets/aks/#prerequisites","title":"Prerequisites","text":"<ol> <li>Azure CLI installed and configured</li> <li>Appropriate Azure RBAC permissions for AKS clusters</li> <li>Access to the target AKS cluster</li> </ol>"},{"location":"data-sources/builtin-toolsets/aks/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, ensure you're authenticated with Azure:</p> <pre><code>az login\naz account set --subscription \"&lt;your subscription id&gt;\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  aks/core:\n    enabled: true\n    config:\n      subscription_id: \"&lt;your Azure subscription ID&gt;\" # Optional\n      resource_group: \"&lt;your AKS resource group&gt;\" # Optional\n      cluster_name: \"&lt;your AKS cluster name&gt;\" # Optional\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <pre><code>holmes:\n  toolsets:\n    aks/core:\n      enabled: true\n      config:\n        subscription_id: \"&lt;your Azure subscription ID&gt;\"\n        resource_group: \"&lt;your AKS resource group&gt;\"\n        cluster_name: \"&lt;your AKS cluster name&gt;\"\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/aks/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can configure additional Azure settings:</p> <pre><code>toolsets:\n  aks/core:\n    enabled: true\n    config:\n      subscription_id: \"&lt;your Azure subscription ID&gt;\"\n      resource_group: \"&lt;your AKS resource group&gt;\"\n      cluster_name: \"&lt;your AKS cluster name&gt;\"\n      location: \"eastus\"  # Azure region\n      timeout: 60  # Request timeout in seconds\n</code></pre>"},{"location":"data-sources/builtin-toolsets/aks/#capabilities","title":"Capabilities","text":"Tool Name Description aks_get_cluster_info Get detailed information about the AKS cluster aks_get_node_pools List and describe AKS node pools aks_get_cluster_credentials Get cluster credentials for kubectl access aks_scale_node_pool Scale a specific node pool aks_get_cluster_logs Fetch AKS cluster logs aks_get_addon_status Get status of AKS addons"},{"location":"data-sources/builtin-toolsets/argocd/","title":"ArgoCD","text":"<p>By enabling this toolset, HolmesGPT will be able to fetch the status, deployment history, and configuration of ArgoCD applications.</p> <p></p>"},{"location":"data-sources/builtin-toolsets/argocd/#prerequisites","title":"Prerequisites","text":""},{"location":"data-sources/builtin-toolsets/argocd/#generating-an-argocd-token","title":"Generating an ArgoCD token","text":"<p>This toolset requires an <code>ARGOCD_AUTH_TOKEN</code> environment variable. Generate an auth token by following these steps.</p>"},{"location":"data-sources/builtin-toolsets/argocd/#adding-a-read-only-policy-to-argocd","title":"Adding a Read-only Policy to ArgoCD","text":"<p>HolmesGPT requires specific permissions to access ArgoCD data. Add the permissions below to your ArgoCD RBAC configuration.</p> <p>Edit the RBAC ConfigMap: <code>kubectl edit configmap argocd-rbac-cm -n argocd</code></p> <pre><code># Add this to the data section of your argocd-rbac-cm configmap.\n# Creates a 'holmesgpt' user with read-only permissions for troubleshooting.\ndata:\n  policy.default: role:readonly\n  policy.csv: |\n    p, role:admin, *, *, *, allow\n    p, role:admin, accounts, apiKey, *, allow\n    p, holmesgpt, accounts, apiKey, holmesgpt, allow\n    p, holmesgpt, projects, get, *, allow\n    p, holmesgpt, applications, get, *, allow\n    p, holmesgpt, repositories, get, *, allow\n    p, holmesgpt, clusters, get, *, allow\n    p, holmesgpt, applications, manifests, */*, allow\n    p, holmesgpt, applications, resources, */*, allow\n    g, admin, role:admin\n</code></pre>"},{"location":"data-sources/builtin-toolsets/argocd/#configuration","title":"Configuration","text":"<p>In addition to setting permissions and generating an auth token, you will need to tell HolmesGPT how to connect to the server. This can be done two ways:</p> <ol> <li>Using port forwarding. This is the recommended approach if your ArgoCD is inside your Kubernetes cluster.</li> <li>Setting the env var <code>ARGOCD_SERVER</code>. This is the recommended approach if your ArgoCD is reachable through a public DNS.</li> </ol>"},{"location":"data-sources/builtin-toolsets/argocd/#1-port-forwarding","title":"1. Port Forwarding","text":"<p>This is the recommended approach if your ArgoCD is inside your Kubernetes cluster.</p> <p>HolmesGPT needs permission to establish a port-forward to ArgoCD. The configuration below includes that authorization.</p> Holmes CLIRobusta Helm Chart <p>Set the following environment variables:</p> <pre><code>export ARGOCD_AUTH_TOKEN=\"&lt;your-argocd-token&gt;\"\nexport ARGOCD_OPTS=\"--port-forward --port-forward-namespace &lt;your_argocd_namespace&gt; --server &lt;your_server_address&gt; --grpc-web\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml:</p> <pre><code>toolsets:\n    argocd/core:\n        enabled: true\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <pre><code>holmes:\n    customClusterRoleRules:\n        - apiGroups: [\"\"]\n          resources: [\"pods/portforward\"]\n          verbs: [\"create\"]\n    additionalEnvVars:\n        - name: ARGOCD_AUTH_TOKEN\n          value: \"&lt;your-argocd-token&gt;\"\n        - name: ARGOCD_OPTS\n          value: \"--port-forward --port-forward-namespace &lt;your_argocd_namespace&gt; --server &lt;your_server_address&gt; --grpc-web\"\n    toolsets:\n        argocd/core:\n            enabled: true\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p> <p>Note</p> <p>For in-cluster address, use the cluster DNS. For example: <code>--port-forward --port-forward-namespace argocd --server argocd-server.argocd.svc.cluster.local --insecure --grpc-web</code></p> <ul> <li>Add <code>--insecure</code> to work with self-signed certificates</li> <li>Change the namespace <code>--port-forward-namespace &lt;your_argocd_namespace&gt;</code> to the namespace in which your ArgoCD service is deployed</li> <li>The option <code>--grpc-web</code> in <code>ARGOCD_OPTS</code> prevents some connection errors from leaking into the tool responses and provides a cleaner output for HolmesGPT</li> </ul>"},{"location":"data-sources/builtin-toolsets/argocd/#2-server-url","title":"2. Server URL","text":"<p>This is the recommended approach if your ArgoCD is reachable through a public DNS.</p> Holmes CLIRobusta Helm Chart <p>Set the following environment variables:</p> <pre><code>export ARGOCD_AUTH_TOKEN=\"&lt;your-argocd-token&gt;\"\nexport ARGOCD_SERVER=\"argocd.example.com\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml:</p> <pre><code>toolsets:\n    argocd/core:\n        enabled: true\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <p>To test, run:</p> <pre><code>holmes ask \"Which ArgoCD applications are failing and why?\"\n</code></pre> <pre><code>holmes:\n    additionalEnvVars:\n        - name: ARGOCD_AUTH_TOKEN\n          value: \"&lt;your-argocd-token&gt;\"\n        - name: ARGOCD_SERVER\n          value: \"argocd.example.com\"\n    toolsets:\n        argocd/core:\n            enabled: true\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p> <p>Note</p> <p>In production, always use a Kubernetes secret instead of hardcoding the token value in your Helm values.</p>"},{"location":"data-sources/builtin-toolsets/argocd/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description argocd_app_list List the applications in ArgoCD argocd_app_get Retrieve information about an existing application, such as its status and configuration argocd_app_manifests Retrieve manifests for an application argocd_app_resources List resources of an application argocd_app_diff Display the differences between the current state of an application and the desired state specified in its Git repository argocd_app_history List the deployment history of an application in ArgoCD argocd_repo_list List all the Git repositories that ArgoCD is currently managing argocd_proj_list List all available projects argocd_proj_get Retrieve information about an existing project, such as its applications and policies argocd_cluster_list List all known clusters"},{"location":"data-sources/builtin-toolsets/aws/","title":"AWS","text":""},{"location":"data-sources/builtin-toolsets/aws/#security","title":"Security","text":"<p>Set of tools to audit AWS CloudTrail events and audit logs.</p>"},{"location":"data-sources/builtin-toolsets/aws/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, add the following environment variables:</p> <pre><code>export AWS_ACCESS_KEY_ID=\"&lt;your AWS access key ID&gt;\"\nexport AWS_SECRET_ACCESS_KEY=\"&lt;your AWS secret access key&gt;\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\n</code></pre> <p>Then, Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n    aws/security:\n        enabled: true\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <p>To test, run:</p> <pre><code>holmes ask \"Are there any security misconfigurations in my signup application, particularly in the database?\"\n</code></pre> <p>This builtin toolset is currently only available in HolmesGPT CLI.</p>"},{"location":"data-sources/builtin-toolsets/aws/#capabilities","title":"Capabilities","text":"Tool Name Description aws_cloudtrail_event_lookup Fetches events of a specified type from AWS CloudTrail along with the users that called them aws_cloudtrail_event_details Fetches and returns full event details for an AWS CloudTrail event in JSON format given an event ID aws_user_audit_logs Fetches audit logs for a specified user from AWS CloudTrail in the past 24 hours. Provide username as was output by aws_event_lookup or aws_event_details"},{"location":"data-sources/builtin-toolsets/aws/#rds","title":"RDS","text":"<p>Read access to Amazon RDS instances, events, and logs.</p>"},{"location":"data-sources/builtin-toolsets/aws/#configuration_1","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Configure RDS access with your AWS credentials and region settings.</p> <p>This builtin toolset is currently only available in HolmesGPT CLI.</p>"},{"location":"data-sources/builtin-toolsets/aws/#capabilities_1","title":"Capabilities","text":"Tool Name Description aws_rds_describe_instances Describe RDS instances aws_rds_events Get RDS events aws_rds_logs Retrieve RDS logs"},{"location":"data-sources/builtin-toolsets/azure-sql/","title":"Azure SQL Database","text":"<p>By enabling this toolset, HolmesGPT can analyze Azure SQL Database performance, health, and operational issues using Azure REST APIs and Query Store data.</p> <p>Warning</p> <p>This toolset is in Experimental stage.</p>"},{"location":"data-sources/builtin-toolsets/azure-sql/#prerequisites","title":"Prerequisites","text":"<ol> <li>Azure SQL Database instance</li> <li>Azure authentication (Service Principal or Azure AD Workload Identity)</li> <li>Appropriate Azure and SQL permissions</li> </ol>"},{"location":"data-sources/builtin-toolsets/azure-sql/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart"},{"location":"data-sources/builtin-toolsets/azure-sql/#azure-ad-workload-identity","title":"Azure AD Workload Identity","text":"<p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  azure/sql:\n    enabled: true\n    config:\n      database:\n        subscription_id: \"your-subscription-id\"\n        resource_group: \"your-resource-group\"\n        server_name: \"your-azure-sql-server-name\"\n        database_name: \"your-azure-sql-database-name\"\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/azure-sql/#service-principal","title":"Service Principal","text":"<pre><code>toolsets:\n  azure/sql:\n    enabled: true\n    config:\n      tenant_id: \"your-tenant-id\"\n      client_id: \"your-client-id\"\n      client_secret: \"your-client-secret\"\n      database:\n        subscription_id: \"your-subscription-id\"\n        resource_group: \"your-resource-group\"\n        server_name: \"your-azure-sql-server-name\"\n        database_name: \"your-azure-sql-database-name\"\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/azure-sql/#azure-ad-workload-identity_1","title":"Azure AD Workload Identity","text":"<pre><code>holmes:\n  toolsets:\n    azure/sql:\n      enabled: true\n      config:\n        database:\n          subscription_id: \"your-subscription-id\"\n          resource_group: \"your-resource-group\"\n          server_name: \"your-azure-sql-server-name\"\n          database_name: \"your-azure-sql-database-name\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/azure-sql/#service-principal_1","title":"Service Principal","text":"<pre><code>holmes:\n  toolsets:\n    azure/sql:\n      enabled: true\n      config:\n        tenant_id: \"your-tenant-id\"\n        client_id: \"your-client-id\"\n        client_secret: \"your-client-secret\"\n        database:\n          subscription_id: \"your-subscription-id\"\n          resource_group: \"your-resource-group\"\n          server_name: \"your-azure-sql-server-name\"\n          database_name: \"your-azure-sql-database-name\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/azure-sql/#roles-access-controls","title":"Roles / Access controls","text":"<p>The service principal requires these roles:</p>"},{"location":"data-sources/builtin-toolsets/azure-sql/#azure","title":"Azure","text":"<pre><code>Azure Level (RBAC):\n\u251c\u2500\u2500 Monitoring Reader (subscription)\n\u251c\u2500\u2500 SQL DB Contributor (resource group)\n</code></pre>"},{"location":"data-sources/builtin-toolsets/azure-sql/#sql","title":"SQL","text":"<pre><code>Database Level (SQL permissions):\n\u251c\u2500\u2500 CREATE USER [holmes-service-principal] FROM EXTERNAL PROVIDER\n\u251c\u2500\u2500 GRANT VIEW SERVER STATE TO [holmes-service-principal]\n\u2514\u2500\u2500 ALTER ROLE db_datareader ADD MEMBER [holmes-service-principal]\n</code></pre>"},{"location":"data-sources/builtin-toolsets/azure-sql/#query-store","title":"Query Store","text":"<p>In addition, Query Store should be enabled on target databases</p>"},{"location":"data-sources/builtin-toolsets/azure-sql/#capabilities","title":"Capabilities","text":"Tool Name Description analyze_database_health_status Analyze overall database health and status analyze_database_performance Analyze database performance metrics analyze_database_connections Analyze database connection patterns and issues analyze_database_storage Analyze database storage usage and growth get_top_cpu_queries Get queries with highest CPU usage get_slow_queries Get slowest performing queries get_top_data_io_queries Get queries with highest data I/O usage get_top_log_io_queries Get queries with highest log I/O usage get_active_alerts Get active alerts for the database analyze_connection_failures Analyze connection failure patterns"},{"location":"data-sources/builtin-toolsets/bash/","title":"Bash Toolset","text":"<p>The bash toolset provides secure execution of common command-line tools used for troubleshooting and system analysis. It replaces multiple YAML-based toolsets with a single, comprehensive toolset that includes safety validation and command parsing.</p> <p>\u26a0\ufe0f Security Note: This toolset executes commands on the system where Holmes is running. Only validated, safe commands are allowed, and the toolset is disabled by default for security reasons.</p>"},{"location":"data-sources/builtin-toolsets/bash/#supported-commands","title":"Supported Commands","text":"<p>The bash toolset supports the following categories of commands:</p>"},{"location":"data-sources/builtin-toolsets/bash/#cloud-providers","title":"Cloud Providers","text":"<p>AWS CLI (<code>aws</code>)</p> <ul> <li>Supports various AWS services and operations</li> <li>Commands are validated for safety before execution</li> </ul> <p>Azure CLI (<code>az</code>)</p> <ul> <li>Supports Azure operations including AKS management</li> <li>Network and account operations</li> </ul>"},{"location":"data-sources/builtin-toolsets/bash/#kubernetes-tools","title":"Kubernetes Tools","text":"<p>kubectl</p> <ul> <li>Standard Kubernetes operations: get, describe, logs, events</li> <li>Resource management and cluster inspection</li> <li>Live metrics via <code>kubectl top</code></li> </ul> <p>Helm</p> <ul> <li>Helm chart operations</li> <li>Repository management</li> <li>Release inspection</li> </ul> <p>ArgoCD</p> <ul> <li>Application management</li> <li>Deployment status checking</li> </ul>"},{"location":"data-sources/builtin-toolsets/bash/#container-tools","title":"Container Tools","text":"<p>Docker</p> <ul> <li>Container inspection and management</li> <li>Image operations</li> <li>Basic Docker commands</li> </ul>"},{"location":"data-sources/builtin-toolsets/bash/#text-processing-utilities","title":"Text Processing Utilities","text":"<p>Data Processing</p> <ul> <li><code>grep</code> - Text searching and pattern matching</li> <li><code>jq</code> - JSON processing and querying</li> <li><code>sed</code> - Stream editing and text transformation</li> <li><code>awk</code> - Pattern scanning and text processing</li> </ul> <p>File Utilities</p> <ul> <li><code>cut</code> - Column extraction</li> <li><code>sort</code> - Data sorting</li> <li><code>uniq</code> - Duplicate removal</li> <li><code>head</code> - Show first lines</li> <li><code>tail</code> - Show last lines</li> <li><code>wc</code> - Word, line, and character counting</li> </ul> <p>Text Transformation</p> <ul> <li><code>tr</code> - Character translation and deletion</li> <li><code>base64</code> - Base64 encoding/decoding</li> </ul>"},{"location":"data-sources/builtin-toolsets/bash/#special-tools","title":"Special Tools","text":"<p>kubectl_run_image</p> <p>Creates temporary debug pods in Kubernetes clusters for diagnostic commands:</p> <ul> <li>Runs commands in specified container images</li> <li>Automatically cleans up temporary pods</li> <li>Supports custom namespaces and timeouts</li> <li>Useful for network debugging, DNS resolution, and environment inspection</li> </ul>"},{"location":"data-sources/builtin-toolsets/bash/#command-validation","title":"Command Validation","text":"<p>All commands undergo security validation before execution:</p> <ul> <li>Only whitelisted commands and options are allowed</li> <li>Dangerous operations are blocked (file writes, system calls, etc.)</li> <li>Commands are parsed and validated for safety</li> <li>Pipe operations between supported commands are allowed</li> </ul>"},{"location":"data-sources/builtin-toolsets/confluence/","title":"Confluence","text":"<p>By enabling this toolset, HolmesGPT will be able to fetch Confluence pages. This is particularly useful if you store runbooks in Confluence and want Holmes to run investigations using these runbooks.</p> <p>This toolset requires an Atlassian API Key.</p>"},{"location":"data-sources/builtin-toolsets/confluence/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Set the following environment variables and the Confluence toolset will be automatically enabled:</p> <pre><code>export CONFLUENCE_USER=\"&lt;Confluence username&gt;\"\nexport CONFLUENCE_API_KEY=\"&lt;Confluence API key&gt;\"\nexport CONFLUENCE_BASE_URL=\"&lt;Confluence base URL&gt;\"\n</code></pre> <p>To test, run:</p> <pre><code>holmes ask \"why is my application failing? Get relevant runbooks from Confluence\"\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <p>Helm Values:</p> <pre><code>holmes:\n    additionalEnvVars:\n        - name: CONFLUENCE_USER\n          value: &lt;Confluence username&gt;\n        - name: CONFLUENCE_API_KEY\n          value: &lt;Confluence API key&gt;\n        - name: CONFLUENCE_BASE_URL\n          value: &lt;Confluence base URL&gt;\n    toolsets:\n        confluence:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/confluence/#capabilities","title":"Capabilities","text":"Tool Name Description fetch_confluence_url Fetch a page in Confluence. Use this to fetch Confluence runbooks if they are present before starting your investigation."},{"location":"data-sources/builtin-toolsets/coralogix-logs/","title":"Coralogix logs","text":"<p>By enabling this toolset, HolmesGPT will fetch pod logs from Coralogix.</p> Important: Disable Default Logging Toolset <p>Only one logging toolset should be enabled at a time. If you enable this toolset, disable the default <code>kubernetes/logs</code> toolset.</p> <p>Available Log Sources:</p> <ul> <li>Kubernetes logs - Direct pod log access (enabled by default)</li> <li>Loki - Centralized logs via Loki</li> <li>OpenSearch logs - Logs from OpenSearch/Elasticsearch</li> <li>Coralogix logs - Logs via Coralogix platform</li> <li>DataDog - Logs from DataDog</li> </ul> <p>\ud83d\udca1 Choose one: Only enable one logging toolset at a time for best performance.</p>"},{"location":"data-sources/builtin-toolsets/coralogix-logs/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Coralogix API key which is assigned the <code>DataQuerying</code> permission preset</li> <li>A Coralogix domain. For example <code>eu2.coralogix.com</code></li> <li>Your team's name or hostname. For example <code>your-company-name</code></li> </ol> <p>You can deduce the <code>domain</code> and <code>team_hostname</code> configuration fields by looking at the URL you use to access the Coralogix UI.</p> <p>For example if you access Coralogix at <code>https://my-team.app.eu2.coralogix.com/</code> then the <code>team_hostname</code> is <code>my-team</code> and the Coralogix <code>domain</code> is <code>eu2.coralogix.com</code>.</p>"},{"location":"data-sources/builtin-toolsets/coralogix-logs/#configuration","title":"Configuration","text":"<p> Holmes CLI Holmes Helm Chart Robusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  coralogix/logs:\n    enabled: true\n    config:\n      api_key: \"&lt;your Coralogix API key&gt;\"\n      domain: \"eu2.coralogix.com\"\n      team_hostname: \"your-company-name\"\n\n  kubernetes/logs:\n    enabled: false  # Disable default Kubernetes logging</code></pre> <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>toolsets:\n  coralogix/logs:\n    enabled: true\n    config:\n      api_key: \"&lt;your Coralogix API key&gt;\"\n      domain: \"eu2.coralogix.com\"\n      team_hostname: \"your-company-name\"\n\n  kubernetes/logs:\n    enabled: false  # Disable default Kubernetes logging</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code>:</p> <pre><code>holmes:\n  toolsets:\n    coralogix/logs:\n      enabled: true\n      config:\n        api_key: \"&lt;your Coralogix API key&gt;\"\n        domain: \"eu2.coralogix.com\"\n        team_hostname: \"your-company-name\"\n\n    kubernetes/logs:\n      enabled: false  # Disable default Kubernetes logging</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;</code></pre> </p>"},{"location":"data-sources/builtin-toolsets/coralogix-logs/#custom-labels-configuration-optional","title":"Custom Labels Configuration (Optional)","text":"<p>By default, the Coralogix toolset expects logs to use standard Kubernetes field names. If your Coralogix deployment uses different field names for Kubernetes metadata, you can customize the label mappings.</p> <p>This is useful when:</p> <ul> <li>Your log ingestion pipeline uses custom field names</li> <li>You have a non-standard Coralogix setup with different metadata fields</li> <li>Your Kubernetes logs are structured differently in Coralogix</li> </ul> <p>To find the correct field names, examine your logs in the Coralogix UI and identify how pod names, namespaces, log messages, and timestamps are labeled.</p>"},{"location":"data-sources/builtin-toolsets/coralogix-logs/#example-with-custom-labels","title":"Example with Custom Labels","text":"<p> Holmes CLI Holmes Helm Chart Robusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  coralogix/logs:\n    enabled: true\n    config:\n      api_key: \"&lt;your Coralogix API key&gt;\"\n      domain: \"eu2.coralogix.com\"\n      team_hostname: \"your-company-name\"\n      labels:\n        namespace: \"resource.attributes.k8s.pod.name\" # Default\n        pod: \"resource.attributes.k8s.namespace.name\" # Default\n        log_message: \"logRecord.body\"                 # Default\n        timestamp: \"logRecord.attributes.time\"        # Default\n\n  kubernetes/logs:\n    enabled: false  # Disable default Kubernetes logging</code></pre> <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>toolsets:\n  coralogix/logs:\n    enabled: true\n    config:\n      api_key: \"&lt;your Coralogix API key&gt;\"\n      domain: \"eu2.coralogix.com\"\n      team_hostname: \"your-company-name\"\n      labels:\n        namespace: \"resource.attributes.k8s.pod.name\" # Default\n        pod: \"resource.attributes.k8s.namespace.name\" # Default\n        log_message: \"logRecord.body\"                 # Default\n        timestamp: \"logRecord.attributes.time\"        # Default\n\n  kubernetes/logs:\n    enabled: false  # Disable default Kubernetes logging</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code>:</p> <pre><code>holmes:\n  toolsets:\n    coralogix/logs:\n      enabled: true\n      config:\n        api_key: \"&lt;your Coralogix API key&gt;\"\n        domain: \"eu2.coralogix.com\"\n        team_hostname: \"your-company-name\"\n        labels:\n          namespace: \"resource.attributes.k8s.pod.name\" # Default\n          pod: \"resource.attributes.k8s.namespace.name\" # Default\n          log_message: \"logRecord.body\"                 # Default\n          timestamp: \"logRecord.attributes.time\"        # Default\n\n    kubernetes/logs:\n      enabled: false  # Disable default Kubernetes logging</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;</code></pre> </p> <p>Label Configuration Fields:</p> <ul> <li><code>namespace</code>: Field path for Kubernetes namespace name</li> <li><code>pod</code>: Field path for Kubernetes pod name</li> <li><code>log_message</code>: Field path for the actual log message content</li> <li><code>timestamp</code>: Field path for log timestamp</li> </ul> <p>All label fields are optional and will use the defaults shown above if not specified.</p>"},{"location":"data-sources/builtin-toolsets/coralogix-logs/#logs-retrieval-strategy-optional","title":"Logs Retrieval Strategy (Optional)","text":"<p>Coralogix stores logs in two tiers with different performance characteristics:</p> <ul> <li>Frequent Search: Fast queries with limited retention</li> <li>Archive: Slower queries but longer retention period</li> </ul> <p>You can configure how HolmesGPT retrieves logs using the <code>logs_retrieval_methodology</code> setting:</p>"},{"location":"data-sources/builtin-toolsets/coralogix-logs/#available-strategies","title":"Available Strategies","text":"<ul> <li><code>ARCHIVE_FALLBACK</code> (default): Try Frequent Search first, fallback to Archive if no results</li> <li><code>FREQUENT_SEARCH_ONLY</code>: Only search Frequent Search tier</li> <li><code>ARCHIVE_ONLY</code>: Only search Archive tier</li> <li><code>BOTH_FREQUENT_SEARCH_AND_ARCHIVE</code>: Search both tiers and merge results</li> <li><code>FREQUENT_SEARCH_FALLBACK</code>: Try Archive first, fallback to Frequent Search if no results</li> </ul>"},{"location":"data-sources/builtin-toolsets/coralogix-logs/#example-configuration","title":"Example Configuration","text":"<p> Holmes CLI Holmes Helm Chart Robusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  coralogix/logs:\n    enabled: true\n    config:\n      api_key: \"&lt;your Coralogix API key&gt;\"\n      domain: \"eu2.coralogix.com\"\n      team_hostname: \"your-company-name\"\n      logs_retrieval_methodology: \"ARCHIVE_FALLBACK\"  # Default</code></pre> <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>toolsets:\n  coralogix/logs:\n    enabled: true\n    config:\n      api_key: \"&lt;your Coralogix API key&gt;\"\n      domain: \"eu2.coralogix.com\"\n      team_hostname: \"your-company-name\"\n      logs_retrieval_methodology: \"ARCHIVE_FALLBACK\"  # Default</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code>:</p> <pre><code>holmes:\n  toolsets:\n    coralogix/logs:\n      enabled: true\n      config:\n        api_key: \"&lt;your Coralogix API key&gt;\"\n        domain: \"eu2.coralogix.com\"\n        team_hostname: \"your-company-name\"\n        logs_retrieval_methodology: \"ARCHIVE_FALLBACK\"  # Default</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;</code></pre> </p> <p>Recommendations:</p> <ul> <li>Use <code>ARCHIVE_FALLBACK</code> for most cases (balances speed and coverage)</li> <li>Use <code>FREQUENT_SEARCH_ONLY</code> when you know Holmes does not need to access the log archive</li> <li>Use <code>ARCHIVE_ONLY</code> if the frequent search logs are always empty</li> <li>Use <code>BOTH_FREQUENT_SEARCH_AND_ARCHIVE</code> for comprehensive log coverage (slower)</li> </ul>"},{"location":"data-sources/builtin-toolsets/coralogix-logs/#capabilities","title":"Capabilities","text":"Tool Name Description coralogix_fetch_logs Fetch logs from Coralogix for specified pods and time ranges"},{"location":"data-sources/builtin-toolsets/datadog/","title":"Datadog","text":"<p>Connect HolmesGPT to Datadog for log analysis and metrics access from your Datadog dashboards.</p> Important: Disable Default Logging Toolset <p>Only one logging toolset should be enabled at a time. If you enable this toolset, disable the default <code>kubernetes/logs</code> toolset.</p> <p>Available Log Sources:</p> <ul> <li>Kubernetes logs - Direct pod log access (enabled by default)</li> <li>Loki - Centralized logs via Loki</li> <li>OpenSearch logs - Logs from OpenSearch/Elasticsearch</li> <li>Coralogix logs - Logs via Coralogix platform</li> <li>DataDog - Logs from DataDog</li> </ul> <p>\ud83d\udca1 Choose one: Only enable one logging toolset at a time for best performance.</p>"},{"location":"data-sources/builtin-toolsets/datadog/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Datadog API key with log access permissions</li> <li>A Datadog Application key</li> </ol> <p>You can find these in your Datadog account under Organization Settings &gt; API Keys and Application Keys.</p>"},{"location":"data-sources/builtin-toolsets/datadog/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, set the following environment variables:</p> <pre><code>export DD_API_KEY=\"your-Datadog-api-key\"\nexport DD_APP_KEY=\"your-Datadog-app-key\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  datadog/logs:\n    enabled: true\n    config:\n      site: \"datadoghq.com\"  # or datadoghq.eu for EU, etc.\n\n  kubernetes/logs:\n    enabled: false  # Disable default Kubernetes logging\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <pre><code>holmes:\n  additionalEnvVars:\n    - name: DD_API_KEY\n      value: \"&lt;your Datadog API key&gt;\"\n    - name: DD_APP_KEY\n      value: \"&lt;your Datadog application key&gt;\"\n  toolsets:\n    datadog/logs:\n      enabled: true\n      config:\n        site: \"datadoghq.com\"  # or datadoghq.eu for EU, etc.\n\n    kubernetes/logs:\n      enabled: false  # Disable default Kubernetes logging\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/datadog/#validation","title":"Validation","text":"<p>Test your configuration:</p> <pre><code>holmes ask \"show me recent application errors\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/datadog/#capabilities","title":"Capabilities","text":"Tool Name Description datadog_fetch_logs Fetch logs from Datadog for specified time ranges and filters datadog_search_logs Search logs in Datadog using query patterns"},{"location":"data-sources/builtin-toolsets/datetime/","title":"Datetime \u2713","text":"<p>Enabled by Default</p> <p>This toolset is enabled by default. You do not need to configure it. You can disable it if you want to but doing so may negatively impact HolmesGPT's ability to investigate issues.</p> <p>By enabling this toolset, HolmesGPT will be able to get the current UTC date and time. This feature should be kept enabled as it can be necessary for other toolsets that rely on dates and time.</p> <p>The following built-in toolsets depend on <code>datetime</code>:</p> <ul> <li>Loki</li> <li>Prometheus</li> <li>Coralogix logs</li> </ul>"},{"location":"data-sources/builtin-toolsets/datetime/#configuration","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        datetime:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/datetime/#capabilities","title":"Capabilities","text":"Tool Name Description get_current_time Return current time information. Useful to build queries that require time information"},{"location":"data-sources/builtin-toolsets/docker/","title":"Docker","text":"<p>Not Recommended for Kubernetes</p> <p>This integration is not recommended for monitoring a Kubernetes cluster because it is neither necessary nor useful. It is documented here for users of HolmesGPT CLI.</p> <p>Read access to Docker resources.</p>"},{"location":"data-sources/builtin-toolsets/docker/#configuration","title":"Configuration","text":"Holmes CLI <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n    docker/core:\n        enabled: true\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/docker/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description docker_images List all Docker images docker_ps List all running Docker containers docker_ps_all List all Docker containers, including stopped ones docker_inspect Inspect detailed information about a Docker container or image docker_logs Fetch the logs of a Docker container docker_top Display the running processes of a container docker_events Get real-time events from the Docker server docker_history Show the history of an image docker_diff Inspect changes to files or directories on a container's filesystem"},{"location":"data-sources/builtin-toolsets/github/","title":"GitHub","text":"<p>By enabling this toolset, HolmesGPT will be able to interact with GitHub repositories, read files, create pull requests, and manage branches. This is useful for accessing runbooks, documentation, or configuration files stored in GitHub.</p>"},{"location":"data-sources/builtin-toolsets/github/#prerequisites","title":"Prerequisites","text":"<ol> <li>A GitHub Personal Access Token with appropriate permissions:</li> <li><code>repo</code> scope for private repositories</li> <li><code>public_repo</code> scope for public repositories</li> <li><code>pull_requests:write</code> for creating PRs</li> </ol> <p>You can create a token at GitHub Settings &gt; Developer settings &gt; Personal access tokens.</p>"},{"location":"data-sources/builtin-toolsets/github/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, set the following environment variable:</p> <pre><code>export GITHUB_TOKEN=\"&lt;your GitHub personal access token&gt;\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  git/github:\n    enabled: true\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <p>To test, run:</p> <pre><code>holmes ask \"Check the troubleshooting guide in our GitHub repository for common deployment issues\"\n</code></pre> <pre><code>holmes:\n  additionalEnvVars:\n    - name: GITHUB_TOKEN\n      value: \"&lt;your GitHub personal access token&gt;\"\n  toolsets:\n    git/github:\n      enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/github/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can configure additional options:</p> <pre><code>toolsets:\n  git/github:\n    enabled: true\n    config:\n      default_branch: \"main\"  # Default branch to use\n      max_file_size: 1048576  # Maximum file size to read (1MB)\n      timeout: 30  # Request timeout in seconds\n</code></pre>"},{"location":"data-sources/builtin-toolsets/github/#capabilities","title":"Capabilities","text":"Tool Name Description github_read_file Read a file from a GitHub repository github_list_files List files in a GitHub repository directory github_search_files Search for files in a repository by name or content github_create_pr Create a new pull request github_update_pr Update an existing pull request github_list_branches List branches in a repository github_create_branch Create a new branch github_get_commit Get details about a specific commit"},{"location":"data-sources/builtin-toolsets/grafanaloki/","title":"Loki","text":"<p>Connect HolmesGPT to Loki for log analysis through Grafana or direct API access. Provides access to historical logs and advanced log queries.</p>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#when-to-use-this","title":"When to Use This","text":"<ul> <li>\u2705 Your Kubernetes logs are centralized in Loki</li> <li>\u2705 You need historical log data beyond what's in pods</li> <li>\u2705 You want advanced log search capabilities</li> </ul>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#prerequisites","title":"Prerequisites","text":"<ul> <li>Loki instance with logs from your Kubernetes cluster</li> <li>Grafana with Loki datasource configured (recommended) OR direct Loki API access</li> </ul> Important: Disable Default Logging Toolset <p>Only one logging toolset should be enabled at a time. If you enable this toolset, disable the default <code>kubernetes/logs</code> toolset.</p> <p>Available Log Sources:</p> <ul> <li>Kubernetes logs - Direct pod log access (enabled by default)</li> <li>Loki - Centralized logs via Loki</li> <li>OpenSearch logs - Logs from OpenSearch/Elasticsearch</li> <li>Coralogix logs - Logs via Coralogix platform</li> <li>DataDog - Logs from DataDog</li> </ul> <p>\ud83d\udca1 Choose one: Only enable one logging toolset at a time for best performance.</p>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#configuration","title":"Configuration","text":"<p>Choose one of the following methods:</p>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#option-1-through-grafana-recommended","title":"Option 1: Through Grafana (Recommended)","text":"<p>Required: - Grafana service account token with Viewer role - Loki datasource UID from Grafana</p> <p>Find your Loki datasource UID: <pre><code># Port forward to Grafana\nkubectl port-forward svc/grafana 3000:80\n\n# Get Loki datasource UID\ncurl -s -u admin:admin http://localhost:3000/api/datasources | jq '.[] | select(.type == \"loki\") | .uid'\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#configuration-grafana-proxy","title":"Configuration (Grafana Proxy)","text":"<p> Holmes CLI Holmes Helm Chart Robusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  grafana/loki:\n    enabled: true\n    config:\n      api_key: &lt;your grafana API key&gt;\n      url: https://xxxxxxx.grafana.net # Your Grafana cloud account URL\n      grafana_datasource_uid: &lt;the UID of the loki data source in Grafana&gt;\n\n  kubernetes/logs:\n    enabled: false # HolmesGPT's default logging mechanism MUST be disabled</code></pre> <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>toolsets:\n  grafana/loki:\n    enabled: true\n    config:\n      api_key: &lt;your grafana API key&gt;\n      url: https://xxxxxxx.grafana.net # Your Grafana cloud account URL\n      grafana_datasource_uid: &lt;the UID of the loki data source in Grafana&gt;\n\n  kubernetes/logs:\n    enabled: false # HolmesGPT's default logging mechanism MUST be disabled</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code>:</p> <pre><code>holmes:\n  toolsets:\n    grafana/loki:\n      enabled: true\n      config:\n        api_key: &lt;your grafana API key&gt;\n        url: https://xxxxxxx.grafana.net # Your Grafana cloud account URL\n        grafana_datasource_uid: &lt;the UID of the loki data source in Grafana&gt;\n\n    kubernetes/logs:\n      enabled: false # HolmesGPT's default logging mechanism MUST be disabled</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;</code></pre> </p>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#direct-connection","title":"Direct Connection","text":"<p>The toolset can directly connect to a Loki instance without proxying through a Grafana instance. This is done by not setting the <code>grafana_datasource_uid</code> field. Not setting this field makes HolmesGPT assume that it is directly connecting to Loki.</p>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#configuration-direct-connection","title":"Configuration (Direct Connection)","text":"<p> Holmes CLI Holmes Helm Chart Robusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  grafana/loki:\n    enabled: true\n    config:\n      url: http://loki.logging\n      headers:\n        X-Scope-OrgID: \"&lt;tenant id&gt;\" # Set the X-Scope-OrgID if loki multitenancy is enabled\n\n  kubernetes/logs:\n    enabled: false # HolmesGPT's default logging mechanism MUST be disabled</code></pre> <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>toolsets:\n  grafana/loki:\n    enabled: true\n    config:\n      url: http://loki.logging\n      headers:\n        X-Scope-OrgID: \"&lt;tenant id&gt;\" # Set the X-Scope-OrgID if loki multitenancy is enabled\n\n  kubernetes/logs:\n    enabled: false # HolmesGPT's default logging mechanism MUST be disabled</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code>:</p> <pre><code>holmes:\n  toolsets:\n    grafana/loki:\n      enabled: true\n      config:\n        url: http://loki.logging\n        headers:\n          X-Scope-OrgID: \"&lt;tenant id&gt;\" # Set the X-Scope-OrgID if loki multitenancy is enabled\n\n    kubernetes/logs:\n      enabled: false # HolmesGPT's default logging mechanism MUST be disabled</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;</code></pre> </p>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"data-sources/builtin-toolsets/grafanaloki/#search-labels","title":"Search Labels","text":"<p>You can tweak the labels used by the toolset to identify Kubernetes resources. This is only needed if your Loki logs settings for <code>pod</code> and <code>namespace</code> differ from the defaults.</p> <p> Holmes CLI Holmes Helm Chart Robusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  grafana/loki:\n    enabled: true\n    config:\n      url: ...\n      labels:\n          pod: \"pod\"\n          namespace: \"namespace\"</code></pre> <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>toolsets:\n  grafana/loki:\n    enabled: true\n    config:\n      url: ...\n      labels:\n          pod: \"pod\"\n          namespace: \"namespace\"</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code>:</p> <pre><code>holmes:\n  toolsets:\n    grafana/loki:\n      enabled: true\n      config:\n        url: ...\n        labels:\n            pod: \"pod\"\n            namespace: \"namespace\"</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;</code></pre> </p> <p>Use the following commands to list Loki's labels and determine which ones to use:</p> <pre><code># Make Loki accessible locally\nkubectl port-forward svc/loki 3100:3100\n\n# List all labels. You may have to add the -H 'X-Scope-OrgID:&lt;org id&gt;' option with a valid org id\ncurl http://localhost:3100/loki/api/v1/labels\n</code></pre>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#capabilities","title":"Capabilities","text":"Tool Name Description fetch_pod_logs Fetches pod logs from Loki"},{"location":"data-sources/builtin-toolsets/grafanatempo/","title":"Tempo","text":"<p>By enabling this toolset, HolmesGPT will be able to fetch trace information from Tempo to debug performance related issues, like high latency in your application.</p>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#proxying-through-grafana","title":"Proxying through Grafana","text":"<p>This is the recommended approach because we intend to add more capabilities to the toolset that are only available with Grafana.</p>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#prerequisites","title":"Prerequisites","text":"<p>A Grafana service account token with the following permissions:</p> <ul> <li>Basic role -&gt; Viewer</li> <li>Data sources -&gt; Reader</li> </ul> <p>Check out this video on creating a Grafana service account token.</p> <p>Getting Grafana URL</p> <p>You can find the Grafana URL required for Tempo in your Grafana cloud account settings.</p> <p>Obtaining the datasource UID</p> <p>You may have multiple Tempo data sources set up in Grafana. HolmesGPT uses a single Tempo datasource to fetch the traces and it needs to know the UID of this datasource.</p> <p>A simple way to get the datasource UID is to access the Grafana API by running the following request:</p> <pre><code># port forward if you are using Robusta's Grafana from your Kubernetes cluster\nkubectl port-forward svc/robusta-grafana 3000:80\n# List the Tempo data sources\ncurl -s -u &lt;username&gt;:&lt;password&gt; http://localhost:3000/api/datasources | jq '.[] | select(.type == \"tempo\")'\n</code></pre> <p>This will return something like:</p> <pre><code>{\n    \"id\": 3,\n    \"uid\": \"klja8hsa-8a9c-4b35-1230-7baab22b02ee\",\n    \"orgId\": 1,\n    \"name\": \"Tempo\",\n    \"type\": \"tempo\",\n    \"typeName\": \"Tempo\",\n    \"typeLogoUrl\": \"/public/app/plugins/datasource/tempo/img/tempo_icon.svg\",\n    \"access\": \"proxy\",\n    \"url\": \"http://tempo-query-frontend.tempo:3100\",\n    \"user\": \"\",\n    \"database\": \"\",\n    \"basicAuth\": false,\n    \"isDefault\": false,\n    \"jsonData\": {\n        \"tlsSkipVerify\": true\n    },\n    \"readOnly\": false\n}\n</code></pre> <p>In this case, the Tempo datasource UID is <code>klja8hsa-8a9c-4b35-1230-7baab22b02ee</code>.</p>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#configuration-grafana-proxy","title":"Configuration (Grafana Proxy)","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  grafana/tempo:\n    enabled: true\n    config:\n      api_key: &lt;your grafana service account token&gt;\n      url: &lt;your grafana url&gt; # e.g. https://acme-corp.grafana.net\n      grafana_datasource_uid: &lt;the UID of the tempo data source in Grafana&gt;\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <p>To test, run:</p> <pre><code>holmes ask \"The payments DB is very slow, check tempo for any trace data\"\n</code></pre> <pre><code>holmes:\n  toolsets:\n    grafana/tempo:\n      enabled: true\n      config:\n        api_key: &lt;your grafana API key&gt;\n        url: &lt;your grafana url&gt; # e.g. https://acme-corp.grafana.net\n        grafana_datasource_uid: &lt;the UID of the tempo data source in Grafana&gt;\n        labels:\n          pod: \"k8s.pod.name\"\n          namespace: \"k8s.namespace.name\"\n          deployment: \"k8s.deployment.name\"\n          node: \"k8s.node.name\"\n          service: \"service.name\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#direct-connection","title":"Direct Connection","text":"<p>The toolset can directly connect to a Tempo instance without proxying through a Grafana instance. This is done by not setting the <code>grafana_datasource_uid</code> field. Not setting this field makes HolmesGPT assume that it is directly connecting to Tempo.</p>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#configuration-direct-connection","title":"Configuration (Direct Connection)","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  grafana/tempo:\n    enabled: true\n    config:\n      url: http://tempo.monitoring\n      headers:\n        X-Scope-OrgID: \"&lt;tenant id&gt;\" # Set the X-Scope-OrgID if tempo multitenancy is enabled\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <pre><code>holmes:\n  toolsets:\n    grafana/tempo:\n      enabled: true\n      config:\n        url: http://tempo.monitoring\n        headers:\n          X-Scope-OrgID: \"&lt;tenant id&gt;\" # Set the X-Scope-OrgID if tempo multitenancy is enabled\n</code></pre>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"data-sources/builtin-toolsets/grafanatempo/#search-labels","title":"Search Labels","text":"<p>You can tweak the labels used by the toolset to identify Kubernetes resources. This is only needed if the trace labels differ from the defaults.</p> Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml:</p> <pre><code>toolsets:\n  grafana/tempo:\n    enabled: true\n    config:\n      url: ...\n      labels:\n        pod: \"k8s.pod.name\"\n        namespace: \"k8s.namespace.name\"\n        deployment: \"k8s.deployment.name\"\n        node: \"k8s.node.name\"\n        service: \"service.name\"\n</code></pre> <pre><code>holmes:\n  toolsets:\n    grafana/tempo:\n      enabled: true\n      config:\n        url: ...\n        labels:\n          pod: \"k8s.pod.name\"\n          namespace: \"k8s.namespace.name\"\n          deployment: \"k8s.deployment.name\"\n          node: \"k8s.node.name\"\n          service: \"service.name\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#capabilities","title":"Capabilities","text":"Tool Name Description fetch_tempo_tags List the tags available in Tempo fetch_tempo_traces Lists Tempo traces. At least one of <code>service_name</code>, <code>pod_name</code>, or <code>deployment_name</code> argument is required. fetch_tempo_trace_by_id Retrieves detailed information about a Tempo trace using its trace ID. Use this to investigate a trace."},{"location":"data-sources/builtin-toolsets/helm/","title":"Helm \u2713","text":"<p>Enabled by Default</p> <p>This toolset is enabled by default. You do not need to configure it. You can disable it if you want to but doing so may negatively impact HolmesGPT's ability to investigate issues.</p> <p>By enabling this toolset, HolmesGPT will be able to provide read access to a cluster's Helm charts and releases.</p>"},{"location":"data-sources/builtin-toolsets/helm/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n    helm/core:\n        enabled: true\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <pre><code>holmes:\n    toolsets:\n        helm/core:\n            enabled: true\n    customClusterRoleRules:\n        - apiGroups: [\"\"]\n          resources: [\"secrets\", \"pods\", \"services\", \"configmaps\", \"persistentvolumeclaims\"]\n          verbs: [\"get\", \"list\", \"watch\"]\n        - apiGroups: [\"\"]\n          resources: [\"namespaces\"]\n          verbs: [\"get\"]\n        - apiGroups: [\"apps\"]\n          resources: [\"deployments\", \"statefulsets\", \"daemonsets\"]\n          verbs: [\"get\", \"list\", \"watch\"]\n        - apiGroups: [\"batch\"]\n          resources: [\"jobs\", \"cronjobs\"]\n          verbs: [\"get\", \"list\", \"watch\"]\n        - apiGroups: [\"networking.k8s.io\"]\n          resources: [\"ingresses\"]\n          verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/helm/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description helm_list Use to get all the current helm releases helm_values Use to gather Helm values or any released helm chart helm_status Check the status of a Helm release helm_history Get the revision history of a Helm release helm_manifest Fetch the generated Kubernetes manifest for a Helm release helm_hooks Get the hooks for a Helm release helm_chart Show the chart used to create a Helm release helm_notes Show the notes provided by the Helm chart"},{"location":"data-sources/builtin-toolsets/internet/","title":"Internet \u2713","text":"<p>Enabled by Default</p> <p>This toolset is enabled by default and should typically remain enabled.</p> <p>By enabling this toolset, HolmesGPT will be able to fetch webpages. This tool is beneficial if you provide Holmes with publicly accessible web-based runbooks.</p>"},{"location":"data-sources/builtin-toolsets/internet/#configuration","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        internet:\n            enabled: true\n            config: # optional\n              additional_headers:\n                Authorization: Bearer ...\n</code></pre>"},{"location":"data-sources/builtin-toolsets/internet/#capabilities","title":"Capabilities","text":"Tool Name Description fetch_webpage Fetch a webpage. Use this to fetch runbooks if they are present before starting your investigation (if no other tool like Confluence is more appropriate)"},{"location":"data-sources/builtin-toolsets/kafka/","title":"Kafka","text":"<p>By enabling this toolset, HolmesGPT will be able to fetch metadata from Kafka. This provides Holmes the ability to introspect into Kafka by listing consumers and topics or finding lagging consumer groups.</p> <p>This toolset uses the AdminClient of the confluent-kafka python library. Kafka's Java API is also a good source of documentation.</p>"},{"location":"data-sources/builtin-toolsets/kafka/#configuration","title":"Configuration","text":"<p> Holmes CLI Holmes Helm Chart Robusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n    kafka/admin:\n        enabled: true\n        config:\n            kafka_clusters:\n                - name: aks-prod-kafka\n                  kafka_broker: kafka-1.aks-prod-kafka-brokers.kafka.svc:9095\n                  kafka_username: kafka-plaintext-user\n                  kafka_password: ******\n                  kafka_sasl_mechanism: SCRAM-SHA-512\n                  kafka_security_protocol: SASL_PLAINTEXT\n                - name: gke-stg-kafka\n                  kafka_broker: gke-kafka.gke-stg-kafka-brokers.kafka.svc:9095\n                  kafka_username: kafka-plaintext-user\n                  kafka_password: ****\n                  kafka_sasl_mechanism: SCRAM-SHA-512\n                  kafka_security_protocol: SASL_PLAINTEXT</code></pre> <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>toolsets:\n    kafka/admin:\n        enabled: true\n        config:\n            kafka_clusters:\n                - name: aks-prod-kafka\n                  kafka_broker: kafka-1.aks-prod-kafka-brokers.kafka.svc:9095\n                  kafka_username: kafka-plaintext-user\n                  kafka_password: ******\n                  kafka_sasl_mechanism: SCRAM-SHA-512\n                  kafka_security_protocol: SASL_PLAINTEXT\n                - name: gke-stg-kafka\n                  kafka_broker: gke-kafka.gke-stg-kafka-brokers.kafka.svc:9095\n                  kafka_username: kafka-plaintext-user\n                  kafka_password: ****\n                  kafka_sasl_mechanism: SCRAM-SHA-512\n                  kafka_security_protocol: SASL_PLAINTEXT</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code>:</p> <pre><code>holmes:\n  toolsets:\n      kafka/admin:\n          enabled: true\n          config:\n              kafka_clusters:\n                  - name: aks-prod-kafka\n                    kafka_broker: kafka-1.aks-prod-kafka-brokers.kafka.svc:9095\n                    kafka_username: kafka-plaintext-user\n                    kafka_password: ******\n                    kafka_sasl_mechanism: SCRAM-SHA-512\n                    kafka_security_protocol: SASL_PLAINTEXT\n                  - name: gke-stg-kafka\n                    kafka_broker: gke-kafka.gke-stg-kafka-brokers.kafka.svc:9095\n                    kafka_username: kafka-plaintext-user\n                    kafka_password: ****\n                    kafka_sasl_mechanism: SCRAM-SHA-512\n                    kafka_security_protocol: SASL_PLAINTEXT</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;</code></pre> </p> <p>Below is a description of the configuration field for each cluster:</p> Config key Description name Give a meaningful name to your cluster. Holmes will use it to decide what cluster to look into. Names must be unique across all clusters. kafka_broker List of host/port pairs to use for establishing the initial connection to the Kafka cluster kafka_username Username for SASL authentication kafka_password Password for SASL authentication kafka_sasl_mechanism SASL mechanism (e.g., SCRAM-SHA-512) kafka_security_protocol Security protocol (e.g., SASL_PLAINTEXT)"},{"location":"data-sources/builtin-toolsets/kafka/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description kafka_list_topics List all Kafka topics kafka_describe_topic Get detailed information about a specific topic kafka_list_consumers List all consumer groups kafka_describe_consumer Get detailed information about a consumer group kafka_consumer_lag Check consumer lag for a consumer group"},{"location":"data-sources/builtin-toolsets/kubernetes/","title":"Kubernetes Toolsets","text":""},{"location":"data-sources/builtin-toolsets/kubernetes/#core","title":"Core \u2713","text":"<p>Enabled by Default</p> <p>This toolset is enabled by default and should typically remain enabled.</p> <p>By enabling this toolset, HolmesGPT will be able to describe and find Kubernetes resources like nodes, deployments, pods, etc.</p>"},{"location":"data-sources/builtin-toolsets/kubernetes/#configuration","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        kubernetes/core:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/kubernetes/#capabilities","title":"Capabilities","text":"Tool Name Description kubectl_describe Run kubectl describe command on a specific resource kubectl_get_by_name Get details of a specific resource with labels kubectl_get_by_kind_in_namespace List all resources of a given type in a namespace kubectl_get_by_kind_in_cluster List all resources of a given type across the cluster kubectl_find_resources Search for resources matching a keyword kubectl_get_yaml Get YAML definition of a resource kubectl_events Get events for a specific resource kubectl_memory_requests_all_namespaces Get memory requests for all pods across all namespaces in MiB kubectl_memory_requests_namespace Get memory requests for all pods in a specific namespace in MiB kubernetes_jq_query Query Kubernetes resources using jq filters"},{"location":"data-sources/builtin-toolsets/kubernetes/#logs","title":"Logs \u2713","text":"<p>Enabled by Default</p> <p>This toolset is enabled by default. You do not need to configure it.</p> <p>By enabling this toolset, HolmesGPT will be able to read Kubernetes pod logs.</p> Important: Disable Default Logging Toolset <p>Only one logging toolset should be enabled at a time. If you enable this toolset, disable the default <code>kubernetes/logs</code> toolset.</p> <p>Available Log Sources:</p> <ul> <li>Kubernetes logs - Direct pod log access (enabled by default)</li> <li>Loki - Centralized logs via Loki</li> <li>OpenSearch logs - Logs from OpenSearch/Elasticsearch</li> <li>Coralogix logs - Logs via Coralogix platform</li> <li>DataDog - Logs from DataDog</li> </ul> <p>\ud83d\udca1 Choose one: Only enable one logging toolset at a time for best performance.</p>"},{"location":"data-sources/builtin-toolsets/kubernetes/#configuration_1","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        kubernetes/logs:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/kubernetes/#capabilities_1","title":"Capabilities","text":"Tool Name Description kubectl_logs Fetch logs from a specific pod kubectl_logs_all_containers Fetch logs from all containers in a pod kubectl_previous_logs Fetch previous logs from a pod kubectl_previous_logs_all_containers Fetch previous logs from all containers in a pod kubectl_container_logs Fetch logs from a specific container in a pod kubectl_logs_grep Search for specific patterns in pod logs kubectl_logs_all_containers_grep Search for patterns in logs from all containers"},{"location":"data-sources/builtin-toolsets/kubernetes/#live-metrics","title":"Live Metrics","text":"<p>This toolset retrieves real-time CPU and memory usage for pods and nodes.</p>"},{"location":"data-sources/builtin-toolsets/kubernetes/#configuration_2","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        kubernetes/live_metrics:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/kubernetes/#capabilities_2","title":"Capabilities","text":"Tool Name Description kubectl_top_pods Get current CPU and memory usage for pods kubectl_top_nodes Get current CPU and memory usage for nodes"},{"location":"data-sources/builtin-toolsets/kubernetes/#prometheus-stack","title":"Prometheus Stack","text":"<p>This toolset fetches Prometheus target definitions. Requires specific cluster role rules.</p>"},{"location":"data-sources/builtin-toolsets/kubernetes/#configuration_3","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        kubernetes/prometheus_stack:\n            enabled: true\n    customClusterRoleRules:\n        - apiGroups: [\"monitoring.coreos.com\"]\n          resources: [\"servicemonitors\", \"podmonitors\", \"prometheusrules\"]\n          verbs: [\"get\", \"list\"]\n</code></pre>"},{"location":"data-sources/builtin-toolsets/kubernetes/#capabilities_3","title":"Capabilities","text":"Tool Name Description kubectl_get_prometheus_targets Get Prometheus monitoring targets kubectl_get_service_monitors Get ServiceMonitor resources kubectl_get_pod_monitors Get PodMonitor resources"},{"location":"data-sources/builtin-toolsets/kubernetes/#resource-lineage-extras","title":"Resource Lineage Extras","text":"<p>Two variations of resource lineage toolsets: one native and one using kubectl krew. Provides tools to fetch children/dependents and parents/dependencies of Kubernetes resources.</p>"},{"location":"data-sources/builtin-toolsets/kubernetes/#configuration_4","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        kubernetes/resource_lineage_extras:\n            enabled: true\n        # OR\n        kubernetes/resource_lineage_extras_krew:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/kubernetes/#capabilities_4","title":"Capabilities","text":"Tool Name Description kubectl_lineage_children Get child/dependent resources of a Kubernetes resource kubectl_lineage_parents Get parent/dependency resources of a Kubernetes resource"},{"location":"data-sources/builtin-toolsets/mongodb-atlas/","title":"MongoDB Atlas","text":"<p>By enabling this toolset, HolmesGPT can access MongoDB Atlas projects and processes to analyze logs, alerts, events, slow queries, and various metrics to understand the state of MongoDB projects.</p> <p>Warning</p> <p>This toolset is in Experimental stage.</p>"},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#prerequisites","title":"Prerequisites","text":"<ol> <li>MongoDB Atlas account</li> <li>MongoDB Atlas API keys (Public and Private)</li> <li>MongoDB Atlas project ID</li> <li>Appropriate MongoDB Atlas permissions</li> </ol>"},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, set the following environment variables:</p> <pre><code>export MONGODB_ATLAS_PUBLIC_KEY=\"&lt;your-public-api-key&gt;\"\nexport MONGODB_ATLAS_PRIVATE_KEY=\"&lt;your-private-api-key&gt;\"\nexport MONGODB_ATLAS_PROJECT_ID=\"&lt;your-project-id&gt;\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  MongoDBAtlas:\n    enabled: true\n    config:\n      public_key: \"&lt;your-public-api-key&gt;\"\n      private_key: \"&lt;your-private-api-key&gt;\"\n      project_id: \"&lt;your-project-id&gt;\"\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <pre><code>holmes:\n  additionalEnvVars:\n    - name: MONGODB_ATLAS_PUBLIC_KEY\n      value: \"&lt;your-public-api-key&gt;\"\n    - name: MONGODB_ATLAS_PRIVATE_KEY\n      value: \"&lt;your-private-api-key&gt;\"\n    - name: MONGODB_ATLAS_PROJECT_ID\n      value: \"&lt;your-project-id&gt;\"\n  toolsets:\n    MongoDBAtlas:\n      enabled: true\n      config:\n        public_key: \"&lt;your-public-api-key&gt;\"\n        private_key: \"&lt;your-private-api-key&gt;\"\n        project_id: \"&lt;your-project-id&gt;\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#setting-up-mongodb-atlas-api-keys","title":"Setting up MongoDB Atlas API Keys","text":"<ol> <li>Log into MongoDB Atlas and navigate to your organization</li> <li>Go to Access Manager \u2192 API Keys</li> <li>Create a new API key:</li> <li>Set appropriate permissions for your use case</li> <li>Copy the public key and private key</li> <li>Get your Project ID:</li> <li>Navigate to your project</li> <li>Copy the Project ID from the project settings</li> </ol>"},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#required-permissions","title":"Required Permissions","text":"<p>The API key requires the following permissions:</p> <ul> <li>Project Read Only - To read project information</li> <li>Project Data Access Admin - To access database logs and metrics</li> <li>Project Monitoring Admin - To access monitoring data and alerts</li> </ul>"},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#capabilities","title":"Capabilities","text":"Tool Name Description atlas_return_project_alerts Get alerts for the MongoDB Atlas project atlas_return_project_processes Get information about processes in the project atlas_return_project_slow_queries Get slow queries from the project (last 24 hours) atlas_return_events_from_project Get events from the project (last 24 hours) atlas_return_logs_for_host_in_project Get logs for a specific host in the project atlas_return_event_type_from_project Get events of a specific type from the project"},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#usage-guidelines","title":"Usage Guidelines","text":""},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#performance-analysis","title":"Performance Analysis","text":"<p>When investigating performance issues:</p> <ol> <li>Start with alerts and events: Use <code>atlas_return_project_alerts</code> and <code>atlas_return_events_from_project</code> first to identify known issues</li> <li>Check slow queries: Use <code>atlas_return_project_slow_queries</code> to identify performance bottlenecks</li> <li>Review logs: Use <code>atlas_return_logs_for_host_in_project</code> for detailed log analysis</li> </ol>"},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#time-range-limitations","title":"Time Range Limitations","text":"<ul> <li><code>atlas_return_project_slow_queries</code> returns data from the last 24 hours only</li> <li><code>atlas_return_events_from_project</code> returns data from the last 24 hours only</li> <li>If you need data from a different time range, the toolset will inform you it's not currently supported</li> </ul>"},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#query-analysis","title":"Query Analysis","text":"<ul> <li>When analyzing slow queries, the toolset will show the actual query text for every slow query</li> <li>For requests asking for a specific number of slow queries (e.g., \"top 10 slow queries\"), the toolset will not duplicate queries from different processes</li> </ul>"},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#troubleshooting","title":"Troubleshooting","text":""},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#common-issues","title":"Common Issues","text":"<ol> <li>Authentication Failures</li> <li>Verify your API keys are correct</li> <li>Check that the API key has appropriate permissions</li> <li> <p>Ensure the project ID is correct</p> </li> <li> <p>Permission Errors</p> </li> <li>Verify the API key has the required permissions listed above</li> <li> <p>Check that the key is associated with the correct organization</p> </li> <li> <p>No Data Returned</p> </li> <li>Verify the project ID is correct</li> <li>Check that there are active processes in the project</li> <li>Ensure the time range contains relevant data</li> </ol>"},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#api-rate-limits","title":"API Rate Limits","text":"<p>MongoDB Atlas API has rate limits. If you encounter rate limiting:</p> <ul> <li>Wait before making additional requests</li> <li>Consider the frequency of your queries</li> <li>Check MongoDB Atlas documentation for current rate limits</li> </ul>"},{"location":"data-sources/builtin-toolsets/mongodb-atlas/#references","title":"References","text":"<ul> <li>MongoDB Atlas API Documentation</li> <li>MongoDB Atlas API Authentication</li> <li>MongoDB Atlas Monitoring</li> </ul>"},{"location":"data-sources/builtin-toolsets/newrelic/","title":"New Relic","text":"<p>By enabling this toolset, HolmesGPT will be able to pull traces and logs from New Relic for investigations.</p> Important: Disable Default Logging Toolset <p>Only one logging toolset should be enabled at a time. If you enable this toolset, disable the default <code>kubernetes/logs</code> toolset.</p> <p>Available Log Sources:</p> <ul> <li>Kubernetes logs - Direct pod log access (enabled by default)</li> <li>Loki - Centralized logs via Loki</li> <li>OpenSearch logs - Logs from OpenSearch/Elasticsearch</li> <li>Coralogix logs - Logs via Coralogix platform</li> <li>DataDog - Logs from DataDog</li> </ul> <p>\ud83d\udca1 Choose one: Only enable one logging toolset at a time for best performance.</p>"},{"location":"data-sources/builtin-toolsets/newrelic/#prerequisites","title":"Prerequisites","text":"<ol> <li>A New Relic API Key with necessary permissions to access traces and logs</li> <li>Your New Relic Account ID</li> </ol> <p>You can find these in your New Relic account under Administration &gt; API keys and Account settings.</p>"},{"location":"data-sources/builtin-toolsets/newrelic/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  newrelic:\n    enabled: true\n    config:\n      nr_api_key: \"&lt;your New Relic API key&gt;\"\n      nr_account_id: \"&lt;your New Relic account ID&gt;\"\n\n  kubernetes/logs:\n    enabled: false  # Disable default Kubernetes logging if using New Relic for logs\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <pre><code>holmes:\n  toolsets:\n    newrelic:\n      enabled: true\n      config:\n        nr_api_key: \"&lt;your New Relic API key&gt;\"\n        nr_account_id: \"&lt;your New Relic account ID&gt;\"\n\n    kubernetes/logs:\n      enabled: false  # Disable default Kubernetes logging if using New Relic for logs\n</code></pre> <p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p>"},{"location":"data-sources/builtin-toolsets/newrelic/#capabilities","title":"Capabilities","text":"Tool Name Description <p>| <code>newrelic_get_logs</code> | Retrieve logs from New Relic for a specific application and time range | | <code>newrelic_get_traces</code> | Retrieve traces from New Relic based on duration threshold or specific trace ID |</p> <p>For more information, see the New Relic API documentation.</p>"},{"location":"data-sources/builtin-toolsets/notion/","title":"Notion","text":"<p>Notion Integration for HolmesGPT</p> <p>Enabling this toolset allows HolmesGPT to fetch pages from Notion, making it useful when providing Notion-based runbooks.</p>"},{"location":"data-sources/builtin-toolsets/notion/#setup-instructions","title":"Setup Instructions","text":"<ol> <li> <p>Create a Webhook Integration</p> <ul> <li>Go to the Notion Developer Portal.</li> <li>Create a new integration with read content capabilities.</li> </ul> </li> <li> <p>Grant Access to Pages</p> <ul> <li>Open the desired Notion page.</li> <li>Click the three dots in the top right.</li> <li>Select Connections and add your integration.</li> </ul> </li> <li> <p>Configure Authentication</p> <ul> <li>Retrieve the Internal Integration Secret from Notion.</li> <li>Create a Kubernetes secret in your cluster with this key.</li> <li>Configure the <code>NOTION_AUTH</code> environment variable.</li> </ul> </li> </ol>"},{"location":"data-sources/builtin-toolsets/notion/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, set the environment variable: <pre><code>export NOTION_AUTH=\"&lt;your Notion integration secret&gt;\"\n</code></pre></p> <p>Then add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist: <pre><code>toolsets:\n    notion:\n        enabled: true\n</code></pre></p> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <pre><code>holmes:\n    additionalEnvVars:\n        - name: NOTION_AUTH\n          value: \"&lt;your Notion integration secret&gt;\"\n    toolsets:\n        notion:\n            enabled: true\n            config:\n                additional_headers:\n                    Authorization: Bearer {{ env.NOTION_AUTH }}\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/notion/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description fetch_notion_webpage Fetch a Notion webpage. Use this to fetch Notion runbooks if they are present before starting your investigation"},{"location":"data-sources/builtin-toolsets/opensearch-logs/","title":"OpenSearch Logs","text":"<p>By enabling this toolset, HolmesGPT will fetch pod logs from OpenSearch.</p> Important: Disable Default Logging Toolset <p>Only one logging toolset should be enabled at a time. If you enable this toolset, disable the default <code>kubernetes/logs</code> toolset.</p> <p>Available Log Sources:</p> <ul> <li>Kubernetes logs - Direct pod log access (enabled by default)</li> <li>Loki - Centralized logs via Loki</li> <li>OpenSearch logs - Logs from OpenSearch/Elasticsearch</li> <li>Coralogix logs - Logs via Coralogix platform</li> <li>DataDog - Logs from DataDog</li> </ul> <p>\ud83d\udca1 Choose one: Only enable one logging toolset at a time for best performance.</p>"},{"location":"data-sources/builtin-toolsets/opensearch-logs/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  opensearch/logs:\n    enabled: true\n    config:\n      opensearch_url: &lt;your opensearch/elastic URL&gt;\n      index_pattern: &lt;name of the index to use&gt; # The pattern matching the indexes containing the logs. Supports wildcards. For example `fluentd-*`\n      opensearch_auth_header: \"ApiKey &lt;...&gt;\" # An optional header value set to the `Authorization` header for every request to opensearch\n      labels: # set the labels according to how values are mapped in your opensearch cluster\n        pod: \"kubernetes.pod_name\"\n        namespace: \"kubernetes.namespace_name\"\n        timestamp: \"@timestamp\"\n        message: \"message\"\n\n  kubernetes/logs:\n    enabled: false # HolmesGPT's default logging mechanism MUST be disabled\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <pre><code>holmes:\n  toolsets:\n    opensearch/logs:\n      enabled: true\n      config:\n        opensearch_url: https://skdjasid.europe-west1.gcp.cloud.es.io:443 # The URL to your opensearch cluster.\n        index_pattern: fluentd-* # The pattern matching the indexes containing the logs. Supports wildcards\n        opensearch_auth_header: \"ApiKey b0ZlwQWEsdwAkv047bafirkallDFWJIWDWdwlQQ==\" # An optional header value set to the `Authorization` header for every request to opensearch.\n        labels: # set the labels according to how values are mapped in your opensearch cluster\n          pod: \"kubernetes.pod_name\"\n          namespace: \"kubernetes.namespace_name\"\n          timestamp: \"@timestamp\"\n          message: \"message\"\n\n    kubernetes/logs:\n      enabled: false # HolmesGPT's default logging mechanism MUST be disabled\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/opensearch-logs/#configuring-index_pattern-and-labels","title":"Configuring index_pattern and labels","text":"<p>You can tweak the labels used by the toolset to identify kubernetes resources. This is optional and only needed if your logs settings differ from the defaults in the example below.</p> <pre><code>toolsets:\n  opensearch/logs:\n    enabled: true\n    config:\n      index_pattern: fluentd-*\n      labels:\n        pod: \"kubernetes.pod_name\"\n        namespace: \"kubernetes.namespace_name\"\n        timestamp: \"@timestamp\"\n        message: \"message\"\n</code></pre> <p>Below is a screenshot of a query that was done using Elastic dev tools to find out what should be the values for the labels.</p> <p></p> <p>In the image above, the following values and labels are identified by a yellow rectangle:</p> Configuration field Value Description index_pattern fluentd-* This defines what opensearch indexes should be used to fetch logs pod kubernetes.pod_name The kubernetes pod name namespace kubernetes.namespace_name The kubernetes namespace timestamp @timestamp This timestamp is used to search logs by time range. message message This is the content of the log message"},{"location":"data-sources/builtin-toolsets/opensearch-logs/#capabilities","title":"Capabilities","text":"Tool Name Description fetch_pod_logs Retrieve logs using opensearch"},{"location":"data-sources/builtin-toolsets/opensearch-status/","title":"OpenSearch status","text":"<p>By enabling this toolset, HolmesGPT will be able to access cluster metadata information like health, shards, and settings. This allows HolmesGPT to better troubleshoot problems with one or more OpenSearch clusters.</p>"},{"location":"data-sources/builtin-toolsets/opensearch-status/#configuration","title":"Configuration","text":"<p>The configuration for OpenSearch is passed through to the underlying opensearch-py library. Consult this library's user guide or reference documentation for configuring the connection to OpenSearch, including how to authenticate this toolset to an OpenSearch cluster.</p> <p> Holmes CLI Holmes Helm Chart Robusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n    opensearch/status:\n        enabled: true\n        config:\n            opensearch_clusters:\n                - hosts:\n                    - host1.com\n                    - host2.com\n                  headers:\n                    header1: \"value1\"\n                  use_ssl: &lt;boolean&gt;\n                  ssl_assert_hostname: &lt;boolean&gt;\n                  verify_certs: &lt;boolean&gt;\n                  ssl_show_warn: &lt;boolean&gt;\n                  http_auth:\n                    username: &lt;basic auth username&gt;\n                    password: &lt;basic auth password&gt;</code></pre> <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>toolsets:\n    opensearch/status:\n        enabled: true\n        config:\n            opensearch_clusters:\n                - hosts:\n                    - host1.com\n                    - host2.com\n                  headers:\n                    header1: \"value1\"\n                  use_ssl: &lt;boolean&gt;\n                  ssl_assert_hostname: &lt;boolean&gt;\n                  verify_certs: &lt;boolean&gt;\n                  ssl_show_warn: &lt;boolean&gt;\n                  http_auth:\n                    username: &lt;basic auth username&gt;\n                    password: &lt;basic auth password&gt;</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code>:</p> <pre><code>holmes:\n  toolsets:\n      opensearch/status:\n          enabled: true\n          config:\n              opensearch_clusters:\n                  - hosts:\n                      - host1.com\n                      - host2.com\n                    headers:\n                      header1: \"value1\"\n                    use_ssl: &lt;boolean&gt;\n                    ssl_assert_hostname: &lt;boolean&gt;\n                    verify_certs: &lt;boolean&gt;\n                    ssl_show_warn: &lt;boolean&gt;\n                    http_auth:\n                      username: &lt;basic auth username&gt;\n                      password: &lt;basic auth password&gt;</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;</code></pre> </p> <p>Here is an example of an insecure OpenSearch configuration for local development using a bearer token:</p> Holmes CLIRobusta Helm Chart <p>First, set the environment variables: <pre><code>export OPENSEARCH_URL=\"&lt;opensearch host URL&gt;\"\nexport OPENSEARCH_BEARER_TOKEN=\"&lt;secret bearer token&gt;\"\n</code></pre></p> <p>Then add the following to ~/.holmes/config.yaml: <pre><code>toolsets:\n    opensearch/status:\n        enabled: true\n        config:\n            opensearch_clusters:\n                - hosts:\n                    - host: \"{{ env.OPENSEARCH_URL }}\"\n                      port: 9200\n</code></pre></p> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <pre><code>holmes:\n    additionalEnvVars:\n        - name: OPENSEARCH_URL\n          value: \"&lt;opensearch host URL&gt;\"\n        - name: OPENSEARCH_BEARER_TOKEN\n          value: \"&lt;secret bearer token&gt;\"\n    toolsets:\n        opensearch/status:\n            enabled: true\n            config:\n                opensearch_clusters:\n                    - hosts:\n                        - host: \"{{ env.OPENSEARCH_URL }}\"\n                          port: 9200\n</code></pre>"},{"location":"data-sources/builtin-toolsets/opensearch-status/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description opensearch_cluster_health Get cluster health information opensearch_cluster_stats Get cluster statistics opensearch_node_info Get information about cluster nodes opensearch_index_stats Get statistics for specific indices opensearch_shard_allocation Get shard allocation information"},{"location":"data-sources/builtin-toolsets/prometheus/","title":"Prometheus","text":"<p>Connect HolmesGPT to Prometheus for metrics analysis and query generation. This integration enables detection of memory leaks, CPU throttling, queue backlogs, and performance issues.</p>"},{"location":"data-sources/builtin-toolsets/prometheus/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running and accessible Prometheus server</li> <li>Ensure HolmesGPT can connect to the Prometheus endpoint</li> </ul>"},{"location":"data-sources/builtin-toolsets/prometheus/#configuration","title":"Configuration","text":"<p> Holmes CLI Holmes Helm Chart Robusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n    prometheus/metrics:\n        enabled: true\n        config:\n            prometheus_url: http://&lt;your-prometheus-service&gt;:9090\n\n            # Optional:\n            #headers:\n            #    Authorization: \"Basic &lt;base_64_encoded_string&gt;\"</code></pre> <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>toolsets:\n    prometheus/metrics:\n        enabled: true\n        config:\n            prometheus_url: http://&lt;your-prometheus-service&gt;:9090\n\n            # Optional:\n            #headers:\n            #    Authorization: \"Basic &lt;base_64_encoded_string&gt;\"</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code>:</p> <pre><code>holmes:\n  toolsets:\n      prometheus/metrics:\n          enabled: true\n          config:\n              prometheus_url: http://&lt;your-prometheus-service&gt;:9090\n\n              # Optional:\n              #headers:\n              #    Authorization: \"Basic &lt;base_64_encoded_string&gt;\"</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;</code></pre> </p> <p>\ud83d\udca1 Alternative: Set the <code>PROMETHEUS_URL</code> environment variable instead of using the config file.</p>"},{"location":"data-sources/builtin-toolsets/prometheus/#validation","title":"Validation","text":"<p>To test your connection, run:</p> <pre><code>holmes ask \"Show me the CPU usage for the last hour\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/prometheus/#troubleshooting","title":"Troubleshooting","text":""},{"location":"data-sources/builtin-toolsets/prometheus/#finding-your-prometheus-url","title":"Finding your Prometheus URL","text":"<p>There are several ways to find your Prometheus URL:</p> <p>Option 1: Simple method (port-forwarding)</p> <pre><code># Find Prometheus services\nkubectl get svc -A | grep prometheus\n\n# Port forward for testing\nkubectl port-forward svc/&lt;your-prometheus-service&gt; 9090:9090 -n &lt;namespace&gt;\n# Then access Prometheus at: http://localhost:9090\n</code></pre> <p>Option 2: Advanced method (get full cluster DNS URL)</p> <p>If you want to find the full internal DNS URL for Prometheus, run:</p> <pre><code>kubectl get svc --all-namespaces -o jsonpath='{range .items[*]}{.metadata.name}{\".\"}{.metadata.namespace}{\".svc.cluster.local:\"}{.spec.ports[0].port}{\"\\n\"}{end}' | grep prometheus | grep -Ev 'operat|alertmanager|node|coredns|kubelet|kube-scheduler|etcd|controller' | awk '{print \"http://\"$1}'\n</code></pre> <p>This will print all possible Prometheus service URLs in your cluster. Pick the one that matches your deployment.</p>"},{"location":"data-sources/builtin-toolsets/prometheus/#common-issues","title":"Common Issues","text":"<ul> <li>Connection refused: Check if the Prometheus URL is accessible from HolmesGPT.</li> <li>Authentication errors: Verify the headers configuration for secured Prometheus endpoints.</li> <li>No metrics returned: Ensure that Prometheus is scraping your targets.</li> </ul>"},{"location":"data-sources/builtin-toolsets/prometheus/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can further customize the Prometheus toolset with the following options:</p> <pre><code>toolsets:\n  prometheus/metrics:\n    enabled: true\n    config:\n      prometheus_url: http://&lt;prometheus-host&gt;:9090\n      healthcheck: \"-/healthy\"  # Path for health checking (default: -/healthy)\n      headers:\n        Authorization: \"Basic &lt;base_64_encoded_string&gt;\"\n      metrics_labels_time_window_hrs: 48  # Time window (hours) for fetching labels (default: 48)\n      metrics_labels_cache_duration_hrs: 12  # How long to cache labels (hours, default: 12)\n      fetch_labels_with_labels_api: false  # Use labels API instead of series API (default: false)\n      fetch_metadata_with_series_api: false  # Use series API for metadata (default: false)\n      tool_calls_return_data: true  # If false, disables returning Prometheus data (default: true)\n</code></pre> <p>Config option explanations:</p> <ul> <li><code>prometheus_url</code>: The base URL for Prometheus. Should include protocol and port.</li> <li><code>healthcheck</code>: Path used for health checking Prometheus or Mimir/Cortex endpoint. Defaults to <code>-/healthy</code> for Prometheus, use <code>/ready</code> for Grafana Mimir.</li> <li><code>headers</code>: Extra headers for all Prometheus HTTP requests (e.g., for authentication).</li> <li><code>metrics_labels_time_window_hrs</code>: Time window (in hours) for fetching labels. Set to <code>null</code> to fetch all labels.</li> <li><code>metrics_labels_cache_duration_hrs</code>: How long to cache labels (in hours). Set to <code>null</code> to disable caching.</li> <li><code>fetch_labels_with_labels_api</code>: Use the Prometheus labels API to fetch labels (can improve performance, but increases HTTP calls).</li> <li><code>fetch_metadata_with_series_api</code>: Use the series API for metadata (only set to true if the metadata API is disabled or not working).</li> <li><code>tool_calls_return_data</code>: If <code>false</code>, disables returning Prometheus data to HolmesGPT (useful if you hit token limits).</li> </ul>"},{"location":"data-sources/builtin-toolsets/prometheus/#capabilities","title":"Capabilities","text":"Tool Name Description list_available_metrics List all available Prometheus metrics execute_prometheus_instant_query Execute an instant PromQL query execute_prometheus_range_query Execute a range PromQL query for time series data get_current_time Get current timestamp for time-based queries"},{"location":"data-sources/builtin-toolsets/prometheus/#coralogix-prometheus-configuration","title":"Coralogix Prometheus Configuration","text":"<p>To use a Coralogix PromQL endpoint with HolmesGPT:</p> <ol> <li>Go to Coralogix Documentation and choose the relevant PromQL endpoint for your region.</li> <li>In Coralogix, create an API key with permissions to query metrics (Data Flow \u2192 API Keys).</li> <li> <p>Create a Kubernetes secret for the API key and expose it as an environment variable in your Helm values:</p> <pre><code>holmes:\n  additionalEnvVars:\n    - name: CORALOGIX_API_KEY\n      valueFrom:\n        secretKeyRef:\n          name: coralogix-api-key\n          key: CORALOGIX_API_KEY\n</code></pre> </li> <li> <p>Add the following under your toolsets in the Helm chart:</p> <pre><code>holmes:\n  toolsets:\n    prometheus/metrics:\n      enabled: true\n      config:\n        healthcheck: \"/api/v1/query?query=up\"  # This is important for Coralogix\n        prometheus_url: \"https://prom-api.eu2.coralogix.com\"  # Use your region's endpoint\n        headers:\n          token: \"{{ env.CORALOGIX_API_KEY }}\"\n        metrics_labels_time_window_hrs: 72\n        metrics_labels_cache_duration_hrs: 12\n        fetch_labels_with_labels_api: true\n        tool_calls_return_data: true\n        fetch_metadata_with_series_api: true\n</code></pre> </li> </ol>"},{"location":"data-sources/builtin-toolsets/prometheus/#grafana-cloud-mimir-configuration","title":"Grafana Cloud (Mimir) Configuration","text":"<p>To connect HolmesGPT to Grafana Cloud's Prometheus/Mimir endpoint:</p> <ol> <li>Create a service account token in Grafana Cloud:</li> <li>Navigate to \"Administration \u2192 Service accounts\"</li> <li>Create a new service account</li> <li> <p>Generate a service account token (starts with <code>glsa_</code>)</p> </li> <li> <p>Find your Prometheus datasource UID: <pre><code>curl -H \"Authorization: Bearer YOUR_GLSA_TOKEN\" \\\n     \"https://YOUR-INSTANCE.grafana.net/api/datasources\" | \\\n     jq '.[] | select(.type==\"prometheus\") | {name, uid}'\n</code></pre></p> </li> <li> <p>Configure HolmesGPT: <pre><code>holmes:\n  toolsets:\n    prometheus/metrics:\n      enabled: true\n      config:\n        prometheus_url: https://YOUR-INSTANCE.grafana.net/api/datasources/proxy/uid/PROMETHEUS_DATASOURCE_UID\n        fetch_labels_with_labels_api: false  # Important for Mimir\n        fetch_metadata_with_series_api: true  # Important for Mimir\n        headers:\n          Authorization: Bearer YOUR_GLSA_TOKEN\n</code></pre></p> </li> </ol> <p>Important notes:</p> <ul> <li>Use the proxy endpoint URL format <code>/api/datasources/proxy/uid/</code> - this handles authentication and routing to Mimir automatically</li> <li>Set <code>fetch_labels_with_labels_api: false</code> for optimal Mimir compatibility</li> <li>Set <code>fetch_metadata_with_series_api: true</code> for proper metadata retrieval</li> </ul>"},{"location":"data-sources/builtin-toolsets/rabbitmq/","title":"RabbitMQ","text":"<p>By enabling this toolset, HolmesGPT will be able to detect RabbitMQ partitions, memory alerts, and disk alerts and suggest mitigations.</p> <p>This toolset follows a two-step process to detect partition:</p> <ol> <li>The nodes and partitioning status is obtained by fetching information from the configured <code>management_url</code>.</li> <li>If some nodes are reported as not-running, the toolset will try to contact these nodes individually and deduce any partitioning state for any node that is actually running.</li> </ol>"},{"location":"data-sources/builtin-toolsets/rabbitmq/#configuration","title":"Configuration","text":"<p> Holmes CLI Holmes Helm Chart Robusta Helm Chart <p>Add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  rabbitmq/core:\n    enabled: true\n    config:\n      clusters:\n        - id: rabbitmq # must be unique across all configured clusters\n          username: &lt;user&gt;\n          password: &lt;password&gt;\n          management_url: &lt;http://rabbitmq.rabbitmq:15672&gt;</code></pre> <p>When using the standalone Holmes Helm Chart, update your <code>values.yaml</code>:</p> <pre><code>toolsets:\n  rabbitmq/core:\n    enabled: true\n    config:\n      clusters:\n        - id: rabbitmq # must be unique across all configured clusters\n          username: &lt;user&gt;\n          password: &lt;password&gt;\n          management_url: &lt;http://rabbitmq.rabbitmq:15672&gt;</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade holmes holmes/holmes --values=values.yaml</code></pre> <p>When using the Robusta Helm Chart (which includes HolmesGPT), update your <code>generated_values.yaml</code>:</p> <pre><code>holmes:\n  toolsets:\n    rabbitmq/core:\n      enabled: true\n      config:\n        clusters:\n          - id: rabbitmq # must be unique across all configured clusters\n            username: &lt;user&gt;\n            password: &lt;password&gt;\n            management_url: &lt;http://rabbitmq.rabbitmq:15672&gt;</code></pre> <p>Apply the configuration:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;</code></pre> </p>"},{"location":"data-sources/builtin-toolsets/rabbitmq/#advanced-configuration","title":"Advanced configuration","text":"<p>Below is the full list of options for this toolset:</p> <pre><code>rabbitmq/core:\n  enabled: true\n  config:\n    clusters:\n      - id: rabbitmq # must be unique across all configured clusters\n        username: &lt;user&gt;\n        password: &lt;password&gt;\n        management_url: &lt;http://rabbitmq.rabbitmq:15672&gt;\n        request_timeout_seconds: 30 # timeout for HTTP requests\n</code></pre>"},{"location":"data-sources/builtin-toolsets/rabbitmq/#capabilities","title":"Capabilities","text":"Tool Name Description get_rabbitmq_cluster_status Get cluster status and partition information get_rabbitmq_node_info Get detailed information about RabbitMQ nodes get_rabbitmq_queue_info Get information about queues get_rabbitmq_exchange_info Get information about exchanges get_rabbitmq_memory_usage Get memory usage statistics get_rabbitmq_disk_usage Get disk usage statistics"},{"location":"data-sources/builtin-toolsets/robusta/","title":"Robusta","text":"<p>Optional - Requires Robusta SaaS</p> <p>This toolset is NOT enabled by default. It requires integration with the Robusta SaaS platform and proper authentication credentials.</p> <p>The Robusta toolset provides advanced observability capabilities by connecting HolmesGPT to the Robusta SaaS platform. When enabled, it gives HolmesGPT access to historical data, change tracking, and resource recommendations that are not available from standard Kubernetes APIs.</p>"},{"location":"data-sources/builtin-toolsets/robusta/#prerequisites","title":"Prerequisites","text":"<p>To use this toolset, you need:</p> <ol> <li>An active Robusta SaaS account</li> <li>Valid authentication credentials (provided via environment variables or configuration file)</li> <li>The Robusta platform deployed in your cluster</li> </ol>"},{"location":"data-sources/builtin-toolsets/robusta/#what-it-adds","title":"What It Adds","text":"<p>When connected to Robusta SaaS, HolmesGPT gains access to:</p> <ul> <li>Historical Alert Data: Fetch detailed metadata about past alerts and incidents, including context that may no longer be available in Prometheus or AlertManager</li> <li>Change Tracking: Query configuration changes across your entire cluster within specific time ranges, helping identify what changed before an incident</li> <li>Resource Recommendations: Get AI-powered recommendations for resource requests and limits based on actual historical usage patterns</li> </ul>"},{"location":"data-sources/builtin-toolsets/robusta/#configuration","title":"Configuration","text":"<p>The toolset requires authentication to Robusta SaaS. You can provide credentials in three ways:</p>"},{"location":"data-sources/builtin-toolsets/robusta/#option-1-automatic-via-robusta-helm-chart","title":"Option 1: Automatic (via Robusta Helm Chart)","text":"<p>If you deploy HolmesGPT as part of the Robusta Helm chart, credentials are automatically configured. The Helm chart handles mounting the necessary secrets and configuration files.</p>"},{"location":"data-sources/builtin-toolsets/robusta/#option-2-environment-variables","title":"Option 2: Environment Variables","text":"<pre><code>export ROBUSTA_UI_TOKEN=\"&lt;base64-encoded-token&gt;\"\n# OR provide individual credentials:\nexport ROBUSTA_ACCOUNT_ID=\"&lt;account-id&gt;\"\nexport STORE_URL=\"&lt;store-url&gt;\"\nexport STORE_API_KEY=\"&lt;api-key&gt;\"\nexport STORE_EMAIL=\"&lt;email&gt;\"\nexport STORE_PASSWORD=\"&lt;password&gt;\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/robusta/#option-3-configuration-file","title":"Option 3: Configuration File","text":"<p>The toolset will automatically look for credentials in <code>/etc/robusta/config/active_playbooks.yaml</code> (or the path specified by <code>ROBUSTA_CONFIG_PATH</code>).</p>"},{"location":"data-sources/builtin-toolsets/robusta/#enabling-the-toolset","title":"Enabling the Toolset","text":"<pre><code>holmes:\n    toolsets:\n        robusta:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/robusta/#capabilities","title":"Capabilities","text":"Tool Name Description fetch_finding_by_id Fetches detailed metadata about a specific Robusta finding (alerts, deployment updates, etc.) including historical context fetch_configuration_changes Retrieves all configuration changes in a given time range, optionally filtered by namespace or workload fetch_resource_recommendation Provides resource optimization recommendations based on actual historical usage for Deployments, StatefulSets, DaemonSets, and Jobs"},{"location":"data-sources/builtin-toolsets/robusta/#use-cases","title":"Use Cases","text":"<p>This toolset is particularly useful for:</p> <ul> <li>Root Cause Analysis: Understanding what configuration changes occurred before an incident</li> <li>Resource Optimization: Getting data-driven recommendations for right-sizing workloads</li> <li>Historical Investigation: Accessing alert context and metadata that may have been lost or expired in other systems</li> <li>Change Management: Tracking who changed what and when across your infrastructure</li> </ul>"},{"location":"data-sources/builtin-toolsets/robusta/#notes","title":"Notes","text":"<ul> <li>The toolset will only be functional if valid Robusta SaaS credentials are provided</li> <li>If credentials are missing or invalid, the toolset will be disabled automatically</li> <li>This integration provides read-only access to your Robusta data</li> </ul>"},{"location":"data-sources/builtin-toolsets/servicenow/","title":"ServiceNow","text":"<p>By enabling this toolset, HolmesGPT will be able to interact with ServiceNow for ticket management, incident tracking, and accessing knowledge base articles during investigations.</p> <p>Warning</p> <p>This toolset is in Experimental stage.</p>"},{"location":"data-sources/builtin-toolsets/servicenow/#prerequisites","title":"Prerequisites","text":"<ol> <li>ServiceNow instance URL</li> <li>ServiceNow API key or username and password</li> <li>Appropriate ServiceNow roles and permissions</li> </ol>"},{"location":"data-sources/builtin-toolsets/servicenow/#setting-up-servicenow-api-access","title":"Setting up ServiceNow API Access","text":"<p>Full configuration guide</p>"},{"location":"data-sources/builtin-toolsets/servicenow/#create-an-inbound-authentication-profile","title":"Create an inbound authentication profile","text":"<ol> <li>Navigate to All &gt; System Web Services &gt; API Access Policies &gt; Inbound Authentication Profiles</li> <li>Select New</li> <li>Select Create API Key authentication profiles</li> <li>Auth Parameter &gt; add x-sn-apikey: Auth Header</li> <li>Submit the form</li> </ol>"},{"location":"data-sources/builtin-toolsets/servicenow/#create-a-rest-api-key","title":"Create a REST API key","text":"<ol> <li>Navigate to All &gt; System Web Services &gt; API Access Policies &gt; REST API Key</li> <li>Select New</li> <li>Set name, description, and user. Set expiry date if desired. &gt; Submit</li> <li>Open the record that was created to view the token generated by the ServiceNow AI Platform for the user</li> </ol>"},{"location":"data-sources/builtin-toolsets/servicenow/#create-a-rest-api-access-policy","title":"Create a REST API Access policy","text":"<ol> <li>Navigate to All &gt; System Web Services &gt; REST API Access Policies</li> <li>Select New</li> <li>REST API = Table API</li> <li>Uncheck Apply to all tables &gt; Select table &gt; change_request</li> <li>In select profile from step 1 (API Key)</li> </ol>"},{"location":"data-sources/builtin-toolsets/servicenow/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart"},{"location":"data-sources/builtin-toolsets/servicenow/#api-key-authentication-recommended","title":"API Key Authentication (Recommended)","text":"<p>First, set the following environment variables:</p> <pre><code>export SERVICENOW_INSTANCE=\"&lt;your servicenow instance name&gt;\"  # e.g., \"dev12345\"\nexport SERVICENOW_API_KEY=\"&lt;your servicenow api key&gt;\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  servicenow/tickets:\n    enabled: true\n    config:\n      api_key: \"&lt;your servicenow api key&gt;\"\n      instance: \"&lt;your servicenow instance name&gt;\"\n      verify_ssl: true\n      timeout: 30\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/servicenow/#usernamepassword-authentication","title":"Username/Password Authentication","text":"<p>First, set the following environment variables:</p> <pre><code>export SERVICENOW_INSTANCE=\"&lt;your servicenow instance url&gt;\"\nexport SERVICENOW_USERNAME=\"&lt;your servicenow username&gt;\"\nexport SERVICENOW_PASSWORD=\"&lt;your servicenow password&gt;\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist:</p> <pre><code>toolsets:\n  servicenow/tickets:\n    enabled: true\n    config:\n      verify_ssl: true\n      timeout: 30\n</code></pre> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/servicenow/#api-key-authentication-recommended_1","title":"API Key Authentication (Recommended)","text":"<pre><code>holmes:\n  additionalEnvVars:\n    - name: SERVICENOW_INSTANCE\n      value: \"&lt;your servicenow instance name&gt;\"\n    - name: SERVICENOW_API_KEY\n      value: \"&lt;your servicenow api key&gt;\"\n  toolsets:\n    servicenow/tickets:\n      enabled: true\n      config:\n        api_key: \"&lt;your servicenow api key&gt;\"\n        instance: \"&lt;your servicenow instance name&gt;\"\n        verify_ssl: true\n        timeout: 30\n</code></pre>"},{"location":"data-sources/builtin-toolsets/servicenow/#usernamepassword-authentication_1","title":"Username/Password Authentication","text":"<pre><code>holmes:\n  additionalEnvVars:\n    - name: SERVICENOW_INSTANCE\n      value: \"&lt;your servicenow instance url&gt;\"\n    - name: SERVICENOW_USERNAME\n      value: \"&lt;your servicenow username&gt;\"\n    - name: SERVICENOW_PASSWORD\n      value: \"&lt;your servicenow password&gt;\"\n  toolsets:\n    servicenow/tickets:\n      enabled: true\n      config:\n        verify_ssl: true\n        timeout: 30\n</code></pre>"},{"location":"data-sources/builtin-toolsets/servicenow/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can customize ServiceNow integration settings:</p> <pre><code>toolsets:\n  servicenow/tickets:\n    enabled: true\n    config:\n      verify_ssl: true\n      timeout: 30  # Request timeout in seconds\n      max_results: 100  # Maximum number of tickets to fetch\n      default_table: \"incident\"  # Default ServiceNow table to query\n</code></pre>"},{"location":"data-sources/builtin-toolsets/servicenow/#capabilities","title":"Capabilities","text":"Tool Name Description servicenow_create_incident Create a new incident ticket in ServiceNow servicenow_get_incident Get details of a specific incident servicenow_search_incidents Search for incidents based on criteria servicenow_update_incident Update an existing incident servicenow_get_knowledge_base Search ServiceNow knowledge base articles servicenow_create_change_request Create a change request ticket"},{"location":"data-sources/builtin-toolsets/slab/","title":"Slab","text":"<p>By enabling this toolset, HolmesGPT will be able to consult runbooks from Slab pages.</p> <p>Retrieve your Slab API token prior to configuring this toolset. Do note that Slab API is only available for Slab premium users. See here.</p>"},{"location":"data-sources/builtin-toolsets/slab/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, set the environment variable: <pre><code>export SLAB_API_KEY=\"&lt;your Slab API key&gt;\"\n</code></pre></p> <p>Then add the following to ~/.holmes/config.yaml. Create the file if it doesn't exist: <pre><code>toolsets:\n    slab:\n        enabled: true\n</code></pre></p> <p>After making changes to your configuration, run: <pre><code>holmes toolset refresh\n</code></pre></p> <pre><code>holmes:\n    additionalEnvVars:\n        - name: SLAB_API_KEY\n          value: \"&lt;your Slab API key&gt;\"\n    toolsets:\n        slab:\n            enabled: true\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p> <p>To test, run:</p> <pre><code>holmes ask \"Why is my pod failing, if it's a crashloopbackoff use the runbooks from Slab\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/slab/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description fetch_slab_document Fetch a document from Slab. Use this to fetch runbooks if they are present before starting your investigation."},{"location":"development/","title":"Development","text":"<p>Extend HolmesGPT with custom integrations and contribute to the project.</p>"},{"location":"development/#available-development-guides","title":"Available Development Guides","text":"<ul> <li>Contributing Guidelines - How to contribute to the HolmesGPT project</li> <li>Contributing Documentation - How to contribute to documentation</li> </ul>"},{"location":"development/#evaluations","title":"Evaluations","text":"<ul> <li>Evaluations Overview - Understanding HolmesGPT's evaluation framework</li> <li>Adding a New Eval - Create your own evaluation tests</li> <li>Reporting with Braintrust - Analyze evaluation results</li> </ul>"},{"location":"development/#development-resources","title":"Development Resources","text":"<ul> <li>Source Code: GitHub Repository</li> <li>Issue Tracker: GitHub Issues</li> <li>Community: Slack Channel</li> <li>Documentation: You're reading it!</li> </ul>"},{"location":"development/evals/","title":"HolmesGPT Evaluations","text":"<p>Evaluations are automated tests that measure HolmesGPT's accuracy on real-world scenarios.</p> <p>They are used to both catch regressions and measure the impact of new features.</p> <p>Example: pod crashloop eval.</p>"},{"location":"development/evals/#eval-tags","title":"Eval Tags","text":"<p>Evals are tagged and grouped into categories. Two common tags are <code>easy</code> and <code>medium</code>:</p> <ul> <li><code>easy</code> - regression tests - scenarios that HolmesGPT passes today and must continue to pass after any change</li> <li><code>medium</code> - more challenging scenarios that push boundaries of what HolmesGPT can do</li> </ul> <p>Changes to HolmesGPT are good if they allow us to promote an eval from <code>easy</code> to <code>medium</code> without increasing latency by too much.</p>"},{"location":"development/evals/#getting-started","title":"Getting Started","text":""},{"location":"development/evals/#prerequisites","title":"Prerequisites","text":"<p>Install HolmesGPT python dependencies:</p> <pre><code>poetry install --with=dev\n</code></pre>"},{"location":"development/evals/#basic-commands","title":"Basic Commands","text":"<pre><code># Run all easy evals - these should always pass assuming you have a kubernetes cluster with sufficient resources and a 'good enough model' (e.g. gpt-4o)\nRUN_LIVE=true poetry run pytest -m 'llm and easy' --no-cov\n\n# Run a specific eval\nRUN_LIVE=true poetry run pytest tests/llm/test_ask_holmes.py -k \"01_how_many_pods\"\n\n# Run evals with a specific tag\nRUN_LIVE=true poetry run pytest -m \"llm and logs\" --no-cov\n</code></pre>"},{"location":"development/evals/#testing-different-models","title":"Testing Different Models","text":"<p>The <code>MODEL</code> environment variable is equivalent to the <code>--model</code> flag on the <code>holmes ask</code> CLI command. You can test HolmesGPT with different LLM providers:</p> <pre><code># Test with GPT-4 (default)\nRUN_LIVE=true MODEL=gpt-4o poetry run pytest -m 'llm and easy'\n\n# Test with Claude\n# Note: CLASSIFIER_MODEL must be set to OpenAI or Azure as Anthropic models are not currently supported for classification\nRUN_LIVE=true MODEL=anthropic/claude-3-5-sonnet-20241022 CLASSIFIER_MODEL=gpt-4o poetry run pytest -m 'llm and easy'\n\n# Test with Azure OpenAI\n# Set required Azure environment variables for your deployment\nexport AZURE_API_KEY=your-azure-api-key\nexport AZURE_API_BASE=https://your-deployment.openai.azure.com/\nexport AZURE_API_VERSION=2024-02-15-preview\nRUN_LIVE=true MODEL=azure/your-deployment-name CLASSIFIER_MODEL=azure/your-deployment-name poetry run pytest -m 'llm and easy'\n</code></pre> <p>Important Notes:</p> <ul> <li>When using Anthropic models, you must set <code>CLASSIFIER_MODEL</code> to an OpenAI or Azure model because the evaluation framework's classifier currently only supports these providers</li> <li>For any model provider, ensure you have the necessary API keys and environment variables set (e.g., <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, <code>AZURE_API_KEY</code>)</li> <li>The model specified here is passed directly to LiteLLM, so any model supported by LiteLLM can be used</li> </ul>"},{"location":"development/evals/#multi-model-benchmarking","title":"Multi-Model Benchmarking","text":"<p>HolmesGPT supports running evaluations across multiple models simultaneously to compare their performance:</p> <pre><code># Test multiple models in a single run\n# Models are specified as comma-separated list\nRUN_LIVE=true MODEL=gpt-4o,anthropic/claude-3-5-sonnet-20241022,gpt-4o-mini \\\n  CLASSIFIER_MODEL=gpt-4o \\\n  poetry run pytest -m 'llm and easy' --no-cov\n\n# Run with multiple iterations for statistically significant results\nRUN_LIVE=true ITERATIONS=10 \\\n  MODEL=gpt-4o,anthropic/claude-3-5-sonnet-20241022 \\\n  CLASSIFIER_MODEL=gpt-4o \\\n  poetry run pytest -m 'llm and easy' -n 10\n\n# Test specific scenario across models\nRUN_LIVE=true MODEL=gpt-4o,gpt-4o-mini \\\n  poetry run pytest tests/llm/test_ask_holmes.py -k \"01_how_many_pods\"\n</code></pre> <p>When running multi-model benchmarks: - Results will show a Model Comparison Table with side-by-side performance metrics - Each model's pass rate, execution times, and P90 percentiles are displayed - Tests are parameterized by model, so you'll see separate results for each model/test combination - Use <code>CLASSIFIER_MODEL</code> to ensure consistent scoring across all models</p>"},{"location":"development/evals/#running-evals-with-multiple-iterations","title":"Running Evals with Multiple Iterations","text":"<p>LLMs are non-deterministic - they produce different outputs for the same input. 10 iterations is a good rule of thumb for reliable results.</p> <pre><code># Recommended: Run with multiple iterations\nRUN_LIVE=true ITERATIONS=10 poetry run pytest -m 'llm and easy' --no-cov\n\n# Quick check: Single run (less reliable)\nRUN_LIVE=true poetry run pytest -m 'llm and easy' --no-cov\n</code></pre>"},{"location":"development/evals/#using-run_livetrue-vs-mock-data","title":"Using RUN_LIVE=true vs Mock Data","text":"<p>Some evals support mock-data and don't need a live Kubernetes cluster to run. However, for the most accurate evaluation you should set <code>RUN_LIVE=true</code> which tests HolmesGPT with a live Kubernetes cluster not mock data.</p> <p>This is important because LLMs can take multiple paths to reach conclusions, and mock data only captures one path. See Using Mock Data for rare cases when mocks are necessary.</p>"},{"location":"development/evals/#environment-variables","title":"Environment Variables","text":"<p>Essential variables for controlling test behavior:</p> Variable Purpose Example <code>RUN_LIVE</code> Use real tools instead of mocks <code>RUN_LIVE=true</code> <code>ITERATIONS</code> Run each test N times <code>ITERATIONS=10</code> <code>MODEL</code> LLM to test <code>MODEL=gpt-4o</code> <code>CLASSIFIER_MODEL</code> LLM for scoring (needed for Anthropic) <code>CLASSIFIER_MODEL=gpt-4o</code> <code>ASK_HOLMES_TEST_TYPE</code> Message building flow (<code>cli</code> or <code>server</code>) <code>ASK_HOLMES_TEST_TYPE=server</code>"},{"location":"development/evals/#ask_holmes_test_type-details","title":"ASK_HOLMES_TEST_TYPE Details","text":"<p>The <code>ASK_HOLMES_TEST_TYPE</code> environment variable controls how messages are built in ask_holmes tests:</p> <ul> <li><code>cli</code> (default): Uses <code>build_initial_ask_messages</code> like the CLI ask() command. This mode:</li> <li>Simulates the CLI interface behavior</li> <li>Does not support conversation history tests (will skip them)</li> <li> <p>Includes runbook loading and system prompts as done in the CLI</p> </li> <li> <p><code>server</code>: Uses <code>build_chat_messages</code> with ChatRequest for server-style flow. This mode:</p> </li> <li>Simulates the API/server interface behavior</li> <li>Supports conversation history tests</li> <li>Uses the ChatRequest model for message building</li> </ul> <pre><code># Test with CLI-style message building (default)\nRUN_LIVE=true poetry run pytest -k \"test_name\"\n\n# Test with server-style message building\nRUN_LIVE=true ASK_HOLMES_TEST_TYPE=server poetry run pytest -k \"test_name\"\n</code></pre>"},{"location":"development/evals/#advanced-usage","title":"Advanced Usage","text":""},{"location":"development/evals/#parallel-execution","title":"Parallel Execution","text":"<p>Speed up test runs with parallel workers:</p> <pre><code># Run with 10 parallel workers\nRUN_LIVE=true ITERATIONS=10 poetry run pytest tests/llm/ -n 10\n</code></pre>"},{"location":"development/evals/#debugging-failed-tests","title":"Debugging Failed Tests","text":"<p>When tests fail, use these techniques to investigate:</p> <pre><code># 1. Verbose output to see details\nRUN_LIVE=true pytest -vv -s tests/llm/test_ask_holmes.py -k \"failing_test\"\n\n# 2. Keep resources after test for inspection\nRUN_LIVE=true pytest -k \"test\" --skip-cleanup\n\n# 3. Iterate quickly without setup/cleanup\nRUN_LIVE=true pytest -k \"test\" --skip-setup --skip-cleanup\n\n# 4. Clean up when done debugging\nRUN_LIVE=true pytest -k \"test\" --skip-setup\n</code></pre>"},{"location":"development/evals/#model-comparison-workflow","title":"Model Comparison Workflow","text":""},{"location":"development/evals/#recommended-multi-model-testing-single-run","title":"Recommended: Multi-Model Testing (Single Run)","text":"<p>Use the <code>MODEL</code> environment variable to test multiple models in a single run:</p> <pre><code># Compare multiple models simultaneously - RECOMMENDED approach\nRUN_LIVE=true ITERATIONS=10 \\\n  MODEL=gpt-4o,anthropic/claude-3-5-sonnet-20241022,gpt-4o-mini \\\n  CLASSIFIER_MODEL=gpt-4o \\\n  poetry run pytest -m 'llm and easy' -n 10\n\n# This will generate a comparison table showing:\n# - Side-by-side pass rates for each model\n# - Execution time comparisons\n# - Cost comparisons\n# - Best performing models summary\n</code></pre>"},{"location":"development/evals/#alternative-single-model-testing-separate-runs","title":"Alternative: Single-Model Testing (Separate Runs)","text":"<p>For cases where you need separate experiments or different configurations per model:</p> <pre><code># Run separate experiments for each model\n# Useful when you need different settings or want to track experiments separately\n\n# 1. Baseline with GPT-4\nRUN_LIVE=true ITERATIONS=10 EXPERIMENT_ID=baseline_gpt4o MODEL=gpt-4o pytest -n 10 tests/llm/\n\n# 2. Compare with Claude (using GPT-4 as classifier since Anthropic models can't classify)\nRUN_LIVE=true ITERATIONS=10 EXPERIMENT_ID=claude35 MODEL=anthropic/claude-3-5-sonnet-20241022 CLASSIFIER_MODEL=gpt-4o pytest -n 10 tests/llm/\n\n# 3. Test a smaller model\nRUN_LIVE=true ITERATIONS=10 EXPERIMENT_ID=gpt4o_mini MODEL=gpt-4o-mini pytest -n 10 tests/llm/\n</code></pre>"},{"location":"development/evals/#braintrust-integration","title":"Braintrust Integration","text":"<p>Results are automatically tracked if Braintrust is configured:</p> <pre><code># Set these once in your environment\nexport BRAINTRUST_API_KEY=your-key\nexport BRAINTRUST_ORG=your-org\n\n# Then run any evaluation command - results will be tracked automatically\nRUN_LIVE=true MODEL=gpt-4o,anthropic/claude-3-5-sonnet-20241022 pytest -m 'llm and easy'\n</code></pre>"},{"location":"development/evals/#test-markers","title":"Test Markers","text":"<p>Filter tests by functionality:</p> <pre><code># Regression tests (should always pass)\nRUN_LIVE=true ITERATIONS=10 poetry run pytest -m \"llm and easy\"\n\n# Challenging tests\nRUN_LIVE=true ITERATIONS=10 poetry run pytest -m \"llm and medium\"\n\n# Tests involving logs\nRUN_LIVE=true poetry run pytest -m \"llm and logs\"\n</code></pre> <p>See <code>pyproject.toml</code> for all available markers.</p>"},{"location":"development/evals/adding-new-eval/","title":"Adding a New Eval","text":"<p>Create test cases that measure HolmesGPT's diagnostic accuracy and help track improvements over time.</p>"},{"location":"development/evals/adding-new-eval/#test-types","title":"Test Types","text":"<ul> <li>Ask Holmes: Chat-like Q&amp;A interactions</li> <li>Investigation: AlertManager event analysis</li> </ul>"},{"location":"development/evals/adding-new-eval/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Create test folder: <code>tests/llm/fixtures/test_ask_holmes/99_your_test/</code></p> </li> <li> <p>Create <code>test_case.yaml</code>: <pre><code>user_prompt: 'Is the nginx pod healthy?'\nexpected_output:\n  - nginx pod is healthy\nbefore_test: kubectl apply -f ./manifest.yaml\nafter_test: kubectl delete -f ./manifest.yaml\n</code></pre></p> </li> <li> <p>Create <code>manifest.yaml</code> with your test scenario: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: default\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n</code></pre></p> </li> <li> <p>Run test: <pre><code>pytest tests/llm/test_ask_holmes.py -k \"99_your_test\" -v\n</code></pre></p> </li> </ol>"},{"location":"development/evals/adding-new-eval/#test-configuration","title":"Test Configuration","text":""},{"location":"development/evals/adding-new-eval/#required-fields","title":"Required Fields","text":"<ul> <li><code>user_prompt</code>: Question for Holmes</li> <li><code>expected_output</code>: List of required elements in response</li> <li><code>before_test</code>/<code>after_test</code>: Setup/teardown commands (run with <code>RUN_LIVE=true</code>)</li> </ul>"},{"location":"development/evals/adding-new-eval/#optional-fields","title":"Optional Fields","text":"<ul> <li><code>tags</code>: List of test markers (e.g., <code>[easy, kubernetes, logs]</code>)</li> <li><code>skip</code>: Boolean to skip test</li> <li><code>skip_reason</code>: Explanation why test is skipped</li> <li><code>mocked_date</code>: Override system time for test (e.g., <code>\"2025-06-23T11:34:00Z\"</code>)</li> <li><code>cluster_name</code>: Specify kubernetes cluster name</li> <li><code>include_files</code>: List of files to include in context (like CLI's <code>--include</code> flag)</li> <li><code>runbooks</code>: Override runbook catalog:   <pre><code>runbooks:\n  catalog:\n    - description: \"Database Connection Troubleshooting\"\n      link: \"database_troubleshooting.md\"\n      update_date: \"2025-07-01\"\n</code></pre></li> <li><code>toolsets</code>: Configure toolsets (can also use separate <code>toolsets.yaml</code> file):   <pre><code>toolsets:\n  aws/lambda:\n    enabled: true\n  aws/cloudwatch:\n    enabled: false\n</code></pre></li> <li><code>port_forwards</code>: Configure port forwarding for tests</li> <li><code>test_env_vars</code>: Environment variables during test execution</li> <li><code>mock_policy</code>: Control mock behavior (<code>always_mock</code>, <code>never_mock</code>, or <code>inherit</code>)</li> <li><code>conversation_history</code>: For multi-turn conversation tests</li> <li><code>expected_sections</code>: For investigation tests only</li> </ul>"},{"location":"development/evals/adding-new-eval/#mock-generation","title":"Mock Generation","text":"<pre><code># Generate mocks for one test\nITERATIONS=100 pytest tests/llm/test_ask_holmes.py -k \"your_test\" --generate-mocks\n\n# Remove any existing mocks for your test and generate them from scratch\npytest tests/llm/test_ask_holmes.py -k \"your_test\" --regenerate-all-mocks\n</code></pre> <p>Mock files are named: <code>{tool_name}_{context}.txt</code></p>"},{"location":"development/evals/adding-new-eval/#advanced-features","title":"Advanced Features","text":""},{"location":"development/evals/adding-new-eval/#toolsets-configuration","title":"Toolsets Configuration","text":"<p>You can configure which toolsets are available during your test in two ways:</p> <ol> <li> <p>Inline in test_case.yaml: <pre><code>toolsets:\n  kubernetes/core:\n    enabled: true\n  aws/cloudwatch:\n    enabled: false\n</code></pre></p> </li> <li> <p>Separate toolsets.yaml file (preferred for complex configurations): <pre><code># toolsets.yaml\ntoolsets:\n  grafana/loki:\n    enabled: true\n    config:\n      url: http://loki.app-143.svc.cluster.local:3100\n      api_key: \"\"\n  kafka/admin:\n    enabled: true\n    config:\n      kafka_clusters:\n        - name: \"kafka\"\n          kafka_broker: \"kafka:9092\"\n</code></pre></p> </li> </ol>"},{"location":"development/evals/adding-new-eval/#port-forwarding","title":"Port Forwarding","text":"<p>Some tests require access to services that are not directly exposed. You can configure port forwards that will be automatically set up and torn down for your test:</p> <pre><code>port_forwards:\n  - namespace: app-01\n    service: rabbitmq\n    local_port: 15672\n    remote_port: 15672\n  - namespace: app-01\n    service: prometheus\n    local_port: 9090\n    remote_port: 9090\n</code></pre> <p>Note: Use unique local ports across all tests to avoid conflicts</p> <p>Port forwards are:</p> <ul> <li>Automatically started before any tests run</li> <li>Shared across all tests in a session to avoid conflicts</li> <li>Always cleaned up after tests complete, even if tests are interrupted</li> <li>Run regardless of <code>--skip-setup</code> or <code>--skip-cleanup</code> flags</li> </ul> <p>Important notes:</p> <ul> <li>Use unique local ports across all tests to avoid conflicts</li> <li>Port forwards persist for the entire test session</li> <li>If a port is already in use, the test will fail with helpful debugging information</li> <li>Use <code>lsof -ti :&lt;port&gt;</code> to find processes using a port</li> <li>Port forwards work with both mock and live (<code>RUN_LIVE=true</code>) test modes</li> </ul>"},{"location":"development/evals/adding-new-eval/#toolset-configuration","title":"Toolset Configuration","text":"<p>Create <code>toolsets.yaml</code> to customize available tools:</p> <pre><code>toolsets:\n  prometheus/metrics:\n    enabled: true\n    config:\n      prometheus_url: \"http://custom-prometheus:9090\"\n  grafana/dashboards:\n    enabled: false  # Disable specific toolsets\n</code></pre>"},{"location":"development/evals/adding-new-eval/#mock-policy","title":"Mock Policy","text":"<pre><code>mock_policy: \"inherit\"  # Options: inherit (default), never_mock, always_mock\n</code></pre> <ul> <li><code>inherit</code>: Use global settings</li> <li><code>never_mock</code>: Force live execution (skipped if RUN_LIVE not set)</li> <li><code>always_mock</code>: Always use mocks (avoid when possible)</li> </ul>"},{"location":"development/evals/adding-new-eval/#custom-runbooks","title":"Custom Runbooks","text":"<pre><code>runbooks:\n  catalog:\n    - description: \"DNS troubleshooting\"\n      link: \"dns-runbook.md\"  # Place .md file in test directory\n</code></pre> <p>Options:</p> <ul> <li>No field: Use default runbooks</li> <li><code>runbooks: {}</code>: No runbooks available</li> <li><code>runbooks: {catalog: [...]}</code>: Custom catalog</li> </ul>"},{"location":"development/evals/adding-new-eval/#tagging","title":"Tagging","text":"<p>Evals support tags for organization, filtering, and reporting purposes. Tags help categorize tests by their characteristics and enable selective test execution.</p>"},{"location":"development/evals/adding-new-eval/#available-tags","title":"Available Tags","text":"<p>The valid tags are defined in the test constants file in the repository.</p> <p>Some examples</p> <ul> <li><code>logs</code> - Tests HolmesGPT's ability to find and interpret logs correctly</li> <li><code>context_window</code> - Tests handling of data that exceeds the LLM's context window</li> <li><code>synthetic</code> - Tests that use manually generated mock data (cannot be run live)</li> <li><code>datetime</code> - Tests date/time handling and interpretation</li> <li>etc.</li> </ul>"},{"location":"development/evals/adding-new-eval/#using-tags-in-test-cases","title":"Using Tags in Test Cases","text":"<p>Add tags to your <code>test_case.yaml</code>:</p> <pre><code>user_prompt: \"Show me the logs for the pod `robusta-holmes` since last Thursday\"\ntags:\n  - logs\n  - datetime\n  - synthetic\nexpected_output:\n  - Database unavailable\n  - Memory pressure\n</code></pre>"},{"location":"development/evals/reporting/","title":"Reporting with Braintrust","text":"<p>This guide explains how to use Braintrust to analyze evaluation results, debug failures, and compare model performance.</p>"},{"location":"development/evals/reporting/#overview","title":"Overview","text":"<p>Braintrust is a platform for tracking and analyzing LLM evaluations. HolmesGPT evals can be used without Braintrust but using Braintrust has a few advantages:</p> <ul> <li>We can track how Holmes perform over time</li> <li>It's easier to run and debug many evals with Braintrust over simpler pytests because Braintrust organises the different components of a HolmesGPT investigation like the input, tool calls, reasoning for scoring, etc.</li> </ul>"},{"location":"development/evals/reporting/#setting-up-braintrust","title":"Setting Up Braintrust","text":""},{"location":"development/evals/reporting/#1-create-account","title":"1. Create Account","text":"<ol> <li>Visit braintrust.dev</li> <li>Sign up for an account</li> <li>Create a new project (e.g., \"HolmesGPT\")</li> </ol>"},{"location":"development/evals/reporting/#2-get-api-key","title":"2. Get API Key","text":"<ol> <li>Click your profile icon (top right)</li> <li>Go to Settings \u2192 API Keys</li> <li>Generate a new API key</li> <li>Copy the key (starts with <code>sk-</code>)</li> </ol>"},{"location":"development/evals/reporting/#3-configure-environment","title":"3. Configure Environment","text":"<pre><code>export BRAINTRUST_API_KEY=sk-your-api-key-here\nexport BRAINTRUST_ORG=your-org-name\n</code></pre> <p>Note: Both <code>BRAINTRUST_API_KEY</code> and <code>BRAINTRUST_ORG</code> are required for Braintrust integration to work.</p>"},{"location":"development/evals/reporting/#running-evaluations-with-braintrust","title":"Running Evaluations with Braintrust","text":""},{"location":"development/evals/reporting/#basic-evaluation-run","title":"Basic Evaluation Run","text":"<pre><code>export BRAINTRUST_API_KEY=sk-your-key\nexport BRAINTRUST_ORG=your-org\n\n# Run all regression tests with Braintrust tracking\nRUN_LIVE=true poetry run pytest -m 'llm and easy' --no-cov\n\n# Run specific test with tracking\nRUN_LIVE=true poetry run pytest tests/llm/test_ask_holmes.py -k \"01_how_many_pods\"\n</code></pre>"},{"location":"development/evals/reporting/#named-experiment","title":"Named Experiment","text":"<pre><code>export BRAINTRUST_API_KEY=sk-your-key\nexport BRAINTRUST_ORG=your-org\n\n# Run with multiple iterations for reliable results\nRUN_LIVE=true ITERATIONS=10 EXPERIMENT_ID=baseline_gpt4o MODEL=gpt-4o poetry run pytest -m 'llm and easy' -n 10\n\n# Compare with different model\nRUN_LIVE=true ITERATIONS=10 EXPERIMENT_ID=claude35 MODEL=anthropic/claude-3-5-sonnet CLASSIFIER_MODEL=gpt-4o poetry run pytest -m 'llm and easy' -n 10\n</code></pre>"},{"location":"development/evals/reporting/#key-environment-variables","title":"Key Environment Variables","text":"Variable Purpose <code>UPLOAD_DATASET</code> Sync test cases to Braintrust <code>EXPERIMENT_ID</code> Name your experiment run. This makes it easier to find and track in Braintrust's UI <code>MODEL</code> The LLM model for Holmes to use <code>CLASSIFIER_MODEL</code> The LLM model to use for scoring the answer (LLM as judge)"},{"location":"development/evals/reporting/#analyzing-evaluation-results","title":"Analyzing Evaluation Results","text":""},{"location":"development/evals/reporting/#output","title":"Output","text":"<p>The main Span of an evaluation will present the input (either the AlertManager issue or the user's question for Ask Holmes) as well as HolmesGPT's answer.</p> <p></p>"},{"location":"development/evals/reporting/#score-types","title":"Score Types","text":"<p>Correctness Score: - Measures accuracy of LLM responses - Values: 0 or 1 - Shows how well output matches expectations</p> <p></p>"},{"location":"development/evals/reporting/#debugging-failed-evaluations","title":"Debugging Failed Evaluations","text":""},{"location":"development/evals/reporting/#1-identify-failing-tests","title":"1. Identify Failing Tests","text":"<p>In the experiment view: - Sort by score (ascending) to see worst performers - Filter by specific score types - Look for patterns in failures</p>"},{"location":"development/evals/reporting/#2-examine-tool-call-traces","title":"2. Examine Tool Call Traces","text":"<p>Click on a failing test to see: - Input: The original prompt/question - Tool Calls: Which tools the LLM invoked - Tool Results: What data each tool returned - Output: The LLM's final response - Expected: What the test expected</p> <p></p>"},{"location":"installation/cli-installation/","title":"Install CLI","text":"<p>Run HolmesGPT from your terminal as a standalone CLI tool.</p>"},{"location":"installation/cli-installation/#installation-options","title":"Installation Options","text":"Homebrew (Mac/Linux)PipxFrom Source (Poetry)Docker Container <ol> <li> <p>Add our tap:    <pre><code>brew tap robusta-dev/homebrew-holmesgpt\n</code></pre></p> </li> <li> <p>Install HolmesGPT:    <pre><code>brew install holmesgpt\n</code></pre></p> </li> <li> <p>To upgrade to the latest version:    <pre><code>brew upgrade holmesgpt\n</code></pre></p> </li> <li> <p>Verify installation:    <pre><code>holmes ask --help\n</code></pre></p> </li> </ol> <ol> <li> <p>Install pipx</p> </li> <li> <p>Install HolmesGPT:    <pre><code>pipx install holmesgpt\n</code></pre></p> </li> <li> <p>Verify installation:    <pre><code>holmes ask --help\n</code></pre></p> </li> </ol> <p>For development or custom builds:</p> <ol> <li> <p>Install Poetry</p> </li> <li> <p>Install HolmesGPT:    <pre><code>git clone https://github.com/robusta-dev/holmesgpt.git\ncd holmesgpt\npoetry install --no-root\n</code></pre></p> </li> <li> <p>Verify installation:    <pre><code>poetry run holmes ask --help\n</code></pre></p> </li> </ol> <p>Run HolmesGPT using the prebuilt Docker container:</p> <pre><code>docker run -it --net=host \\\n  -e OPENAI_API_KEY=\"your-api-key\" \\\n  -v ~/.holmes:/root/.holmes \\\n  -v ~/.aws:/root/.aws \\\n  -v ~/.config/gcloud:/root/.config/gcloud \\\n  -v $HOME/.kube/config:/root/.kube/config \\\n  us-central1-docker.pkg.dev/genuine-flight-317411/devel/holmes ask \"what pods are unhealthy and why?\"\n</code></pre> <p>Note: Pass environment variables using <code>-e</code> flags. An example for OpenAI is shown above. Adjust it for other AI providers by passing <code>-e GEMINI_API_KEY</code>, <code>-e ANTHROPIC_API_KEY</code>, etc. See the Environment Variables Reference for a complete list.</p>"},{"location":"installation/cli-installation/#quick-start","title":"Quick Start","text":"<p>After installation, choose your AI provider and follow the steps below. See supported AI Providers for more details.</p> OpenAI (Default)Azure OpenAIAWS BedrockAnthropic ClaudeGoogle GeminiGoogle Vertex AIOllama <ol> <li> <p>Set up API key:     <pre><code>export OPENAI_API_KEY=\"your-api-key\"\n</code></pre></p> <p>See OpenAI Configuration for more details.</p> </li> <li> <p>Create a test pod to investigate:     <pre><code>kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/main/pending_pods/pending_pod_node_selector.yaml\n</code></pre></p> </li> <li> <p>Ask your first question:     <pre><code># Uses gpt-4o by default\nholmes ask \"what is wrong with the user-profile-import pod?\"\n\n# Or specify a different model\nholmes ask \"what is wrong with the user-profile-import pod?\" --model=\"gpt-5\"\n</code></pre></p> </li> </ol> <ol> <li> <p>Set up API key:     <pre><code>export AZURE_API_VERSION=\"2024-02-15-preview\"\nexport AZURE_API_BASE=\"https://your-resource.openai.azure.com\"\nexport AZURE_API_KEY=\"your-azure-api-key\"\n</code></pre></p> <p>See Azure OpenAI Configuration for more details.</p> </li> <li> <p>Create a test pod to investigate:     <pre><code>kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/main/pending_pods/pending_pod_node_selector.yaml\n</code></pre></p> </li> <li> <p>Ask your first question:     <pre><code>holmes ask \"what is wrong with the user-profile-import pod?\" --model=\"azure/&lt;your-model-name&gt;\"\n</code></pre></p> </li> </ol> <ol> <li> <p>Set up API key:     <pre><code>export AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_DEFAULT_REGION=\"your-region\"\n</code></pre></p> <p>See AWS Bedrock Configuration for more details.</p> </li> <li> <p>Create a test pod to investigate:     <pre><code>kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/main/pending_pods/pending_pod_node_selector.yaml\n</code></pre></p> </li> <li> <p>Ask your first question:     <pre><code>holmes ask \"what is wrong with the user-profile-import pod?\" --model=\"bedrock/&lt;your-model-name&gt;\"\n</code></pre></p> <p>Note: You must install <code>boto3&gt;=1.28.57</code> and replace <code>&lt;your-model-name&gt;</code> with an actual model name like <code>anthropic.claude-3-5-sonnet-20240620-v1:0</code>. See Finding Available Models for instructions.</p> </li> </ol> <p>Ask follow-up questions to refine your investigation</p> <ol> <li> <p>Set up API key:     <pre><code>export ANTHROPIC_API_KEY=\"your-api-key\"\n</code></pre></p> <p>See Anthropic Configuration for more details.</p> </li> <li> <p>Create a test pod to investigate:     <pre><code>kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/main/pending_pods/pending_pod_node_selector.yaml\n</code></pre></p> </li> <li> <p>Ask your first question:     <pre><code>holmes ask \"what is wrong with the user-profile-import pod?\" --model=\"anthropic/&lt;your-model-name&gt;\"\n</code></pre></p> </li> </ol> <ol> <li> <p>Set up API key:     <pre><code>export GEMINI_API_KEY=\"your-gemini-api-key\"\n</code></pre></p> <p>See Google Gemini Configuration for more details.</p> </li> <li> <p>Create a test pod to investigate:     <pre><code>kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/main/pending_pods/pending_pod_node_selector.yaml\n</code></pre></p> </li> <li> <p>Ask your first question:     <pre><code>holmes ask \"what is wrong with the user-profile-import pod?\" --model=\"gemini/&lt;your-gemini-model&gt;\"\n</code></pre></p> </li> </ol> <ol> <li> <p>Set up credentials:     <pre><code>export VERTEXAI_PROJECT=\"your-project-id\"\nexport VERTEXAI_LOCATION=\"us-central1\"\nexport GOOGLE_APPLICATION_CREDENTIALS=\"path/to/service-account-key.json\"\n</code></pre></p> <p>See Google Vertex AI Configuration for more details.</p> </li> <li> <p>Create a test pod to investigate:     <pre><code>kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/main/pending_pods/pending_pod_node_selector.yaml\n</code></pre></p> </li> <li> <p>Ask your first question:     <pre><code>holmes ask \"what is wrong with the user-profile-import pod?\" --model=\"vertex_ai/&lt;your-vertex-model&gt;\"\n</code></pre></p> </li> </ol> <ol> <li> <p>Set up API key:     No API key required for local Ollama installation.</p> <p>See Ollama Configuration for more details.</p> </li> <li> <p>Create a test pod to investigate:     <pre><code>kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/main/pending_pods/pending_pod_node_selector.yaml\n</code></pre></p> </li> <li> <p>Ask your first question:     <pre><code>holmes ask \"what is wrong with the user-profile-import pod?\" --model=\"ollama/&lt;your-model-name&gt;\"\n</code></pre></p> </li> </ol> <p>Note: Only LiteLLM supported Ollama models work with HolmesGPT. Check the LiteLLM Ollama documentation for supported models.</p>"},{"location":"installation/cli-installation/#next-steps","title":"Next Steps","text":"<ul> <li>Add Data Sources - Use built-in toolsets to connect with ArgoCD, Confluence, and monitoring tools</li> <li>Connect Remote MCP Servers - Extend capabilities with external MCP servers</li> </ul>"},{"location":"installation/cli-installation/#need-help","title":"Need Help?","text":"<ul> <li>Join our Slack - Get help from the community</li> <li>Request features on GitHub - Suggest improvements or report bugs</li> <li>Troubleshooting guide - Common issues and solutions</li> </ul>"},{"location":"installation/kubernetes-installation/","title":"Install Helm Chart","text":"<p>Deploy HolmesGPT as a service in your Kubernetes cluster with an HTTP API.</p> <p>When to use the Helm chart?</p> <p>Most users should use the CLI or UI/TUI instead. Using the Helm chart is only recommended if you're building a custom integration over an HTTP API.</p>"},{"location":"installation/kubernetes-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster</li> <li>Helm</li> <li>kubectl configured to access your cluster</li> <li>Supported AI Provider API key.</li> </ul>"},{"location":"installation/kubernetes-installation/#installation","title":"Installation","text":"<ol> <li> <p>Add the Helm repository: <pre><code>helm repo add robusta https://robusta-charts.storage.googleapis.com\nhelm repo update\n</code></pre></p> </li> <li> <p>Create <code>values.yaml</code> file:</p> <p>Create a <code>values.yaml</code> file to configure HolmesGPT with your API key:</p> OpenAIAnthropicAzure OpenAIOther AI Providers <pre><code># values.yaml\nadditionalEnvVars:\n- name: OPENAI_API_KEY\n  value: \"your-openai-api-key\"\n# Or load from secret:\n# - name: OPENAI_API_KEY\n#   valueFrom:\n#     secretKeyRef:\n#       name: holmes-secrets\n#       key: openai-api-key\n</code></pre> <pre><code># values.yaml\nadditionalEnvVars:\n- name: ANTHROPIC_API_KEY\n  value: \"your-anthropic-api-key\"\n# Or load from secret:\n# - name: ANTHROPIC_API_KEY\n#   valueFrom:\n#     secretKeyRef:\n#       name: holmes-secrets\n#       key: anthropic-api-key\n</code></pre> <pre><code># values.yaml\nadditionalEnvVars:\n- name: AZURE_API_KEY\n  value: \"your-azure-api-key\"\n- name: AZURE_API_BASE\n  value: \"https://your-resource.openai.azure.com/\"\n- name: AZURE_API_VERSION\n  value: \"2024-02-15-preview\"\n# Or load from secret:\n# - name: AZURE_API_KEY\n#   valueFrom:\n#     secretKeyRef:\n#       name: holmes-secrets\n#       key: azure-api-key\n# - name: AZURE_API_BASE\n#   valueFrom:\n#     secretKeyRef:\n#       name: holmes-secrets\n#       key: azure-api-base\n</code></pre> <pre><code># values.yaml\nadditionalEnvVars:\n# Google Gemini\n- name: GEMINI_API_KEY\n  value: \"your-gemini-api-key\"\n# AWS Bedrock\n- name: AWS_ACCESS_KEY_ID\n  value: \"your-access-key\"\n- name: AWS_SECRET_ACCESS_KEY\n  value: \"your-secret-key\"\n- name: AWS_REGION_NAME\n  value: \"your-region\"\n# Or load from secret:\n# - name: GEMINI_API_KEY\n#   valueFrom:\n#     secretKeyRef:\n#       name: holmes-secrets\n#       key: gemini-api-key\n</code></pre> <p>Configuration Guide: Each AI provider requires different environment variables. See the AI Providers documentation for the specific environment variables needed for your chosen provider, then add them to the <code>additionalEnvVars</code> section as shown above. For a complete list of all environment variables, see the Environment Variables Reference.</p> </li> <li> <p>Install HolmesGPT: <pre><code>helm install holmesgpt robusta/holmes -f values.yaml\n</code></pre></p> </li> </ol>"},{"location":"installation/kubernetes-installation/#usage","title":"Usage","text":"<p>After installation, test the service with a simple API call:</p> <pre><code># Port forward to access the service locally\n# Note: Service name is {release-name}-holmes\nkubectl port-forward svc/holmesgpt-holmes 8080:80\n\n# If you used a different release name or namespace:\n# kubectl port-forward svc/{your-release-name}-holmes 8080:80 -n {your-namespace}\n\n# Test with a basic question\ncurl -X POST http://localhost:8080/api/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"ask\": \"list pods in namespace default?\"}'\n</code></pre> <p>Note: Responses may take some time when HolmesGPT needs to gather large amounts of data to answer your question. Streaming APIs are coming soon to stream results.</p> <p>For complete API documentation, see the HTTP API Reference.</p>"},{"location":"installation/kubernetes-installation/#upgrading","title":"Upgrading","text":"<pre><code>helm repo update\nhelm upgrade holmesgpt robusta/holmes -f values.yaml\n</code></pre>"},{"location":"installation/kubernetes-installation/#uninstalling","title":"Uninstalling","text":"<pre><code>helm uninstall holmesgpt\n</code></pre>"},{"location":"installation/kubernetes-installation/#need-help","title":"Need Help?","text":"<ul> <li>Join our Slack - Get help from the community</li> <li>Request features on GitHub - Suggest improvements or report bugs</li> <li>Troubleshooting guide - Common issues and solutions</li> </ul>"},{"location":"installation/python-installation/","title":"Install Python SDK","text":"<p>Embed HolmesGPT in your own applications for programmatic root cause analysis, based on observability data.</p>"},{"location":"installation/python-installation/#install-holmesgpt-python-package","title":"Install HolmesGPT Python Package","text":"<pre><code>pip install holmesgpt # Installs latest stable version\n</code></pre> <p>Install unreleased version from GitHub: <pre><code>pip install \"https://github.com/robusta-dev/holmesgpt/archive/refs/heads/master.zip\"\n</code></pre></p>"},{"location":"installation/python-installation/#quick-start","title":"Quick Start","text":"<pre><code>import os\nfrom holmes.config import Config\nfrom holmes.core.prompt import build_initial_ask_messages\nfrom rich.console import Console\n\nprint(\"\ud83d\ude80 Initializing HolmesGPT...\")\n\n# Create configuration\nprint(\"Creating configuration...\")\nconfig = Config(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    model=\"gpt-4o\",\n    max_steps=10\n)\nprint(f\"\u2705 Configuration created with model: {config.model}\")\n\n# Create AI instance and console\nprint(\"Creating AI instance...\")\nai = config.create_console_toolcalling_llm()\nconsole = Console()\nprint(\"\u2705 AI instance ready\")\n\n# Ask a question\nquestion = \"what pods are failing in production?\"\nprint(f\"\\n\ud83d\udd0d Asking: '{question}'\")\n\n# Build initial messages with system prompt\nmessages = build_initial_ask_messages(\n    console=console,\n    initial_user_prompt=question,\n    file_paths=None,\n    tool_executor=ai.tool_executor,\n    runbooks=config.get_runbook_catalog(),\n    system_prompt_additions=None\n)\n\nprint(\"Holmes is thinking...\")\nresponse = ai.call(messages)\nprint(f\"Holmes: {response.result}\")\n</code></pre>"},{"location":"installation/python-installation/#tool-details-example","title":"Tool Details Example","text":"<p>Here's a complete working example that shows detailed progress, available tools, toolsets, and which tools Holmes uses:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nComplete example of using HolmesGPT Python SDK with progress tracking\n\"\"\"\n\nimport os\nfrom holmes.config import Config\nfrom holmes.core.prompt import build_initial_ask_messages\nfrom rich.console import Console\n\ndef main():\n    print(\"\ud83d\ude80 Starting HolmesGPT Python SDK Example\")\n    print(\"=\" * 60)\n\n    # Set API key (you can also set OPENAI_API_KEY environment variable)\n    api_key = os.getenv(\"OPENAI_API_KEY\", \"your-api-key-here\")\n\n    print(\"Step 1: Creating configuration...\")\n    # Create configuration\n    config = Config(\n        api_key=api_key,\n        model=\"gpt-4o\",\n        max_steps=10\n    )\n    print(f\"\u2705 Configuration created with model: {config.model}\")\n\n    print(\"\\nStep 2: Creating AI instance...\")\n    # Create AI instance and console\n    ai = config.create_console_toolcalling_llm()\n    console = Console()\n    print(\"\u2705 AI instance created successfully\")\n\n    print(\"\\nStep 3: Listing available toolsets...\")\n    # Show available toolsets\n    toolsets = ai.tool_executor.toolsets\n    print(f\"Loaded {len(toolsets)} toolsets:\")\n    for toolset in toolsets:\n        print(f\"   \u2022 {toolset.name} ({'enabled' if toolset.enabled else 'disabled'})\")\n\n    print(\"\\nStep 4: Listing available tools from loaded toolsets...\")\n    # Show available tools\n    available_tools = list(ai.tool_executor.tools_by_name.keys())\n    print(f\"Listed {len(available_tools)} tools:\")\n    for tool in sorted(available_tools):\n        print(f\"   \u2022 {tool}\")\n\n    print(\"\\nStep 5: Asking questions...\")\n    # Ask questions\n    questions = [\n        \"what pods are failing in production?\",\n        \"show me recent kubernetes events\",\n        \"what are the resource usage patterns in my cluster?\"\n    ]\n\n    for i, question in enumerate(questions, 1):\n        print(f\"\\n\ud83d\udd0d Question {i}/{len(questions)}: {question}\")\n        print(\"=\" * 60)\n\n        try:\n            print(\"Holmes is thinking...\")\n\n            # Build initial messages\n            messages = build_initial_ask_messages(\n                console=console,\n                initial_user_prompt=question,\n                file_paths=None,\n                tool_executor=ai.tool_executor,\n                runbooks=config.get_runbook_catalog(),\n                system_prompt_additions=None\n            )\n\n            response = ai.call(messages)\n            print(f\"Holmes: {response.result}\")\n\n            # Show tools that were used\n            if response and response.tool_calls:\n                tool_names = [tool.tool_name for tool in response.tool_calls]\n                if tool_names:\n                    print(f\"\\nTools used: {tool_names}\")\n\n                    # Print contents of each tool response\n                    print(\"\\nTool responses:\")\n                    for j, tool in enumerate(response.tool_calls, 1):\n                        print(f\"\\n   {j}. {tool.tool_name}:\")\n                        print(f\"      Result: {tool.result}\")\n                        if hasattr(tool, 'error') and tool.error:\n                            print(f\"      Error: {tool.error}\")\n\n        except Exception as e:\n            print(f\"\u274c Error: {e}\")\n\n        print(\"-\" * 60)\n\n    print(\"\\n\u2705 Example completed!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Save this as <code>holmesgpt_tool_details_example.py</code> and run:</p> <pre><code># Make sure your API key is set\nexport OPENAI_API_KEY=\"your-actual-api-key\"\n\n# Run the example\npython holmesgpt_tool_details_example.py\n</code></pre> <p>This will show you:</p> <ul> <li>Configuration creation progress</li> <li>List of available tools (kubectl, prometheus, etc.)</li> <li>List of available toolsets and their status</li> <li>System prompt loading progress</li> <li>Progress for each question being asked</li> <li>Which tools Holmes used for each question</li> </ul>"},{"location":"installation/python-installation/#follow-up-questions-example","title":"Follow-up Questions Example","text":"<p>Here's how to ask follow-up questions that maintain conversation context:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nExample showing how to ask follow-up questions with conversation context\n\"\"\"\n\nimport os\nfrom holmes.config import Config\nfrom holmes.plugins.prompts import load_and_render_prompt\nfrom holmes.core.prompt import build_initial_ask_messages\nfrom rich.console import Console\n\ndef main():\n    print(\"\ud83d\ude80 Starting HolmesGPT Follow-up Questions Example\")\n    print(\"=\" * 60)\n\n    # Create configuration\n    config = Config(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o\",\n        max_steps=10\n    )\n\n    # Create AI instance and console\n    ai = config.create_console_toolcalling_llm()\n    console = Console()\n\n    # First question\n    print(\"\\n\ud83d\udd0d First Question:\")\n    first_question = \"what pods are failing in my cluster?\"\n    print(f\"User: {first_question}\")\n\n    # Build initial messages (includes system prompt + first user message)\n    messages = build_initial_ask_messages(\n        console=console,\n        initial_user_prompt=first_question,\n        file_paths=None,\n        tool_executor=ai.tool_executor,\n        runbooks=config.get_runbook_catalog(),\n        system_prompt_additions=None\n    )\n\n    # Call AI with initial messages\n    print(\"Holmes is thinking...\")\n    response = ai.call(messages)\n    messages = response.messages  # Update messages with full conversation\n\n    print(f\"Holmes: {response.result}\")\n\n    # Follow-up question\n    followup_question = \"Can you show me the logs for those failing pods?\"\n\n    print(f\"\\n\ud83d\udd0d Follow-up Question:\")\n    print(f\"User: {followup_question}\")\n\n    # Add the follow-up question to the conversation\n    messages.append({\"role\": \"user\", \"content\": followup_question})\n\n    # Call AI with updated message history\n    print(\"Holmes is thinking...\")\n    response = ai.call(messages)\n    messages = response.messages  # Update messages with latest response\n\n    print(f\"Holmes: {response.result}\")\n\n    # Show tools used\n    if response.tool_calls:\n        tool_names = [tool.tool_name for tool in response.tool_calls]\n        print(f\"Tools used: {tool_names}\")\n\n    print(\"\\n\u2705 Conversation completed!\")\n    print(f\"Total messages in conversation: {len(messages)}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Key Points for Follow-up Questions:</p> <ol> <li>Use <code>build_initial_ask_messages()</code> for the first question</li> <li>Use <code>ai.call(messages)</code> instead of <code>ai.prompt_call()</code></li> <li>Update messages after each response: <code>messages = response.messages</code></li> <li>Append new questions: <code>messages.append({\"role\": \"user\", \"content\": question})</code></li> <li>Messages contain full conversation history for context</li> </ol>"},{"location":"installation/python-installation/#configuration-options","title":"Configuration Options","text":""},{"location":"installation/python-installation/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from holmes.config import Config\n\n# Basic configuration example\nconfig = Config(\n    api_key=\"your-api-key\",\n    model=\"gpt-4o\",  # or \"claude-3-sonnet\", \"gpt-3.5-turbo\", etc.\n    max_steps=10\n)\n\n# Minimal configuration (API key only)\nconfig = Config(api_key=\"your-api-key\")\n\n# Environment-based configuration\nconfig = Config()  # Will auto-detect API key from OPENAI_API_KEY\n</code></pre>"},{"location":"installation/python-installation/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>from holmes.config import Config\n\n# Complete configuration with custom toolsets and runbooks\nconfig = Config(\n    # LLM settings\n    api_key=\"your-api-key\",\n    model=\"gpt-4o\",\n    max_steps=10,\n\n    # Custom toolsets and runbooks\n    custom_toolsets=[\"/path/to/custom/toolset.yaml\"],\n    custom_runbooks=[\"/path/to/custom/runbook.yaml\"],\n)\n</code></pre>"},{"location":"installation/python-installation/#api-reference","title":"API Reference","text":""},{"location":"installation/python-installation/#config","title":"Config","text":"<p>Main configuration class for HolmesGPT.</p> <p>Constructor Parameters:</p> <ul> <li><code>api_key</code> (str, optional) - LLM API key (can also use environment variables)</li> <li><code>model</code> (str, optional) - Model to use (default: \"gpt-4o\")</li> <li><code>max_steps</code> (int, optional) - Maximum investigation steps (default: 10)</li> <li><code>custom_toolsets</code> (list, optional) - Custom toolset file paths</li> <li><code>custom_runbooks</code> (list, optional) - Custom runbook file paths</li> </ul> <p>Class Methods:</p> <ul> <li><code>Config.load_from_file(path)</code> - Load configuration from YAML file</li> <li><code>Config.load_from_env()</code> - Load configuration from environment variables</li> </ul> <p>Instance Methods:</p> <ul> <li><code>create_console_toolcalling_llm()</code> - Create AI instance for investigations</li> </ul>"},{"location":"installation/python-installation/#toolcallingllm","title":"ToolCallingLLM","text":"<p>Main AI instance for running investigations.</p> <p>Methods:</p> <ul> <li><code>prompt_call(system_prompt, user_prompt)</code> - Ask a question and get response</li> <li><code>call(messages)</code> - Call with full message history</li> </ul>"},{"location":"installation/python-installation/#environment-variables","title":"Environment Variables","text":"<p>Instead of passing <code>api_key</code> to the Config constructor, you can set these environment variables and use <code>Config()</code> without parameters:</p> <pre><code># AI Provider (choose one)\nexport OPENAI_API_KEY=\"your-openai-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\nexport GOOGLE_API_KEY=\"your-google-key\"\n\n# Optional: Custom configuration\nexport HOLMES_CONFIG_PATH=\"/path/to/config.yaml\"\nexport HOLMES_LOG_LEVEL=\"INFO\"\n</code></pre> <p>\ud83d\udcda See Also: Check the Environment Variables Reference for complete documentation of all available environment variables.</p> <p>Usage with environment variables: <pre><code>import os\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nconfig = Config()  # Will auto-detect API key from environment\n</code></pre></p>"},{"location":"installation/python-installation/#need-help","title":"Need Help?","text":"<ul> <li>Join our Slack - Get help from the community</li> <li>Request features on GitHub - Suggest improvements or report bugs</li> <li>Troubleshooting guide - Common issues and solutions</li> </ul>"},{"location":"installation/ui-installation/","title":"Install UI/TUI","text":"<p>Use HolmesGPT through graphical and terminal interfaces via third-party integrations.</p>"},{"location":"installation/ui-installation/#k9s-plugin","title":"K9s Plugin","text":"<p>Integrate HolmesGPT into your K9s Kubernetes terminal for instant analysis.</p> <p></p>"},{"location":"installation/ui-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>K9s must be installed - See the K9s installation guide</li> <li>HolmesGPT CLI and API key - Follow the CLI Installation Guide to install Holmes and configure your AI provider</li> </ul>"},{"location":"installation/ui-installation/#plugin-options","title":"Plugin Options","text":"Basic Plugin (Shift + H) - Quick investigation with predefined question <p>Add to your K9s plugins configuration file:</p> <ul> <li>Linux: <code>~/.config/k9s/plugins.yaml</code> or <code>~/.k9s/plugins.yaml</code></li> <li>macOS: <code>~/Library/Application Support/k9s/plugins.yaml</code> or <code>~/.k9s/plugins.yaml</code></li> <li>Windows: <code>%APPDATA%/k9s/plugins.yaml</code></li> </ul> <p>Read more about K9s plugins here and check your plugin path here.</p> <pre><code>plugins:\n  holmesgpt:\n    shortCut: Shift-H\n    description: Ask HolmesGPT\n    scopes:\n      - all\n    command: bash\n    background: false\n    confirm: false\n    args:\n      - -c\n      - |\n        # Check if we're already using the correct context\n        CURRENT_CONTEXT=$(kubectl config current-context 2&gt;/dev/null || echo \"\")\n        if [ \"$CURRENT_CONTEXT\" = \"$CONTEXT\" ]; then\n          # Already using the correct context, run HolmesGPT directly\n          holmes ask \"why is $NAME of $RESOURCE_NAME in -n $NAMESPACE not working as expected\"\n        else\n          # Create temporary kubeconfig to avoid changing user's system context\n          # K9s passes $CONTEXT but we need to ensure HolmesGPT uses the same context\n          # without permanently switching the user's kubectl context\n          TEMP_KUBECONFIG=$(mktemp)\n          kubectl config view --raw &gt; $TEMP_KUBECONFIG\n          KUBECONFIG=$TEMP_KUBECONFIG kubectl config use-context $CONTEXT\n          # KUBECONFIG environment variable is passed to holmes and all its child processes\n          KUBECONFIG=$TEMP_KUBECONFIG holmes ask \"why is $NAME of $RESOURCE_NAME in -n $NAMESPACE not working as expected\"\n          rm -f $TEMP_KUBECONFIG\n        fi\n        echo \"Press 'q' to exit\"\n        while : ; do\n        read -n 1 k &lt;&amp;1\n        if [[ $k = q ]] ; then\n        break\n        fi\n        done\n</code></pre> Advanced Plugin (Shift + Q) - Interactive plugin with custom questions <p>Add to your K9s plugins configuration file:</p> <ul> <li>Linux: <code>~/.config/k9s/plugins.yaml</code> or <code>~/.k9s/plugins.yaml</code></li> <li>macOS: <code>~/Library/Application Support/k9s/plugins.yaml</code> or <code>~/.k9s/plugins.yaml</code></li> <li>Windows: <code>%APPDATA%/k9s/plugins.yaml</code></li> </ul> <p>Read more about K9s plugins here and check your plugin path here.</p> <pre><code>plugins:\n  custom-holmesgpt:\n    shortCut: Shift-Q\n    description: Custom HolmesGPT Ask\n    scopes:\n      - all\n    command: bash\n    background: false\n    confirm: false\n    args:\n      - -c\n      - |\n        INSTRUCTIONS=\"# Edit the line below. Lines starting with '#' will be ignored.\"\n        DEFAULT_ASK_COMMAND=\"why is $NAME of $RESOURCE_NAME in -n $NAMESPACE not working as expected\"\n        QUESTION_FILE=$(mktemp)\n\n        echo \"$INSTRUCTIONS\" &gt; \"$QUESTION_FILE\"\n        echo \"$DEFAULT_ASK_COMMAND\" &gt;&gt; \"$QUESTION_FILE\"\n\n        # Open the line in the default text editor\n        ${EDITOR:-nano} \"$QUESTION_FILE\"\n\n        # Read the modified line, ignoring lines starting with '#'\n        user_input=$(grep -v '^#' \"$QUESTION_FILE\")\n\n        echo \"Running: holmes ask '$user_input'\"\n        # Check if we're already using the correct context\n        CURRENT_CONTEXT=$(kubectl config current-context 2&gt;/dev/null || echo \"\")\n        if [ \"$CURRENT_CONTEXT\" = \"$CONTEXT\" ]; then\n          # Already using the correct context, run HolmesGPT directly\n          holmes ask \"$user_input\"\n        else\n          # Create temporary kubeconfig to avoid changing user's system context\n          # K9s passes $CONTEXT but we need to ensure HolmesGPT uses the same context\n          # without permanently switching the user's kubectl context\n          TEMP_KUBECONFIG=$(mktemp)\n          kubectl config view --raw &gt; $TEMP_KUBECONFIG\n          KUBECONFIG=$TEMP_KUBECONFIG kubectl config use-context $CONTEXT\n          # KUBECONFIG environment variable is passed to holmes and all its child processes\n          KUBECONFIG=$TEMP_KUBECONFIG holmes ask \"$user_input\"\n          rm -f $TEMP_KUBECONFIG\n        fi\n        echo \"Press 'q' to exit\"\n        while : ; do\n        read -n 1 k &lt;&amp;1\n        if [[ $k = q ]] ; then\n        break\n        fi\n        done\n</code></pre>"},{"location":"installation/ui-installation/#usage","title":"Usage","text":"<ol> <li>Run K9s and select any Kubernetes resource</li> <li>Press Shift + H for quick analysis or Shift + Q for custom questions</li> </ol>"},{"location":"installation/ui-installation/#web-ui-robusta","title":"Web UI (Robusta)","text":"<p>The fastest way to use HolmesGPT is via the managed Robusta SaaS platform.</p> <p></p>"},{"location":"installation/ui-installation/#get-started","title":"Get Started","text":"<ol> <li>Sign up: platform.robusta.dev</li> <li>Connect your cluster: Follow the in-app wizard to install the Robusta agent and configure data sources.</li> <li> <p>Investigate: Use the \"Ask Holmes\" chat to analyze alerts or ask questions like:</p> <ol> <li>\"What pods are failing in production?\"</li> <li>\"Why did this alert fire?\"</li> </ol> </li> </ol> <p>Multiple AI Providers</p> <p>You can configure multiple AI models for users to choose from in the UI. See Using Multiple Providers for configuration details.</p>"},{"location":"installation/ui-installation/#slack-bot-robusta","title":"Slack Bot (Robusta)","text":"<p>First install Robusta SaaS, then tag HolmesGPT in any Slack message for instant analysis.</p> <p></p>"},{"location":"installation/ui-installation/#setup-slack-bot","title":"Setup Slack Bot","text":""},{"location":"installation/ui-installation/#need-help","title":"Need Help?","text":"<ul> <li>Join our Slack - Get help from the community</li> <li>Request features on GitHub - Suggest improvements or report bugs</li> <li>Troubleshooting guide - Common issues and solutions</li> </ul>"},{"location":"reference/environment-variables/","title":"Environment Variables","text":"<p>This page documents all environment variables that can be used to configure HolmesGPT behavior.</p>"},{"location":"reference/environment-variables/#ai-provider-configuration","title":"AI Provider Configuration","text":""},{"location":"reference/environment-variables/#openai","title":"OpenAI","text":"<ul> <li><code>OPENAI_API_KEY</code> - API key for OpenAI models</li> </ul>"},{"location":"reference/environment-variables/#anthropic","title":"Anthropic","text":"<ul> <li><code>ANTHROPIC_API_KEY</code> - API key for Anthropic Claude models</li> </ul>"},{"location":"reference/environment-variables/#google","title":"Google","text":"<ul> <li><code>GEMINI_API_KEY</code> - API key for Google Gemini models</li> <li><code>GOOGLE_API_KEY</code> - Alternative API key for Google services</li> </ul>"},{"location":"reference/environment-variables/#azure-openai","title":"Azure OpenAI","text":"<ul> <li><code>AZURE_API_KEY</code> - API key for Azure OpenAI service</li> <li><code>AZURE_API_BASE</code> - Base URL for Azure OpenAI endpoint</li> <li><code>AZURE_API_VERSION</code> - API version to use (e.g., \"2024-02-15-preview\")</li> </ul>"},{"location":"reference/environment-variables/#aws-bedrock","title":"AWS Bedrock","text":"<ul> <li><code>AWS_ACCESS_KEY_ID</code> - AWS access key ID</li> <li><code>AWS_SECRET_ACCESS_KEY</code> - AWS secret access key</li> <li><code>AWS_DEFAULT_REGION</code> - AWS region for Bedrock</li> </ul>"},{"location":"reference/environment-variables/#google-vertex-ai","title":"Google Vertex AI","text":"<ul> <li><code>VERTEXAI_PROJECT</code> - Google Cloud project ID</li> <li><code>VERTEXAI_LOCATION</code> - Vertex AI location (e.g., \"us-central1\")</li> <li><code>GOOGLE_APPLICATION_CREDENTIALS</code> - Path to service account key JSON file</li> </ul>"},{"location":"reference/environment-variables/#llm-tool-calling-configuration","title":"LLM Tool Calling Configuration","text":""},{"location":"reference/environment-variables/#llms_with_strict_tool_calls","title":"LLMS_WITH_STRICT_TOOL_CALLS","text":"<p>Default: <code>\"azure/gpt-4o, openai/*\"</code></p> <p>Comma-separated list of model patterns that support strict tool calling. When a model matches one of these patterns, HolmesGPT will: - Enable the <code>strict</code> flag for function definitions - Set <code>additionalProperties: false</code> in tool parameter schemas - Enforce stricter schema validation for tool calls</p> <p>This improves reliability of tool calling for supported models by ensuring the LLM adheres more strictly to the defined tool schemas.</p> <p>Example: <pre><code>export LLMS_WITH_STRICT_TOOL_CALLS=\"azure/gpt-4o,openai/*,anthropic/claude-3*\"\n</code></pre></p>"},{"location":"reference/environment-variables/#tool_schema_no_param_object_if_no_params","title":"TOOL_SCHEMA_NO_PARAM_OBJECT_IF_NO_PARAMS","text":"<p>Default: <code>false</code></p> <p>When set to <code>true</code>, removes the <code>parameters</code> object from tool schemas when a tool has no parameters. This is specifically required for Google Gemini models which don't expect a parameters object for parameterless functions.</p> <p>Example: <pre><code>export TOOL_SCHEMA_NO_PARAM_OBJECT_IF_NO_PARAMS=true\n</code></pre></p> <p>Note: This setting is typically only needed when using Gemini models. Other providers handle empty parameter objects correctly.</p>"},{"location":"reference/environment-variables/#holmesgpt-configuration","title":"HolmesGPT Configuration","text":""},{"location":"reference/environment-variables/#holmes_config_path","title":"HOLMES_CONFIG_PATH","text":"<p>Path to a custom HolmesGPT configuration file. If not set, defaults to <code>~/.holmes/config.yaml</code>.</p> <p>Example: <pre><code>export HOLMES_CONFIG_PATH=\"/path/to/custom/config.yaml\"\n</code></pre></p>"},{"location":"reference/environment-variables/#holmes_log_level","title":"HOLMES_LOG_LEVEL","text":"<p>Controls the logging verbosity of HolmesGPT.</p> <p>Values: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code> Default: <code>INFO</code></p> <p>Example: <pre><code>export HOLMES_LOG_LEVEL=\"DEBUG\"\n</code></pre></p>"},{"location":"reference/environment-variables/#holmes_cache_dir","title":"HOLMES_CACHE_DIR","text":"<p>Directory for caching HolmesGPT data and temporary files.</p>"},{"location":"reference/environment-variables/#holmes_post_processing_prompt","title":"HOLMES_POST_PROCESSING_PROMPT","text":"<p>Custom prompt template for post-processing LLM responses.</p>"},{"location":"reference/environment-variables/#data-source-configuration","title":"Data Source Configuration","text":""},{"location":"reference/environment-variables/#prometheus","title":"Prometheus","text":"<ul> <li><code>PROMETHEUS_URL</code> - URL of the Prometheus server</li> </ul>"},{"location":"reference/environment-variables/#confluence","title":"Confluence","text":"<ul> <li><code>CONFLUENCE_BASE_URL</code> - Base URL of Confluence instance</li> <li><code>CONFLUENCE_EMAIL</code> - Email for Confluence authentication</li> <li><code>CONFLUENCE_API_KEY</code> - API key for Confluence</li> </ul>"},{"location":"reference/environment-variables/#github","title":"GitHub","text":"<ul> <li><code>GITHUB_TOKEN</code> - Personal access token for GitHub API</li> </ul>"},{"location":"reference/environment-variables/#datadog","title":"Datadog","text":"<ul> <li><code>DATADOG_APP_KEY</code> - Datadog application key</li> <li><code>DATADOG_API_KEY</code> - Datadog API key</li> </ul>"},{"location":"reference/environment-variables/#aws","title":"AWS","text":"<ul> <li><code>AWS_ACCESS_KEY_ID</code> - AWS access key (also used for AWS toolset)</li> <li><code>AWS_SECRET_ACCESS_KEY</code> - AWS secret key (also used for AWS toolset)</li> <li><code>AWS_DEFAULT_REGION</code> - Default AWS region</li> </ul>"},{"location":"reference/environment-variables/#mongodb-atlas","title":"MongoDB Atlas","text":"<ul> <li><code>MONGODB_ATLAS_PUBLIC_KEY</code> - Public key for MongoDB Atlas API</li> <li><code>MONGODB_ATLAS_PRIVATE_KEY</code> - Private key for MongoDB Atlas API</li> </ul>"},{"location":"reference/environment-variables/#slab","title":"Slab","text":"<ul> <li><code>SLAB_API_KEY</code> - API key for Slab integration</li> </ul>"},{"location":"reference/environment-variables/#testing-and-development","title":"Testing and Development","text":""},{"location":"reference/environment-variables/#run_live","title":"RUN_LIVE","text":"<p>When set to <code>true</code>, enables live execution of commands in tests instead of using mocked responses. Strongly recommended for accurate test results.</p> <p>Example: <pre><code>export RUN_LIVE=true\n</code></pre></p>"},{"location":"reference/environment-variables/#model","title":"MODEL","text":"<p>Override the default LLM model for testing.</p> <p>Example: <pre><code>export MODEL=\"anthropic/claude-3-5-sonnet-20241022\"\n</code></pre></p>"},{"location":"reference/environment-variables/#classifier_model","title":"CLASSIFIER_MODEL","text":"<p>Model to use for scoring test answers (defaults to MODEL if not set). Required when using Anthropic models as the primary model since Anthropic models cannot be used as classifiers.</p> <p>Example: <pre><code>export CLASSIFIER_MODEL=\"gpt-4o\"\n</code></pre></p>"},{"location":"reference/environment-variables/#iterations","title":"ITERATIONS","text":"<p>Number of times to run each test for reliability testing.</p> <p>Example: <pre><code>export ITERATIONS=10\n</code></pre></p>"},{"location":"reference/environment-variables/#braintrust_api_key","title":"BRAINTRUST_API_KEY","text":"<p>API key for Braintrust integration to track test results.</p>"},{"location":"reference/environment-variables/#braintrust_org","title":"BRAINTRUST_ORG","text":"<p>Braintrust organization name (default: \"robustadev\").</p>"},{"location":"reference/environment-variables/#experiment_id","title":"EXPERIMENT_ID","text":"<p>Custom experiment name for tracking test runs in Braintrust.</p>"},{"location":"reference/environment-variables/#ask_holmes_test_type","title":"ASK_HOLMES_TEST_TYPE","text":"<p>Controls message building flow in ask_holmes tests: - <code>cli</code> (default) - Uses CLI-style message building - <code>server</code> - Uses server-style message building with ChatRequest</p>"},{"location":"reference/environment-variables/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/environment-variables/#basic-setup","title":"Basic Setup","text":"<pre><code># Set up OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\nexport HOLMES_LOG_LEVEL=\"INFO\"\n\n# Run HolmesGPT\nholmes ask \"what pods are failing?\"\n</code></pre>"},{"location":"reference/environment-variables/#gemini-configuration","title":"Gemini Configuration","text":"<pre><code># Configure for Gemini models\nexport GEMINI_API_KEY=\"your-key\"\nexport TOOL_SCHEMA_NO_PARAM_OBJECT_IF_NO_PARAMS=true\n\nholmes ask \"analyze cluster health\" --model=\"gemini/gemini-1.5-pro\"\n</code></pre>"},{"location":"reference/environment-variables/#testing-with-strict-tool-calling","title":"Testing with Strict Tool Calling","text":"<pre><code># Enable strict tool calling for additional models\nexport LLMS_WITH_STRICT_TOOL_CALLS=\"azure/gpt-4o,openai/*,custom/model-*\"\nexport RUN_LIVE=true\nexport MODEL=\"custom/model-v2\"\n\npoetry run pytest tests/llm/ -n 6\n</code></pre>"},{"location":"reference/environment-variables/#see-also","title":"See Also","text":"<ul> <li>AI Providers - Detailed configuration for each AI provider</li> <li>CLI Installation - Setting up the CLI with environment variables</li> <li>Helm Configuration - Kubernetes deployment configuration</li> </ul>"},{"location":"reference/helm-configuration/","title":"Helm Configuration","text":"<p>Configuration reference for HolmesGPT Helm chart.</p> <p>Quick Links:</p> <ul> <li>Installation Tutorial - Step-by-step setup guide</li> <li>values.yaml - Complete configuration reference</li> <li>HTTP API Reference - Test your deployment</li> </ul>"},{"location":"reference/helm-configuration/#basic-configuration","title":"Basic Configuration","text":"<pre><code># values.yaml\n# Image settings\nimage: holmes:0.0.0\nregistry: robustadev\n\n# Logging level\nlogLevel: INFO\n\n# send exceptions to sentry\nenableTelemetry: true\n\n# Resource limits\nresources:\n  requests:\n    cpu: 100m\n    memory: 1024Mi\n  limits:\n    memory: 1024Mi\n\n# Enabled/disable/customize specific toolsets\ntoolsets:\n  kubernetes/core:\n    enabled: true\n  kubernetes/logs:\n    enabled: true\n  robusta:\n    enabled: true\n  internet:\n    enabled: true\n  prometheus/metrics:\n    enabled: true\n  ...\n</code></pre>"},{"location":"reference/helm-configuration/#configuration-options","title":"Configuration Options","text":""},{"location":"reference/helm-configuration/#essential-settings","title":"Essential Settings","text":"Parameter Description Default <code>additionalEnvVars</code> Environment variables (API keys, etc.) <code>[]</code> <code>toolsets</code> Enable/disable specific toolsets (see values.yaml) <code>modelList</code> Configure multiple AI models for UI selection. See Using Multiple Providers <code>{}</code> <code>openshift</code> Enable OpenShift compatibility mode <code>false</code> <code>image</code> HolmesGPT image name <code>holmes:0.0.0</code> <code>registry</code> Container registry <code>robustadev</code> <code>logLevel</code> Log level (DEBUG, INFO, WARN, ERROR) <code>INFO</code> <code>enableTelemetry</code> Send exception reports to sentry <code>true</code> <code>certificate</code> Base64 encoded custom CA certificate for outbound HTTPS requests (e.g., LLM API via proxy) <code>\"\"</code> <code>sentryDSN</code> Sentry DSN for telemetry (see values.yaml)"},{"location":"reference/helm-configuration/#api-key-configuration","title":"API Key Configuration","text":"<p>The most important configuration is setting up API keys for your chosen AI provider:</p> <pre><code>additionalEnvVars:\n- name: OPENAI_API_KEY\n  value: \"your-api-key\"\n# Or load from secret:\n# - name: OPENAI_API_KEY\n#   valueFrom:\n#     secretKeyRef:\n#       name: holmes-secrets\n#       key: openai-api-key\n</code></pre>"},{"location":"reference/helm-configuration/#toolset-configuration","title":"Toolset Configuration","text":"<p>Control which capabilities HolmesGPT has access to:</p> <pre><code>toolsets:\n  kubernetes/core:\n    enabled: true      # Core Kubernetes functionality\n  kubernetes/logs:\n    enabled: true      # Kubernetes logs access\n  robusta:\n    enabled: true      # Robusta platform integration\n  internet:\n    enabled: true      # Internet access for documentation\n  prometheus/metrics:\n    enabled: true      # Prometheus metrics access\n</code></pre>"},{"location":"reference/helm-configuration/#service-account-configuration","title":"Service Account Configuration","text":"<pre><code># Create service account (default: true)\ncreateServiceAccount: true\n\n# Use custom service account name\ncustomServiceAccountName: \"\"\n\n# Service account settings\nserviceAccount:\n  imagePullSecrets: []\n  annotations: {}\n\n# Custom RBAC rules\ncustomClusterRoleRules: []\n</code></pre>"},{"location":"reference/helm-configuration/#resource-configuration","title":"Resource Configuration","text":"<pre><code>resources:\n  requests:\n    cpu: 100m\n    memory: 1024Mi\n  limits:\n    cpu: 100m        # Optional CPU limit\n    memory: 1024Mi\n</code></pre>"},{"location":"reference/helm-configuration/#toolset-configuration_1","title":"Toolset Configuration","text":"<p>Enable or disable specific toolsets:</p> <pre><code>toolsets:\n  kubernetes/core:\n    enabled: true      # Core Kubernetes functionality\n  kubernetes/logs:\n    enabled: true      # Kubernetes logs access\n  robusta:\n    enabled: true      # Robusta platform integration\n  internet:\n    enabled: true      # Internet access for documentation\n  prometheus/metrics:\n    enabled: true      # Prometheus metrics access\n</code></pre>"},{"location":"reference/helm-configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"reference/helm-configuration/#scheduling","title":"Scheduling","text":"<pre><code># Node selection\n# nodeSelector:\n#   kubernetes.io/os: linux\n\n# Pod affinity/anti-affinity\naffinity: {}\n\n# Tolerations\ntolerations: []\n\n# Priority class\npriorityClassName: \"\"\n</code></pre>"},{"location":"reference/helm-configuration/#additional-configuration","title":"Additional Configuration","text":"<pre><code># Additional environment variables\nadditionalEnvVars: []\nadditional_env_vars: []  # Legacy, use additionalEnvVars instead\n\n# Image pull secrets\nimagePullSecrets: []\n\n# Additional volumes\nadditionalVolumes: []\n\n# Additional volume mounts\nadditionalVolumeMounts: []\n\n# OpenShift compatibility mode\nopenshift: false\n\n# Post-processing configuration\nenablePostProcessing: false\npostProcessingPrompt: \"builtin://generic_post_processing.jinja2\"\n\n# Account creation\nenableAccountsCreate: true\n\n# MCP servers configuration\nmcp_servers: {}\n\n# Model list configuration for multiple AI providers (UI only)\n# See: https://holmesgpt.dev/ai-providers/using-multiple-providers/\nmodelList: {}\n</code></pre>"},{"location":"reference/helm-configuration/#example-configurations","title":"Example Configurations","text":""},{"location":"reference/helm-configuration/#minimal-setup","title":"Minimal Setup","text":"<pre><code># values.yaml\nimage: holmes:0.0.0\nregistry: robustadev\nlogLevel: INFO\nenableTelemetry: false\n\nresources:\n  requests:\n    cpu: 100m\n    memory: 512Mi\n  limits:\n    memory: 512Mi\n\ntoolsets:\n  kubernetes/core:\n    enabled: true\n  kubernetes/logs:\n    enabled: true\n  robusta:\n    enabled: false\n  internet:\n    enabled: false\n  prometheus/metrics:\n    enabled: false\n</code></pre>"},{"location":"reference/helm-configuration/#multiple-ai-providers-setup","title":"Multiple AI Providers Setup","text":"<pre><code># values.yaml\nadditionalEnvVars:\n  - name: OPENAI_API_KEY\n    valueFrom:\n      secretKeyRef:\n        name: holmes-secrets\n        key: openai-api-key\n  - name: ANTHROPIC_API_KEY\n    valueFrom:\n      secretKeyRef:\n        name: holmes-secrets\n        key: anthropic-api-key\n  - name: AWS_ACCESS_KEY_ID\n    valueFrom:\n      secretKeyRef:\n        name: holmes-secrets\n        key: aws-access-key-id\n  - name: AWS_SECRET_ACCESS_KEY\n    valueFrom:\n      secretKeyRef:\n        name: holmes-secrets\n        key: aws-secret-access-key\n\nmodelList:\n  gpt-4o:\n    api_key: \"{{ env.OPENAI_API_KEY }}\"\n    model: openai/gpt-4o\n    temperature: 0\n  anthropic-sonnet-4:\n    api_key: \"{{ env.ANTHROPIC_API_KEY }}\"\n    model: anthropic/claude-sonnet-4-20250514\n    temperature: 1\n    thinking:\n      budget_tokens: 10000\n      type: enabled\n  bedrock-sonnet-4:\n    aws_access_key_id: \"{{ env.AWS_ACCESS_KEY_ID }}\"\n    aws_region_name: us-east-1\n    aws_secret_access_key: \"{{ env.AWS_SECRET_ACCESS_KEY }}\"\n    model: bedrock/anthropic.claude-sonnet-4-20250514-v1:0\n    temperature: 1\n    thinking:\n      budget_tokens: 10000\n      type: enabled\n</code></pre>"},{"location":"reference/helm-configuration/#openshift-setup","title":"OpenShift Setup","text":"<pre><code># values.yaml\nopenshift: true\ncreateServiceAccount: true\n\nresources:\n  requests:\n    cpu: 100m\n    memory: 1024Mi\n  limits:\n    memory: 1024Mi\n\ntoolsets:\n  kubernetes/core:\n    enabled: true\n  kubernetes/logs:\n    enabled: true\n</code></pre>"},{"location":"reference/helm-configuration/#configuration-validation","title":"Configuration Validation","text":"<pre><code># Validate configuration\nhelm template holmesgpt robusta/holmes -f values.yaml\n\n# Dry run installation\nhelm install holmesgpt robusta/holmes -f values.yaml --dry-run\n\n# Check syntax\nyamllint values.yaml\n</code></pre>"},{"location":"reference/helm-configuration/#complete-reference","title":"Complete Reference","text":"<p>For the complete and up-to-date configuration reference, see the actual <code>values.yaml</code> file in the repository.</p>"},{"location":"reference/http-api/","title":"HolmesGPT API Reference","text":""},{"location":"reference/http-api/#overview","title":"Overview","text":"<p>The HolmesGPT API provides endpoints for automated investigations, workload health checks, and conversational troubleshooting. This document describes each endpoint, its purpose, request fields, and example usage.</p>"},{"location":"reference/http-api/#endpoints","title":"Endpoints","text":""},{"location":"reference/http-api/#apichat-post","title":"<code>/api/chat</code> (POST)","text":"<p>Description: General-purpose chat endpoint for interacting with the AI assistant. Supports open-ended questions and troubleshooting.</p>"},{"location":"reference/http-api/#request-fields","title":"Request Fields","text":"Field Required Default Type Description ask Yes string User's question conversation_history No list Conversation history (first message must be system) model No string Model name to use <p>Example <pre><code>curl -X POST http://&lt;HOLMES-URL&gt;/api/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"ask\": \"What is the status of my cluster?\",\n    \"conversation_history\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n    ]\n  }'\n</code></pre></p> <p>Example Response <pre><code>{\n  \"analysis\": \"Your cluster is healthy. All nodes are ready and workloads are running as expected.\",\n  \"conversation_history\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is the status of my cluster?\"},\n    {\"role\": \"assistant\", \"content\": \"Your cluster is healthy. All nodes are ready and workloads are running as expected.\"}\n  ],\n  \"tool_calls\": [...],\n  \"follow_up_actions\": [...]\n}\n</code></pre></p>"},{"location":"reference/http-api/#apiinvestigate-post","title":"<code>/api/investigate</code> (POST)","text":"<p>Description: Initiate an automated investigation of an issue or incident.</p>"},{"location":"reference/http-api/#request-fields_1","title":"Request Fields","text":"Field Required Default Type Description source Yes string Source of the issue (e.g., \"prometheus\") title Yes string Title of the investigation description Yes string Description of the issue subject Yes object Subject details (e.g., resource info) context Yes object Additional context source_instance_id No \"ApiRequest\" string Source instance identifier include_tool_calls No false boolean Include tool calls in response include_tool_call_results No false boolean Include tool call results in response prompt_template No \"builtin://generic_investigation.jinja2\" string Prompt template to use sections No object Structured output sections model No string Model name to use <p>Example <pre><code>curl -X POST http://&lt;HOLMES-URL&gt;/api/investigate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"source\": \"prometheus\",\n    \"title\": \"Pod CrashLoopBackOff\",\n    \"description\": \"Pod is crashing repeatedly\",\n    \"subject\": {\"namespace\": \"default\", \"pod\": \"my-pod\"},\n    \"context\": {},\n    \"include_tool_calls\": true,\n    \"model\": \"gpt-4o\"\n  }'\n</code></pre></p> <p>Example Response <pre><code>{\n  \"analysis\": \"The pod 'my-pod' in namespace 'default' is crashing due to an OOMKilled event. Consider increasing memory limits.\",\n  \"sections\": {\n    \"Alert Explanation\": \"...\",\n    \"Key Findings\": \"...\",\n    \"Conclusions and Possible Root Causes\": \"...\",\n    \"Next Steps\": \"...\",\n    \"App or Infra?\": \"...\",\n    \"External links\": \"...\"\n  },\n  \"tool_calls\": [\n    {\n      \"tool_call_id\": \"1\",\n      \"tool_name\": \"kubectl_logs\",\n      \"description\": \"Fetch pod logs\",\n      \"result\": {\"logs\": \"...\"}\n    }\n  ],\n  \"instructions\": [...]\n}\n</code></pre></p>"},{"location":"reference/http-api/#apistreaminvestigate-post","title":"<code>/api/stream/investigate</code> (POST)","text":"<p>Description: Same as <code>/api/investigate</code>, but returns results as a stream for real-time updates.</p>"},{"location":"reference/http-api/#request-fields_2","title":"Request Fields","text":"<p>Same as <code>/api/investigate</code>.</p> <p>Example <pre><code>curl -N -X POST http://&lt;HOLMES-URL&gt;/api/stream/investigate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"source\": \"prometheus\",\n    \"title\": \"Pod CrashLoopBackOff\",\n    \"description\": \"Pod is crashing repeatedly\",\n    \"subject\": {\"namespace\": \"default\", \"pod\": \"my-pod\"},\n    \"context\": {},\n    \"include_tool_calls\": true,\n    \"model\": \"gpt-4o\"\n  }'\n</code></pre></p> <p>Example Response (streamed) <pre><code>event: start_tool_calling\ndata: {\"tool_name\": \"kubectl_describe\", \"id\": \"call_0\"}\n\nevent: start_tool_calling\ndata: {\"tool_name\": \"kubectl_logs\", \"id\": \"call_1\"}\n\nevent: start_tool_calling\ndata: {\"tool_name\": \"kubectl_previous_logs\", \"id\": \"call_2\"}\n\nevent: start_tool_calling\ndata: {\"tool_name\": \"kubectl_memory_requests_namespace\", \"id\": \"call_3\"}\n\nevent: tool_calling_result\ndata: {\"tool_call_id\": \"call_3\", \"role\": \"tool\", \"description\": \"kubectl get pods -n default -o ...\", \"name\": \"kubectl_memory_requests_namespace\", \"result\": {...}}\n\nevent: tool_calling_result\ndata: {\"tool_call_id\": \"call_0\", \"role\": \"tool\", \"description\": \"kubectl describe pod my-pod -n default\", \"name\": \"kubectl_describe\", \"result\": {...}}\n\nevent: tool_calling_result\ndata: {\"tool_call_id\": \"call_2\", \"role\": \"tool\", \"description\": \"kubectl logs my-pod -n default --previous\", \"name\": \"kubectl_previous_logs\", \"result\": {...}}\n\nevent: tool_calling_result\ndata: {\"tool_call_id\": \"call_1\", \"role\": \"tool\", \"description\": \"kubectl logs my-pod -n default\", \"name\": \"kubectl_logs\", \"result\": {...}}\n\nevent: ai_answer_end\ndata: {\"sections\": {\"Alert Explanation\": ...}}\n</code></pre></p>"},{"location":"reference/http-api/#apiissue_chat-post","title":"<code>/api/issue_chat</code> (POST)","text":"<p>Description: Conversational interface for discussing a specific issue or incident, with context from a previous investigation.</p>"},{"location":"reference/http-api/#request-fields_3","title":"Request Fields","text":"Field Required Default Type Description ask Yes string User's question investigation_result Yes object Previous investigation result (see below) issue_type Yes string Type of issue conversation_history No list Conversation history (first message must be system) model No string Model name to use <p>investigation_result object: - <code>result</code> (string, optional): Previous analysis - <code>tools</code> (list, optional): Tools used/results</p> <p>Example <pre><code>curl -X POST http://&lt;HOLMES-URL&gt;/api/issue_chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"ask\": \"How do I fix this issue?\",\n    \"investigation_result\": {\n      \"result\": \"Pod crashed due to OOM.\",\n      \"tools\": []\n    },\n    \"issue_type\": \"CrashLoopBackOff\",\n    \"conversation_history\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n    ]\n  }'\n</code></pre></p> <p>Example Response <pre><code>{\n  \"analysis\": \"To fix the CrashLoopBackOff, increase the memory limit for the pod and check for memory leaks in the application.\",\n  \"conversation_history\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"How do I fix this issue?\"},\n    {\"role\": \"assistant\", \"content\": \"To fix the CrashLoopBackOff, increase the memory limit for the pod and check for memory leaks in the application.\"}\n  ],\n  \"tool_calls\": [...],\n  \"follow_up_actions\": [...]\n}\n</code></pre></p>"},{"location":"reference/http-api/#apiworkload_health_check-post","title":"<code>/api/workload_health_check</code> (POST)","text":"<p>Description: Performs a health check on a specified workload (e.g., a Kubernetes deployment).</p>"},{"location":"reference/http-api/#request-fields_4","title":"Request Fields","text":"Field Required Default Type Description ask Yes string User's question resource Yes object Resource details (e.g., name, kind) alert_history_since_hours No 24 float How many hours back to check alerts alert_history No true boolean Whether to include alert history stored_instructions No true boolean Use stored instructions instructions No [] list Additional instructions include_tool_calls No false boolean Include tool calls in response include_tool_call_results No false boolean Include tool call results in response prompt_template No \"builtin://kubernetes_workload_ask.jinja2\" string Prompt template to use model No string Model name to use <p>Example <pre><code>curl -X POST http://&lt;HOLMES-URL&gt;/api/workload_health_check \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"ask\": \"Why is my deployment unhealthy?\",\n    \"resource\": {\"name\": \"my-deployment\", \"kind\": \"Deployment\"},\n    \"alert_history_since_hours\": 12\n  }'\n</code></pre></p> <p>Example Response <pre><code>{\n  \"analysis\": \"Deployment 'my-deployment' is unhealthy due to repeated CrashLoopBackOff events.\",\n  \"sections\": null,\n  \"tool_calls\": [\n    {\n      \"tool_call_id\": \"2\",\n      \"tool_name\": \"kubectl_get_events\",\n      \"description\": \"Fetch recent events\",\n      \"result\": {\"events\": \"...\"}\n    }\n  ],\n  \"instructions\": [...]\n}\n</code></pre></p>"},{"location":"reference/http-api/#apiworkload_health_chat-post","title":"<code>/api/workload_health_chat</code> (POST)","text":"<p>Description: Conversational interface for discussing the health of a workload.</p>"},{"location":"reference/http-api/#request-fields_5","title":"Request Fields","text":"Field Required Default Type Description ask Yes string User's question workload_health_result Yes object Previous health check result (see below) resource Yes object Resource details conversation_history No list Conversation history (first message must be system) model No string Model name to use <p>workload_health_result object: - <code>analysis</code> (string, optional): Previous analysis - <code>tools</code> (list, optional): Tools used/results</p> <p>Example <pre><code>curl -X POST http://&lt;HOLMES-URL&gt;/api/workload_health_chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"ask\": \"Check the workload health.\",\n    \"workload_health_result\": {\n      \"analysis\": \"Previous health check: all good.\",\n      \"tools\": []\n    },\n    \"resource\": {\"name\": \"my-deployment\", \"kind\": \"Deployment\"},\n    \"conversation_history\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n    ]\n  }'\n</code></pre></p> <p>Example Response <pre><code>{\n  \"analysis\": \"The deployment 'my-deployment' is healthy. No recent issues detected.\",\n  \"conversation_history\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Check the workload health.\"},\n    {\"role\": \"assistant\", \"content\": \"The deployment 'my-deployment' is healthy. No recent issues detected.\"}\n  ],\n  \"tool_calls\": [...]\n}\n</code></pre></p>"},{"location":"reference/http-api/#apimodel-get","title":"<code>/api/model</code> (GET)","text":"<p>Description: Returns a list of available AI models that can be used for investigations and chat.</p> <p>Example <pre><code>curl http://&lt;HOLMES-URL&gt;/api/model\n</code></pre></p> <p>Example Response <pre><code>{\n  \"model_name\": [\"gpt-4o\", \"azure/gpt-4o\", \"robusta\"]\n}\n</code></pre></p>"},{"location":"reference/slash-commands/","title":"Slash Commands","text":"<p>Slash commands provide special actions in HolmesGPT's interactive mode. Type any command with a leading <code>/</code> to execute it.</p>"},{"location":"reference/slash-commands/#built-in-commands","title":"Built-in Commands","text":"Command Description <code>/help</code> Show all available commands and their descriptions <code>/exit</code> Exit interactive mode and return to shell <code>/clear</code> Clear conversation history and start fresh <code>/tools</code> Show available toolsets and their status <code>/context</code> Display token usage and context size information <code>/auto</code> Toggle automatic display of tool outputs <code>/last</code> Show tool outputs from the most recent AI response <code>/show [number\\|name]</code> View specific tool output in scrollable modal <code>/run &lt;command&gt;</code> Execute shell command and optionally share with AI <code>/shell</code> Start interactive shell session"},{"location":"reference/slash-commands/#command-details","title":"Command Details","text":""},{"location":"reference/slash-commands/#help","title":"<code>/help</code>","text":"<p>Displays all available slash commands and their descriptions.</p> <pre><code>&gt; /help\n</code></pre>"},{"location":"reference/slash-commands/#exit","title":"<code>/exit</code>","text":"<p>Exits the interactive session and returns to your shell. Alternative: press <code>Ctrl+C</code> twice.</p> <pre><code>&gt; /exit\n</code></pre>"},{"location":"reference/slash-commands/#clear","title":"<code>/clear</code>","text":"<p>Clears all conversation history and starts fresh. Useful when switching topics or when context becomes too large.</p> <pre><code>&gt; /clear\n</code></pre>"},{"location":"reference/slash-commands/#tools","title":"<code>/tools</code>","text":"<p>Lists all configured toolsets and their current status (enabled/disabled).</p> <pre><code>&gt; /tools\nAvailable toolsets:\n\u2713 kubernetes (enabled)\n\u2713 prometheus (enabled)\n\u2717 grafana (disabled - no URL configured)\n</code></pre>"},{"location":"reference/slash-commands/#context","title":"<code>/context</code>","text":"<p>Displays detailed information about token usage and context size.</p> <pre><code>&gt; /context\nContext Usage:\n- System: 1,250 tokens\n- User: 3,421 tokens\n- Assistant: 2,156 tokens\n- Tools: 5,234 tokens\nTotal: 12,061 / 128,000 tokens (9.4%)\n\nTop tools by token usage:\n1. kubernetes_get_pod_logs: 2,341 tokens\n2. kubernetes_describe_pod: 1,893 tokens\n</code></pre>"},{"location":"reference/slash-commands/#auto","title":"<code>/auto</code>","text":"<p>Controls whether tool outputs are automatically shown after each AI response. When disabled, use <code>/last</code> to view outputs.</p> <pre><code>&gt; /auto\nAuto-display tool outputs: ON\n</code></pre>"},{"location":"reference/slash-commands/#last","title":"<code>/last</code>","text":"<p>Displays all tool outputs from the most recent AI response. Useful when auto-display is off or to review outputs again.</p> <pre><code>&gt; /last\n</code></pre>"},{"location":"reference/slash-commands/#show-numbername","title":"<code>/show [number|name]</code>","text":"<p>Opens a scrollable modal to view full tool output. You can specify outputs by: - Number: <code>/show 3</code> (shows 3<sup>rd</sup> tool output) - Name: <code>/show kubernetes_get_pod_logs</code> (shows specific tool output)</p> <pre><code>&gt; /show 1\n&gt; /show kubernetes_get_pod_logs\n</code></pre>"},{"location":"reference/slash-commands/#modal-navigation","title":"Modal Navigation","text":"<ul> <li><code>j</code>/<code>k</code> or \u2191/\u2193 - Move up/down</li> <li><code>g</code>/<code>G</code> - Go to top/bottom</li> <li><code>d</code>/<code>u</code> - Half page down/up</li> <li><code>f</code>/<code>b</code> or PgDn/PgUp - Full page down/up</li> <li><code>w</code> - Toggle word wrap</li> <li><code>q</code> or Esc - Close modal</li> </ul>"},{"location":"reference/slash-commands/#run-command","title":"<code>/run &lt;command&gt;</code>","text":"<p>Runs a shell command and optionally shares the output with the AI.</p> <pre><code>&gt; /run kubectl get pods -n production\n# Output displayed...\nShare output with AI? (Y/n): y\nAdd a comment or question (optional): why are some pods in CrashLoopBackOff?\n</code></pre>"},{"location":"reference/slash-commands/#shell","title":"<code>/shell</code>","text":"<p>Starts an interactive shell session. When you exit, you can share the entire session with the AI.</p> <pre><code>&gt; /shell\n$ kubectl logs failing-pod-xyz\n$ kubectl describe pod failing-pod-xyz\n$ exit\nShare shell session with AI? (Y/n): y\nAdd a comment or question (optional): The pod keeps failing with OOM errors\n</code></pre>"},{"location":"reference/troubleshooting/","title":"HolmesGPT Not Finding Any Issues? Here's Why.","text":""},{"location":"reference/troubleshooting/#1-truncation-too-much-data","title":"1. Truncation: Too Much Data","text":"<p>Data overflow causes important information to be truncated. See #437 for summarization improvements.</p> <p>Solution:</p> <ul> <li>Use specific namespaces and time ranges</li> <li>Target individual components instead of cluster-wide queries</li> </ul>"},{"location":"reference/troubleshooting/#2-missing-data-access","title":"2. Missing Data Access","text":"<p>HolmesGPT can't access logs, metrics, or traces from your observability stack.</p> <p>Solution:</p> <ul> <li>Verify toolset configuration connects to Prometheus/Grafana/logs</li> <li>Test connectivity: <code>kubectl exec -it &lt;holmes-pod&gt; -- curl http://prometheus:9090/api/v1/query?query=up</code></li> </ul>"},{"location":"reference/troubleshooting/#3-rbac-permissions","title":"3. RBAC Permissions","text":"<p>Service account lacks Kubernetes API permissions.</p> <p>Error Example: <pre><code>pods is forbidden: User \"system:serviceaccount:default:holmesgpt\" cannot get resource \"pods\"\n</code></pre></p> <p>Solution: <pre><code>rbac:\n  create: true\nrbacRules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"services\", \"events\", \"nodes\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n</code></pre></p>"},{"location":"reference/troubleshooting/#4-unclear-prompts","title":"4. Unclear Prompts","text":"<p>Vague questions produce poor results.</p> <p>Bad:</p> <ul> <li>\"Why is my pod not working?\"</li> <li>\"Check if anything is wrong with my cluster\"</li> <li>\"Something is broken in production and users are complaining\"</li> <li>\"My deployment keeps failing but I don't know why\"</li> <li>\"Can you debug this issue I'm having with my application?\"</li> </ul> <p>Good:</p> <ul> <li>\"Why is payment-service pod restarting in production namespace?\"</li> <li>\"What caused memory spike in web-frontend deployment last hour?\"</li> </ul>"},{"location":"reference/troubleshooting/#5-model-issues","title":"5. Model Issues","text":"<p>Older LLM models lack reasoning capability for complex problems.</p> <p>Solution: <pre><code>config:\n  model: \"gpt-4o\"  # or anthropic/claude-3-5-sonnet-20241022\n  temperature: 0.1\n  maxTokens: 2000\n</code></pre></p> <p>Recommended Models:</p> <ul> <li><code>gpt-4o</code> - Best balance of speed/capability</li> <li><code>anthropic/claude-3-5-sonnet-20241022</code> - Superior reasoning</li> <li><code>gpt-4-turbo</code> - Fast analysis</li> </ul>"},{"location":"reference/troubleshooting/#still-stuck","title":"Still stuck?","text":"<p>Join our Slack community or open a GitHub issue for help.</p>"},{"location":"walkthrough/","title":"Walkthrough","text":"<p>Get started with HolmesGPT by running your first investigation.</p>"},{"location":"walkthrough/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>\u2705 HolmesGPT CLI installed - See CLI Installation Guide</li> <li>\u2705 AI provider API key configured - See AI Provider Setup</li> <li>\u2705 kubectl access to a Kubernetes cluster - Any cluster will work</li> </ul>"},{"location":"walkthrough/#run-your-first-investigation","title":"Run Your First Investigation","text":"<p>Let's investigate a pod with HolmesGPT to see the value it provides:</p> <ol> <li> <p>Create a test pod with an issue: <pre><code>kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/main/pending_pods/pending_pod_node_selector.yaml\n</code></pre></p> </li> <li> <p>Ask Holmes to investigate:</p> OpenAI (Default)Azure OpenAIAnthropic ClaudeGoogle GeminiAWS BedrockOllama <pre><code>holmes ask \"describe the user-profile-import pod and explain any issues\"\n</code></pre> <pre><code>holmes ask \"describe the user-profile-import pod and explain any issues\" --model=\"azure/&lt;your-model-name&gt;\"\n</code></pre> <pre><code>holmes ask \"describe the user-profile-import pod and explain any issues\" --model=\"anthropic/&lt;your-model-name&gt;\"\n</code></pre> <pre><code>holmes ask \"describe the user-profile-import pod and explain any issues\" --model=\"google/&lt;your-model-name&gt;\"\n</code></pre> <pre><code>holmes ask \"describe the user-profile-import pod and explain any issues\" --model=\"bedrock/&lt;your-model-name&gt;\"\n</code></pre> <pre><code>holmes ask \"describe the user-profile-import pod and explain any issues\" --model=\"ollama/&lt;your-model-name&gt;\"\n</code></pre> </li> <li> <p>See the value:</p> <p>Holmes will analyze the pod, identify that it's stuck in \"Pending\" state due to an invalid node selector, and suggest specific remediation steps - all without you needing to manually run <code>kubectl describe</code>, check events, or dig through logs.</p> </li> </ol>"},{"location":"walkthrough/#what-you-just-experienced","title":"What You Just Experienced","text":"<p>HolmesGPT automatically:</p> <ul> <li>\u2705 Gathered context - Retrieved pod status, events, and related information</li> <li>\u2705 Identified the root cause - Invalid node selector preventing scheduling</li> <li>\u2705 Provided actionable solutions - Specific commands to fix the issue</li> <li>\u2705 Saved investigation time - No manual troubleshooting steps required</li> </ul>"},{"location":"walkthrough/#clean-up","title":"Clean Up","text":"<p>Remove the test pod:</p> <pre><code>kubectl delete pod user-profile-import\n</code></pre>"},{"location":"walkthrough/#next-steps","title":"Next Steps","text":"<ul> <li>Add integrations - Connect monitoring tools like Prometheus, Grafana, and DataDog</li> <li>Troubleshooting guide - Common issues and solutions</li> <li>Join our Slack - Get help from the community</li> <li>Request features on GitHub - Suggest improvements or report bugs</li> </ul>"},{"location":"walkthrough/cicd-troubleshooting/","title":"CI/CD Pipeline Troubleshooting","text":"<p>HolmesGPT can be integrated into CI/CD pipelines to automatically troubleshoot deployment failures, providing instant insights when deployments fail. Optionally, you can get the AI investigation results sent directly to your Slack channel.</p> <p></p>"},{"location":"walkthrough/cicd-troubleshooting/#automated-deployment-troubleshooting","title":"Automated Deployment Troubleshooting","text":"<p>Example of using HolmesGPT in a CI/CD pipeline to automatically troubleshoot deployment failures and send results to Slack:</p> <pre><code># .github/workflows/deploy.yml or gitlab-ci.yml\n- name: Install HolmesGPT\n  run: |\n    # Clone the branch of holmes you want to install\n    git clone -b slack-markdown-fix https://github.com/robusta-dev/holmesgpt.git /tmp/holmesgpt\n    cd /tmp/holmesgpt\n\n    # Install Poetry\n    curl -sSL https://install.python-poetry.org | python3 - --version 1.4.0\n    export PATH=\"$HOME/.local/bin:$PATH\"\n\n    # Install dependencies using Poetry\n    poetry install\n\n- name: Deploy to EKS\n  env:\n    OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n  run: |\n    # Apply Kubernetes manifests\n    kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/refs/heads/main/image_pull_backoff/no_such_image.yaml\n    # Wait for rollout\n    if ! kubectl rollout status deployment/customer-relations-webapp --timeout=150s; then\n      echo \"Deployment failed - starting HolmesGPT investigation\"\n      # Run HolmesGPT investigation and send directly to Slack\n      cd /tmp/holmesgpt\n      poetry run holmes ask \\\n        \"\ud83d\udea8 EKS Deployment Failed in ${{ github.repository }}\n\n        Environment: EKS Cluster ${{ vars.EKS_CLUSTER_NAME }} in ${{ vars.AWS_REGION }}\n        Commit: ${{ github.sha }}\n        Pipeline: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\n        The deployment failed. Analyze why the pods are not becoming ready. Focus on: image pulls, resource limits, probes, and configuration issues\" \\\n        --no-interactive \\\n        --destination slack \\\n        --slack-token \"${{ secrets.SLACK_TOKEN }}\" \\\n        --slack-channel \"#YOUR_CHANNEL_NAME\"\n      exit 1\n    fi\n</code></pre> Complete Example GitHub Actions Workflow <pre><code>name: Deploy to Production\n\non:\n  push:\n    branches:\n      - main\n  workflow_dispatch:\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        aws-region: ${{ vars.AWS_REGION }}\n\n    - name: Set up kubectl\n      uses: azure/setup-kubectl@v3\n      with:\n        version: v1.28.0\n\n    - name: Configure kubectl for EKS\n      run: |\n        aws eks update-kubeconfig --region ${{ vars.AWS_REGION }} --name ${{ vars.EKS_CLUSTER_NAME }}\n        # Get current user ARN and add to cluster access\n        USER_ARN=$(aws sts get-caller-identity --query Arn --output text)\n        aws eks create-access-entry --cluster-name ${{ vars.EKS_CLUSTER_NAME }} --principal-arn $USER_ARN --region ${{ vars.AWS_REGION }} || true\n        aws eks associate-access-policy --cluster-name ${{ vars.EKS_CLUSTER_NAME }} --principal-arn $USER_ARN --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy --access-scope type=cluster --region ${{ vars.AWS_REGION }} || true\n\n    - name: Debug kubeconfig\n      run: |\n        echo \"Testing kubectl connection...\"\n        kubectl config current-context &gt; /dev/null\n        kubectl cluster-info &gt; /dev/null\n        kubectl auth can-i get pods --namespace=default &gt; /dev/null\n        echo \"kubectl connection verified\"\n\n    - name: Install HolmesGPT\n      run: |\n        # Clone the branch of holmes you want to install\n        git clone -b slack-markdown-fix https://github.com/robusta-dev/holmesgpt.git /tmp/holmesgpt\n        cd /tmp/holmesgpt\n\n        # Install Poetry\n        curl -sSL https://install.python-poetry.org | python3 - --version 1.4.0\n        export PATH=\"$HOME/.local/bin:$PATH\"\n\n        # Install dependencies using Poetry\n        poetry install\n\n    - name: Deploy to EKS\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n      run: |\n        # Apply Kubernetes manifests\n        kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/refs/heads/main/image_pull_backoff/no_such_image.yaml\n        # Wait for rollout\n        if ! kubectl rollout status deployment/customer-relations-webapp --timeout=150s; then\n          echo \"Deployment failed - starting HolmesGPT investigation\"\n          # Run HolmesGPT investigation and send directly to Slack\n          cd /tmp/holmesgpt\n          poetry run holmes ask \\\n            \"\ud83d\udea8 EKS Deployment Failed in ${{ github.repository }}\n\n            Environment: EKS Cluster ${{ vars.EKS_CLUSTER_NAME }} in ${{ vars.AWS_REGION }}\n            Commit: ${{ github.sha }}\n            Pipeline: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\n            The deployment failed. Analyze why the pods are not becoming ready. Focus on: image pulls, resource limits, probes, and configuration issues\" \\\n            --no-interactive \\\n            --destination slack \\\n            --slack-token \"${{ secrets.SLACK_TOKEN }}\" \\\n            --slack-channel \"#YOUR_CHANNEL_NAME\"\n          exit 1\n        fi\n</code></pre>"},{"location":"walkthrough/cicd-troubleshooting/#simplified-approaches","title":"Simplified Approaches","text":"<p>The built-in Slack integration will automatically format and send the analysis to your specified channel. You can also use a simpler approach for basic deployments:</p> <pre><code># Simple deployment check with Slack notification\nkubectl rollout status deployment/app -n prod --timeout=300s || \\\n  holmes ask \"deployment/app in prod namespace failed to roll out\" \\\n    --destination slack \\\n    --slack-token \"$SLACK_TOKEN\" \\\n    --slack-channel \"#alerts\"\n</code></pre>"},{"location":"walkthrough/interactive-mode/","title":"Interactive Mode","text":"<p>HolmesGPT's interactive mode provides a powerful chat-like interface for iterative troubleshooting and investigation of cloud-native issues using LLMs.</p>"},{"location":"walkthrough/interactive-mode/#getting-started","title":"Getting Started","text":""},{"location":"walkthrough/interactive-mode/#launching-interactive-mode","title":"Launching Interactive Mode","text":"<p>Interactive mode is the default when running <code>holmes ask</code>:</p> <pre><code>holmes ask\n</code></pre> <p>You can also provide an initial question:</p> <pre><code>holmes ask \"what pods are failing?\"\n</code></pre>"},{"location":"walkthrough/interactive-mode/#example-workflows","title":"Example Workflows","text":""},{"location":"walkthrough/interactive-mode/#autonomous-ai-investigation","title":"Autonomous AI Investigation","text":"<p>In this example, the AI independently investigates and diagnoses an issue using its available tools:</p> <pre><code>holmes ask\n\n&gt; why is the payment-service in production namespace not responding to requests?\n\nRunning tool #1 kubectl_find_resource: kubectl get -A --show-labels -o wide deployment | grep payment\n  Finished #1 in 1.32s, output length: 894 characters - /show 1 to view contents\n\nRunning tool #2 kubectl_describe: kubectl describe deployment payment-service -n production\n  Finished #2 in 1.45s, output length: 2,341 characters - /show 2 to view contents\n\nRunning tool #3 kubectl_get_by_kind_in_namespace: kubectl get pod -n production -o wide --show-labels | grep -E 'NAME|payment'\n  Finished #3 in 1.28s, output length: 456 characters - /show 3 to view contents\n\nRunning tool #4 fetch_pod_logs: Fetching logs for pod payment-service-7b9c5d4-xyz in namespace production\n  Finished #4 in 2.87s, output length: 8,234 characters - /show 4 to view contents\n\nRunning tool #5 kubectl_events: kubectl get events -n production --field-selector involvedObject.name=payment-service-7b9c5d4-xyz\n  Finished #5 in 1.19s, output length: 1,203 characters - /show 5 to view contents\n\nBased on my investigation, the payment-service is not responding due to:\n\n1. **Database Connection Failures**: The logs show repeated \"connection refused\" errors to `payment-db:5432`\n2. **StatefulSet Down**: The payment-db StatefulSet has 0/1 replicas running\n3. **PVC Pending**: The database's PersistentVolumeClaim is stuck in Pending status\n\nRoot cause appears to be storage-related. Let me check the PVC status.\n\n&gt; can you check why the PVC is pending?\n\nRunning tool #1 kubectl_describe: kubectl describe pvc payment-db-pvc-0 -n production\n  Finished #1 in 1.38s, output length: 1,567 characters - /show 1 to view contents\n\nRunning tool #2 kubectl_get: kubectl get storageclass fast-ssd -o yaml\n  Finished #2 in 1.14s, output length: 134 characters - /show 2 to view contents\n\nRunning tool #3 kubectl_get_by_kind_in_namespace: kubectl get storageclass\n  Finished #3 in 1.23s, output length: 456 characters - /show 3 to view contents\n\nThe PVC is pending because the requested StorageClass doesn't exist:\n\n\n  Error from server (NotFound): storageclasses.storage.k8s.io \"fast-ssd\" not found\n\nAvailable StorageClasses in the cluster:\n- **gp2** (default) - EBS General Purpose SSD\n- **gp3** - EBS General Purpose SSD (newer generation)\n- **io1** - EBS Provisioned IOPS SSD\n- **standard** - EBS Magnetic volumes\n</code></pre>"},{"location":"walkthrough/interactive-mode/#human-in-the-loop-investigation","title":"Human-in-the-Loop Investigation","text":"<p>In this example, the human provides additional context and runs commands the AI couldn't access:</p> <pre><code>holmes ask \"users are reporting intermittent 502 errors on the checkout page but I don't see any pod failures\"\n\nRunning tool #1 kubectl_get_by_kind_in_namespace: kubectl get pod -n production -o wide --show-labels\n  Finished #1 in 1.41s, output length: 3,234 characters - /show 1 to view contents\n\nRunning tool #2 kubectl_top_pods: kubectl top pods -n production --containers\n  Finished #2 in 2.18s, output length: 1,876 characters - /show 2 to view contents\n\nRunning tool #3 fetch_pod_logs: Fetching logs for pod frontend-6d4cf56-abc in namespace production. filter=502|error\n  Finished #3 in 3.21s, output length: 456 characters - /show 3 to view contents\n\nRunning tool #4 prometheus_query: query='rate(http_requests_total{status=\"502\"}[5m])'\n  Finished #4 in 1.92s, output length: 234 characters - /show 4 to view contents\n\nAll pods are healthy with normal resource usage. The 502 errors in the logs show \"upstream connect error\". The errors spike every 10 minutes. I need access to:\n- Load balancer/ingress controller logs\n- Corporate proxy or WAF logs if you use one\n- Database connection pool metrics from inside the app\n- Any recent network or firewall changes\n\n&gt; /run aws elbv2 describe-target-health --target-group-arn arn:aws:elasticloadbalancing:us-east-1:123456:targetgroup/prod-tg/abc123\nRunning command: aws elbv2 describe-target-health --target-group-arn arn:aws:elasticloadbalancing:us-east-1:123456:targetgroup/prod-tg/abc123\n\n{\n  \"TargetHealthDescriptions\": [\n    {\n      \"Target\": {\"Id\": \"i-0abc123\", \"Port\": 80},\n      \"HealthCheckPort\": \"80\",\n      \"TargetHealth\": {\n        \"State\": \"unhealthy\",\n        \"Reason\": \"Target.Timeout\",\n        \"Description\": \"Request timed out\"\n      }\n    }\n  ]\n}\n\nShare output with AI? (Y/n): y\nAdd a comment or question (optional): This is our ALB target group\n\nThe issue is clearer now. The ALB target `i-0abc123` is failing health checks with timeouts. This EC2 instance appears to be outside your Kubernetes cluster.\n\n&gt; /run ssh admin@10.0.1.50 \"sudo tail -100 /var/log/nginx/error.log | grep -i 'too many' | head -5\"\nRunning command: ssh admin@10.0.1.50 \"sudo tail -100 /var/log/nginx/error.log | grep -i 'too many' | head -5\"\n\n2024/11/20 10:00:01 [error] 1234#1234: *456789 limiting connections by zone \"perip\", client: 10.0.2.100\n2024/11/20 10:10:02 [error] 1234#1234: *456790 limiting connections by zone \"perip\", client: 10.0.2.100\n2024/11/20 10:20:01 [error] 1234#1234: *456791 limiting connections by zone \"perip\", client: 10.0.2.100\n2024/11/20 10:30:03 [error] 1234#1234: *456792 limiting connections by zone \"perip\", client: 10.0.2.100\n2024/11/20 10:40:02 [error] 1234#1234: *456793 limiting connections by zone \"perip\", client: 10.0.2.100\n\nShare output with AI? (Y/n): y\nAdd a comment or question (optional): This is the nginx rate limiting on the EC2 instance. 10.0.2.100 is our k8s cluster NAT gateway\n\n**Root Cause Found**: The 502 errors are caused by Nginx rate limiting on the EC2 instance:\n\n1. **Every 10 minutes** your Kubernetes pods hit the rate limit\n2. **Source IP problem**: All K8s traffic appears as one IP (10.0.2.100 - NAT Gateway)\n3. **Nginx configuration**: `limit_req_zone $binary_remote_addr zone=perip` treats all K8s traffic as one client\n\n**Solution**:\n```nginx\n# Update nginx config to use X-Forwarded-For for K8s traffic\nmap $remote_addr $limit_key {\n    \"10.0.2.100\" $http_x_forwarded_for;  # K8s NAT Gateway\n    default      $binary_remote_addr;\n}\nlimit_req_zone $limit_key zone=perip:10m rate=100r/s;\n</code></pre>"},{"location":"walkthrough/interactive-mode/#scenarios-to-consider-using-run-or-shell","title":"Scenarios to Consider Using /run or /shell","text":"<ol> <li>Access to Data HolmesGPT Can't Access on It's Own</li> <li>ssh'ing into a machine</li> <li>run commands with escalated permisssions (sudo)</li> <li> <p>accessing data sources HolmesGPT can't access on it's own</p> </li> <li> <p>Adding Business Context</p> </li> <li>Pulling in marketing data with recent campaign launch times</li> <li> <p>Fetcing planned maintenance windows from a source HolmesGPT does not know about</p> </li> <li> <p>Sharing Development/Testing Insights</p> </li> <li>Point HolmesGPT at recent code deployments or git commits</li> </ol>"},{"location":"walkthrough/interactive-mode/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li>Use <code>/reset</code> when switching topics - This gives you a fresh context and prevents confusion</li> <li>Use <code>/run</code> if the AI is missing something important - Guide the investigation by showing it what it is missing</li> <li>Check <code>/context</code> periodically - Especially during long investigations</li> <li>View evidence with <code>/show</code> - Full outputs often contain important details</li> <li>Add comments when sharing shell output - Helps the AI understand what you're looking for</li> </ol>"},{"location":"walkthrough/interactive-mode/#beyond-interactive-mode","title":"Beyond Interactive Mode","text":"<p>For additional HolmesGPT usage patterns, see:</p> <ul> <li>CI/CD Troubleshooting - Integrate HolmesGPT into deployment pipelines</li> </ul>"},{"location":"walkthrough/investigating-prometheus-alerts/","title":"Investigating Prometheus Alerts","text":"<p>You can investigate Prometheus/AlertManager alerts using HolmesGPT by connecting it to your AlertManager instance. This allows you to run investigations on all active alerts or a specific alert.</p> <p></p>"},{"location":"walkthrough/investigating-prometheus-alerts/#prerequisites","title":"Prerequisites","text":"<ul> <li>HolmesGPT CLI installed (installation guide)</li> <li>An AI provider API key configured (setup guide)</li> <li>Access to your AlertManager instance</li> </ul>"},{"location":"walkthrough/investigating-prometheus-alerts/#investigating-a-prometheus-alert-using-holmesgpt","title":"Investigating a Prometheus Alert Using HolmesGPT","text":""},{"location":"walkthrough/investigating-prometheus-alerts/#step-1-forward-alertmanager","title":"Step 1: Forward AlertManager","text":"<p>First, you need to forward the AlertManager service to your local machine so HolmesGPT can connect to it. Run the following command in your terminal:</p> <pre><code>kubectl port-forward svc/&lt;Your-Alertmanager-Service&gt; 9093:9093\n</code></pre>"},{"location":"walkthrough/investigating-prometheus-alerts/#step-2-create-a-test-alert","title":"Step 2: Create a Test Alert","text":"<p>Now we'll deploy a crashing workload and simulate an alert from AlertManager.</p> <p><pre><code>kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/main/crashpod/broken.yaml\n</code></pre> Since it takes some time for the alert to be generated, we will manually send a <code>KubePodCrashLooping</code> alert to AlertManager for testing purposes. To do this run: <pre><code># Send a KubePodCrashLooping alert directly to AlertManager\ncurl -X POST http://localhost:9093/api/v1/alerts \\\n  -H \"Content-Type: application/json\" \\\n  -d '[\n    {\n      \"labels\": {\n        \"alertname\": \"KubePodCrashLooping\",\n        \"severity\": \"warning\",\n        \"namespace\": \"default\",\n        \"pod\": \"payment-processing-worker\",\n        \"container\": \"worker\",\n        \"job\": \"kubernetes-pods\"\n      },\n      \"annotations\": {\n        \"description\": \"Pod default/payment-processing-worker is crash looping\",\n        \"summary\": \"Pod is in CrashLoopBackOff state\"\n      },\n      \"generatorURL\": \"http://prometheus:9090/graph?g0.expr=increase%28kube_pod_container_status_restarts_total%5B1h%5D%29%20%3E%205\",\n      \"startsAt\": \"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'\"\n    }\n  ]'\n</code></pre> You should now see the <code>KubePodCrashLooping</code> alert in your AlertManager UI at <code>http://localhost:9093</code>.</p>"},{"location":"walkthrough/investigating-prometheus-alerts/#step-3-investigate-alerts","title":"Step 3: Investigate Alerts","text":"<p>Finally let's use the HolmesGPT <code>investigate</code> subcommand to investigate the alerts. Run the following command:</p> <p><pre><code>holmes investigate alertmanager --alertmanager-url http://localhost:9093\n</code></pre> </p> <p>By default, HolmesGPT will fetch all active alerts from AlertManager and investigate them.</p> <p>For our investigation, we will use the <code>--alertmanager-alertname</code> flag to focus on the specific <code>KubePodCrashLooping</code> alert we created earlier. <pre><code>holmes investigate alertmanager --alertmanager-url http://localhost:9093 --alertmanager-alertname \"KubePodCrashLooping\"\n</code></pre> Once the investigation is complete, HolmesGPT will provide the potential Root Cause, next steps, and more.</p>"},{"location":"walkthrough/investigating-prometheus-alerts/#filtering-alerts","title":"Filtering Alerts","text":"<p>The <code>holmes investigate alertmanager</code> command supports many flags. For example, to investigate only critical alerts or alerts in a specific namespace, you can use the <code>--alertmanager-label</code> flag:</p> <pre><code># Critical alerts only\nholmes investigate alertmanager \\\n  --alertmanager-url http://localhost:9093 \\\n  --alertmanager-label \"severity=critical\"\n\n# Production namespace issues\nholmes investigate alertmanager \\\n  --alertmanager-url http://localhost:9093 \\\n  --alertmanager-label \"namespace=production\"\n</code></pre>"},{"location":"walkthrough/investigating-prometheus-alerts/#whats-next","title":"What's Next?","text":"<ul> <li>Add new data sources - Connect HolmesGPT to your databases, APM tools, and custom APIs for deeper investigations.</li> <li>Set up remote MCP - Add data sources as remote Model Context Protocol (MCP) servers.</li> </ul>"},{"location":"walkthrough/investigating-using-aks-mcp-server/","title":"Investigating using AKS MCP Server","text":"<p>You can investigate Azure Kubernetes Service issues using HolmesGPT with the AKS MCP (Model Context Protocol) server.</p> <p></p>"},{"location":"walkthrough/investigating-using-aks-mcp-server/#prerequisites","title":"Prerequisites","text":"<ul> <li>HolmesGPT CLI installed (installation guide)</li> <li>An AI provider API key configured (setup guide)</li> <li>Azure CLI installed and authenticated</li> <li>Access to Azure Kubernetes Service clusters</li> <li>Azure Kubernetes Service VS Code extension installed</li> </ul>"},{"location":"walkthrough/investigating-using-aks-mcp-server/#setting-up-aks-mcp-server","title":"Setting Up AKS MCP Server","text":""},{"location":"walkthrough/investigating-using-aks-mcp-server/#step-1-setup-the-mcp-server","title":"Step 1: Setup the MCP Server","text":"<ul> <li>Open VS Code Command Palette (<code>Ctrl+Shift+P</code> or <code>Cmd+Shift+P</code>)</li> <li>Run: \"AKS: Setup AKS MCP Server\"</li> <li>Follow the setup wizard to configure your Azure credentials and cluster access</li> </ul>"},{"location":"walkthrough/investigating-using-aks-mcp-server/#step-2-update-configuration-for-sse","title":"Step 2: Update Configuration for SSE","text":"<p>After installation, update your VS Code MCP configuration (<code>.vscode/mcp.json</code>) to use SSE transport and start the server    <pre><code>{\n  \"servers\": {\n    \"AKS MCP\": {\n      \"command\": \"/Users/yourname/.vs-kubernetes/tools/aks-mcp/v0.0.3/aks-mcp\",\n      \"args\": [\n        \"--transport\",\n        \"sse\"\n      ]\n    }\n  }\n}\n</code></pre> Note: Change <code>\"stdio\"</code> to <code>\"sse\"</code> in the transport argument.</p>"},{"location":"walkthrough/investigating-using-aks-mcp-server/#step-3-configure-holmesgpt","title":"Step 3: Configure HolmesGPT","text":"<p>Add this configuration to your HolmesGPT config file (<code>~/.holmes/config.yaml</code>):</p> <pre><code>mcp_servers:\n  aks-mcp:\n    description: \"Azure Kubernetes Service(AKS) Model Context Protocol(MCP) server\"\n    url: \"http://localhost:8000/sse\"\n    llm_instructions: \"MCP server to get AKS cluster information, retrieve cluster resources and workloads, analyze network policies and VNet configurations, query control plane logs, fetch cluster metrics and health status. Investigate networking issues with NSGs and load balancers, perform kubectl operations, real-time monitoring of DNS, services across Azure Kubernetes environments\"\n</code></pre>"},{"location":"walkthrough/investigating-using-aks-mcp-server/#investigation-examples","title":"Investigation Examples","text":"<p>Once configured, you can investigate AKS issues using natural language queries:</p>"},{"location":"walkthrough/investigating-using-aks-mcp-server/#cluster-health-issues","title":"Cluster Health Issues","text":"<pre><code>holmes ask \"What issues do I have in my AKS cluster?\"\n</code></pre>"},{"location":"walkthrough/investigating-using-aks-mcp-server/#network-connectivity-problems","title":"Network Connectivity Problems","text":"<pre><code>holmes ask \"My payment deployment can't reach external services investigate why\"\n</code></pre>"},{"location":"walkthrough/investigating-using-aks-mcp-server/#whats-next","title":"What's Next?","text":"<ul> <li>Add more data sources - Combine AKS MCP with other observability tools</li> <li>Set up additional MCP servers - Integrate multiple specialized MCP servers</li> <li>Configure custom toolsets - Create specialized investigation workflows</li> </ul>"}]}